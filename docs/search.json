[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introducci√≥n al an√°lisis estad√≠stico de datos con R",
    "section": "",
    "text": "Prologo\nEn los √∫ltimos a√±os, la cantidad de datos generados en todo el mundo ha crecido exponencialmente, abriendo un mundo de posibilidades para la toma de decisiones en base informaci√≥n valiosa extra√≠da de los datos. En este contexto, la ciencia de datos se ha convertido en una disciplina fundamental para descubrir patrones, tendencias y relaciones ocultas en grandes conjuntos de datos."
  },
  {
    "objectID": "index.html#objetivos-del-libro",
    "href": "index.html#objetivos-del-libro",
    "title": "Introducci√≥n al an√°lisis estad√≠stico de datos con R",
    "section": "Objetivos del libro",
    "text": "Objetivos del libro\nEl objetivo principal de este libro es proporcionar a los lectores una gu√≠a pr√°ctica llevara a cabo un an√°lisis estad√≠stico en ciencia de datos basado en la metodolog√≠a Cross-Industry Standard Process for Data Mining (CRISP-DM), un modelo de proceso de miner√≠a de datos de est√°ndar abierto.\nCentrado en el uso del lenguaje de programaci√≥n R y el IDE RStudio, este libro pretende proporcionar a los lectores las herramientas necesarias para llevar a cabo un an√°lisis estad√≠stico con datos de contaminaci√≥n del aire en la ciudad de Madrid. El enfoque estar√° orientado hacia la reproducibilidad y la gobernanza del dato, garantizando as√≠ la integridad y la transparencia en el an√°lisis."
  },
  {
    "objectID": "index.html#estructura-del-libro",
    "href": "index.html#estructura-del-libro",
    "title": "Introducci√≥n al an√°lisis estad√≠stico de datos con R",
    "section": "Estructura del libro",
    "text": "Estructura del libro\nEl libro se divide en tres cap√≠tulos:\n\nIntroducci√≥n: las bases de un an√°lisis de datos con R.\nToolkit para la ciencia de datos con R.\nAn√°lisis de datos de contaminaci√≥n del aire en la ciudad de Madid."
  },
  {
    "objectID": "index.html#enfoque-del-manual",
    "href": "index.html#enfoque-del-manual",
    "title": "Introducci√≥n al an√°lisis estad√≠stico de datos con R",
    "section": "Enfoque del manual",
    "text": "Enfoque del manual\nEl enfoque te√≥rico-pr√°ctico adoptado en este libro permitir√° a los lectores adquirir los conocimientos fundamentales para aplicar eficazmente las t√©cnicas de miner√≠a de datos en el contexto de la contaminaci√≥n atmosf√©rica. Desde la comprensi√≥n de los fundamentos de CRISP-DM hasta la aplicaci√≥n de modelos predictivos y la validaci√≥n de los resultados, cada paso del proceso se aborda de forma clara y concisa.\nUno de los puntos fuertes de esta obra es su √©nfasis en la reproducibilidad y la gobernanza de los datos. Se ofrecen ejemplos pr√°cticos en R y RStudio que permiten a los lectores aplicar los conceptos te√≥ricos y seguir el proceso de an√°lisis de forma rigurosa y transparente. Adem√°s, se aborda el despliegue de los resultados a trav√©s de diferentes enfoques, como el desarrollo de una ShinyApp o la generaci√≥n de informes reproducibles.\n\n¬°Bienvenidos a este apasionante viaje de descubrimiento y an√°lisis de datos sobre contaminaci√≥n atmosf√©rica utilizando CRISP-DM y el potente lenguaje de programaci√≥n R!\nGema Fern√°ndez-Avil√©s Michal Kinel"
  },
  {
    "objectID": "index.html#licencia-y-copyright",
    "href": "index.html#licencia-y-copyright",
    "title": "Introducci√≥n al an√°lisis estad√≠stico de datos con R",
    "section": "Licencia y copyright",
    "text": "Licencia y copyright\nGu√≠a pr√°ctica para el an√°lisis de datos con R mediante la aplicaci√≥n de la metodolog√≠a CRISP-DM en R ¬© 2023 de Gema Fern√°ndez-Avil√©s y Michal Kinel est√° licenciado bajo una licencia de Creative Commons Reconocimiento-NoComercial-CompartirIgual 4.0 Internacional.\nTodos los logotipos y marcas comerciales que puedan aparecer en este texto son propiedad de sus respectivos due√±os y se incluyen en este texto √∫nicamente con fines did√°cticos."
  },
  {
    "objectID": "001-Introduccion.html#qu√©-vas-a-apernder",
    "href": "001-Introduccion.html#qu√©-vas-a-apernder",
    "title": "1¬† Introducci√≥n: las bases de un an√°lisis de datos con R",
    "section": "1.1 ¬øQu√© vas a apernder?",
    "text": "1.1 ¬øQu√© vas a apernder?\n\nLos conceptos b√°sicos para llevar a cabo un an√°lisis en ciencia de datos.\nUn overview de las herramientas m√°s importantes del software estad√≠stico R y su potencial no solo para el an√°lisis de datos.\nUna de las metodolog√≠as m√°s utilizada en ciencia de datos: la metodolog√≠a Cross Industry Standard Process for Data Mining (CRISP-DM por sus siglas en ingl√©s).\nC√≥mo desarrollar un caso de uso en ciencia de datos aplicando la metodolog√≠as CRISP-DM a un problema actual de importante repercusi√≥n econ√≥mica, sanitaria y social, el control de la calidad del aire."
  },
  {
    "objectID": "001-Introduccion.html#qu√©-no-vas-a-aprender",
    "href": "001-Introduccion.html#qu√©-no-vas-a-aprender",
    "title": "1¬† Introducci√≥n: las bases de un an√°lisis de datos con R",
    "section": "1.2 ¬øQu√© no vas a aprender?",
    "text": "1.2 ¬øQu√© no vas a aprender?\n\nProgramaci√≥n en R\nEscritura en LaTeX.\nContenidos te√≥ricos de estad√≠stica y ciencia de datos."
  },
  {
    "objectID": "001-Introduccion.html#qu√©-es-la-ciencia-de-datos",
    "href": "001-Introduccion.html#qu√©-es-la-ciencia-de-datos",
    "title": "1¬† Introducci√≥n: las bases de un an√°lisis de datos con R",
    "section": "1.3 ¬øQu√© es la ciencia de datos?",
    "text": "1.3 ¬øQu√© es la ciencia de datos?\nLa ciencia de datos es una disciplina emergente que ha sido etiquetada como la profesi√≥n m√°s sexy del siglo XXI (Davenport and Patil 2012). Mediante la combinaci√≥n de modelos matem√°ticos y estad√≠sticos, la programaci√≥n computacional y el conocimiento del negocio o √°rea de aplicaci√≥n (v√©ase Figure¬†1.1), la ciencia de datos obtiene el m√°ximo valor de los datos para apoyar los procesos de toma de decisiones. Es decir, transforma los datos en informaci√≥n y la informaci√≥n en conocimiento.\n\n\n\n\n\nFigura¬†1.1: Diagrama de Venn de la ciencia de datos"
  },
  {
    "objectID": "001-Introduccion.html#cu√°les-son-las-herramientas-b√°sicas-de-r-para-ciencia-de-datos",
    "href": "001-Introduccion.html#cu√°les-son-las-herramientas-b√°sicas-de-r-para-ciencia-de-datos",
    "title": "1¬† Introducci√≥n: las bases de un an√°lisis de datos con R",
    "section": "1.4 ¬øCu√°les son las herramientas b√°sicas de R para ciencia de datos?",
    "text": "1.4 ¬øCu√°les son las herramientas b√°sicas de R para ciencia de datos?\nR es una caja de herramientas en el sentido m√°s amplio (tal y com ilustra la Figure¬†1.2), pues permite llevar a cabo muchas aplicaciones y muy diferentes. Entre ellas, para hacer una idea al lector, con R es posible: sumar o restar, ejecutar modelos ya programados, desarrollar funciones nuevas, dibujar datos y resultados de forma est√°tica y/o interactiva, crear informes y fomentar la reproducibilidad, dise√±ar y publicar blogs, p√°ginas webs, dashboards, cuadros de mando, etc‚Ä¶\n\n\n\n\n\nFigura¬†1.2: La caja de herramientas de R\n\n\n\n\nR tiene tres grandes pilares que son:\n\nRstudio: un entorno de desarrollo integrado (IDE) para R (y Python) dedicado a la computaci√≥n estad√≠stica y gr√°ficos. Incluye (Rstudio):\n\nuna consola,\nun editor sintaxis en color que apoya la ejecuci√≥n de c√≥digo,\nun entrono para las variables y\nun conjunto de utilidades.\n\nTidyverse: una colecci√≥n de paquetes coherentes, que comparten gram√°tica, filosof√≠a y estructura y est√°n dise√±ados para realizar juntos como una canalizaci√≥n completa (pipeline). Todos se basan en la idea de tidy data propuesta por Hadley Wickham\n\ny pueden instalarse con un √∫nico comando en R:\n\n\n\ninstall.packages(\"tidyverse\")\n\nLos paquetes que forman parte del tidyverse son:\n\nreadr: importaci√≥n de datos.\ndplyr: manipulaci√≥n de datos.\ntidyr: ordenaci√≥n de datos.\nggplot2 visualizaci√≥n de datos.\npurrr: programaci√≥n.\ntibble: para tibbles, un nuevo formato de data frames.\nstringr: para caracteres.\nforcats: para factores.\n\n\nQuarto/Rmarkdown: un marco de escritura para ciencia de datos, que combina c√≥digo, resultados y comentarios. Los documentos de Quarto (*.qmd) y R Markdown (*.Rmd) son completamente reproducibles y soportan docenas de formatos de salida tales como PDFs, archivos de Word, presentaciones, art√≠culos cient√≠ficos,‚Ä¶ Materiral de recomendable lectura R Markdown Cookbook, Rmarkdown y el nuevo formato multiplataforma integrado de Quarto\n\n\n\n\n\n\n\nNote\n\n\n\nLibro de referencia obligatoria: R for Data Science (en espa√±ol)"
  },
  {
    "objectID": "001-Introduccion.html#fundamentos",
    "href": "001-Introduccion.html#fundamentos",
    "title": "1¬† Introducci√≥n: las bases de un an√°lisis de datos con R",
    "section": "1.5 Importancia de una metodolog√≠a en ciencia de datos",
    "text": "1.5 Importancia de una metodolog√≠a en ciencia de datos\nLa metodolog√≠a en la ciencia de datos proporciona un camino para encontrar soluciones a un problema espec√≠fico. Este es un proceso c√≠clico que sufre un comportamiento cr√≠tico y que gu√≠a a los analistas de negocios y cient√≠ficos de datos a actuar en consecuencia.\nCRISP-DM es un modelo est√°ndar ampliamente utilizado en la miner√≠a de datos que proporciona un enfoque estructurado para llevar a cabo proyectos de an√°lisis de datos (Wirth and Hipp (2000)). Este modelo se compone de seis fases interconectadas, que abarcan desde la comprensi√≥n del negocio hasta la implementaci√≥n de los resultados del an√°lisis. Las fases de CRISP-DM son:\n\nEntendimiento del negocio.\nComprensi√≥n de los datos.\nPreparaci√≥n de los datos.\nModelado.\nEvaluaci√≥n.\nDespliegue.\n\n\n\n\n\n\nCiclo de vida de un proceso de ciencia de datos"
  },
  {
    "objectID": "001-Introduccion.html#caso-de-estudio-la-calidad-del-aire-en-la-ciudad-de-madrid",
    "href": "001-Introduccion.html#caso-de-estudio-la-calidad-del-aire-en-la-ciudad-de-madrid",
    "title": "1¬† Introducci√≥n: las bases de un an√°lisis de datos con R",
    "section": "1.6 Caso de estudio: la calidad del aire en la ciudad de Madrid",
    "text": "1.6 Caso de estudio: la calidad del aire en la ciudad de Madrid\nLa contaminaci√≥n del aire exterior es uno de los principales problemas que afectan a la salud humana en las zonas urbanas de todo el mundo (Sanchis-Marco, Montero, and Fernandez-Aviles (2022)). Aunque las emisiones de la mayor√≠a de los contaminantes del aire han disminuido sustancialmente en las √∫ltimas d√©cadas, sus concentraciones a√∫n superan los l√≠mites legales en la mayor√≠a de los pa√≠ses, lo que indica que el control de la contaminaci√≥n del aire sigue siendo un desaf√≠o para las sociedades modernas. Cada a√±o, m√°s de 4,2 millones de personas sufren una muerte prematura a causa de la contaminaci√≥n del aire exterior (OMS, 2016). Los principales culpables son el ozono (O3), el di√≥xido de nitr√≥geno (NOx) y, sobre todo, las part√≠culas finas o material particulado (PM) con un di√°metro de 10 micr√≥metros o menos (PM10).\nPor otra parte, Madrid es la tercera ciudad m√°s poblada de la Uni√≥n Europea despu√©s de Londres y Berl√≠n y cuenta con una gran √°rea metropolitana perif√©rica con m√°s de cinco millones de habitantes. Su potente actividad econ√≥mica, incluso en tiempos de pandemia de la Covid-19, se traduce en niveles de PM10 y NO2 superiores a los deseados a causa del transporte ‚Äîen concreto, del tr√°fico rodado‚Äî y de la actividad industrial, que son las principales fuentes de emisi√≥n de PM10 (v√©ase Montero and Fern√°ndez-Avil√©s (2018) y Montero, Fern√°ndez-Avil√©s, and Laureti (2021)).\nEn este contexto, el an√°lisis de datos desempe√±a un papel crucial en la comprensi√≥n de la contaminaci√≥n del aire y sus efectos en la salud humana y el medio ambiente. La disponibilidad de datos geoespaciales permite identificar patrones espaciales, detectar √°reas de alta contaminaci√≥n y evaluar el impacto de las emisiones en diferentes regiones. Resulta muy importante la f√°cil accesibilidad de los datos para la democratizaci√≥n de la informaci√≥n. Un ejemplo de ello es la p√°gina de Datos abiertos del Gobierno de Espa√±a. Gracias a los datos p√∫blicos y el an√°lisis adecuado se facilita la toma de decisiones informadas para abordar y mitigar los problemas de contaminaci√≥n del aire.\nLa mejor forma para comprender y entender nuevos desarrollos es viendo su utilidad a trav√©s de un caso de uso, y en este manual se ha escogido un tema tan importante como es la contaminaci√≥n del aire en la ciudad de Madrid. Ello qued√≥ de manifiesto los d√≠as 14-16 de marzo de 2022 cuando una gran calima cubri√≥ no s√≥lo a Madrid (Figure¬†1.3) sino a Espa√±a entera ti√±iendo el pa√≠s de polvo rojo.\n\n\n\n\n\nFigura¬†1.3: Calidad del aire 15-Marzo-2022\n\n\n\n\nAdem√°s, el Ayuntamiento de Madrid dispone datos abiertos proporcionados a traves delPortal de datos abiertos del Ayuntamiento de Madrid que permiten su an√°lisis sin coste alguno.\nConcretamente, a trav√©s del Sistema Integral de la Calidad del Aire del Ayuntamiento de Madrid (v√©ase Figure¬†1.4), se pueden descargar los datos de los contaminantes registrados en las estaciones de monitoreo de la ciudad desde 2001 en distintos formatos.\n\n\n\n\n\nFigura¬†1.4: Web Ayuntamiento de Madrid\n\n\n\n\nTodo hace que quede m√°s que justificado el inter√©s del caso de estudio que se presenta.\n\n1.6.1 Los datos\nDe acuerdo con Wickham and Grolemund (2016), el cual es considerada una referencia clave en el la ciencia de datos (Data Science with R), las herramientas necesarias para un proyecto t√≠pico de ciencia de datos sigue el siguiente esquema: importar, ordenar, transformar, visualizar, modelar y comunicar (v√©ase Figure¬†1.5)\n\n\n\n\n\nFigura¬†1.5: Ciclo de vida de la ciencia de datos\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPartir con la ingesta y orden de los datos no ser√≠a optimo para llevar a cabo el objetivo de este manual, pues el 80% del tiempo es un proceso rutinario y aburrido y el 20% restante es extra√±o y frustrante.\n\n\nEn el caso que nos ocupa, se recurre al repositorio de Github michal0091/aire_madrid de Kinel (2022a, 2022b) que trata los datos brutos de calidad del aire del Ayuntamiento de Madrid, los organiza y los codifica, facilitando as√≠ su uso (Figure¬†1.6). La salida del conjunto de datos dt_daily_mean_2011.RDS que aqu√≠ se proporciona ser√° la entrada para el an√°lisis que se presenta.\n(TODO) descripci√≥n breve de lo que contienen estas carpetas, datos procesasdos, sin procear, diccionarios.\n\n\n\n\n\nFigura¬†1.6: Repositorio Github con el c√≥digo\n\n\n\n\n\n\n\n\nDavenport, Thomas H, and DJ Patil. 2012. ‚ÄúData Scientist.‚Äù Harvard Business Review 90 (5): 70‚Äì76.\n\n\nMontero, Jos√©-Marƒ±ÃÅa, and Gema Fern√°ndez-Avil√©s. 2018. ‚ÄúFunctional Kriging Prediction of Atmospheric Particulate Matter Concentrations in Madrid, Spain: Is the New Monitoring System Masking Potential Public Health Problems?‚Äù Journal of Cleaner Production 175: 283‚Äì93.\n\n\nMontero, Jos√©-Marƒ±ÃÅa, Gema Fern√°ndez-Avil√©s, and Tiziana Laureti. 2021. ‚ÄúA Local Spatial STIRPAT Model for Outdoor NOx Concentrations in the Community of Madrid, Spain.‚Äù Mathematics 9 (6): 677.\n\n\nSanchis-Marco, Lidia, Jos√©-Marƒ±ÃÅa Montero, and Gema Fernandez-Aviles. 2022. ‚ÄúAn Extended CAViaR Model for Early-Warning of Exceedances of the Air Pollution Standards. The Case of PM10 in the City of Madrid.‚Äù Atmospheric Pollution Research 13 (4): 101355.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O‚ÄôReilly Media, Inc.\".\n\n\nWirth, R√ºdiger, and Jochen Hipp. 2000. ‚ÄúCRISP-DM: Towards a Standard Process Model for Data Mining.‚Äù In Proceedings of the 4th International Conference on the Practical Applications of Knowledge Discovery and Data Mining, 1:29‚Äì39. Manchester."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#r-y-rstudio",
    "href": "002-toolkit-ciencia-datos.html#r-y-rstudio",
    "title": "2¬† R toolkit para la ciencia de datos",
    "section": "2.1 R y RStudio",
    "text": "2.1 R y RStudio\n\n2.1.1 Instalaci√≥n de R y RStudio\nR es un entorno de software libre para la computaci√≥n estad√≠stica y los gr√°ficos. Se compila y ejecuta en una amplia variedad de plataformas UNIX, Windows y MacOS. Las fuentes, los binarios y la documentaci√≥n de R pueden obtenerse a trav√©s de CRAN1. Para descargar R elija su espejo CRAN preferido en www.r-project.org.\nRStudio es un entorno de desarrollo integrado (IDE) para R. Incluye una consola, un editor que resalta la sintaxis y admite la ejecuci√≥n directa del c√≥digo, as√≠ como herramientas para el trazado, el historial, la depuraci√≥n y la gesti√≥n del espacio de trabajo. RStudio est√° disponible en ediciones comerciales y de c√≥digo abierto y se ejecuta en el escritorio (Windows, Mac y Linux) o en un navegador conectado a RStudio Server o RStudio Workbench (Debian/Ubuntu, Red Hat/CentOS y SUSE Linux), est√° disponible en www.rstudio.com.\n\n\n\n\n\n\nNote\n\n\n\nEl orden es importante:\n\nInstale R.\nInstale RStudio.\n\n\n\nCompruebe que tiene instalados los iconos de la Figure¬†2.1 y abra RStudio.\n\n\n\n\n\nFigura¬†2.1: Iconos de R y RStudio\n\n\n\n\n\n\n2.1.2 Interfaz de RStudio\nAl abrir el programa se observan cuatro paneles (v√©ase Figure¬†2.2).\n\nSripts: para escribir c√≥digo.\nConsola: contiene la Consola, Terminal y los Jobs.\nEntorno: contiene el Environment, History, conexiones y Git, entre otros.\nFiles, Plots, Packages, Help y Viewer\n\nLa posici√≥n de los paneles se puede personalizar al gusto del usuario, al igual que la apariencia del IDE.\n\n\n\n\n\nFigura¬†2.2: Paneles de RStudio\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa posici√≥n relativa y el contenido de cada panel pueden personalizarse desde el men√∫: Tools &gt; Global Options &gt; Pane Layout\n\n\n\n\n2.1.3 Instalaci√≥n los paquetes\nLos paquetes son adiciones modulares al software R que a√±aden funcionalidad en forma de nuevas funciones, conjuntos de datos, documentaci√≥n, etc. El repositorio est√°ndar de paquetes R es ‚ÄúComprehensive R Archive Network‚Äù (CRAN).\n\n\n\n\n\n\nNote\n\n\n\nCRAN Task Views:\nOfrecen un breve resumen de los paquetes incluidos.\n\n\nLos paquetes se instalan con la funci√≥n base de R install.packages():\n\ninstall.packages(\"tidyverse\")\n\n\n\n\n\n\n\nNote\n\n\n\nLos paquetes se instalan una √∫nica vez y se leen cada vez que se utilizan.\n\n\nAlternativamente, se pueden instalar desde el panel de Paquetes en RStudio desde una interfaz gr√°fica.\n\n\n2.1.4 Proyectos\nSer organizado desde el principio es uno de los mejores h√°bitos y en R esto se consigue trabajando con proyectos. Un proyecto de R agrupa todo los ficheros del trabajo en una carpeta de forma que se facilita su manejo, el trabajo colaborativo y su reproducibilidad.\n\n\n\n\n\n\nNote\n\n\n\nCrear un proyecto Quarto en R:\nDesde el men√∫: File &gt; New Project y luego elija:\n\nNuevo Directorio\nProyecto Vac√≠o\nElija un nombre para el directorio, por ejemplo, mi_proyecto\nHaga clic en Crear Proyecto.\n\n\n\n\nüìÅ data: para los datos.\nüìÅ img: para las im√°genes.\nüìÇ src: contiene los scripts de R para llevar a cabo las distintas tareas del an√°lisis.\nüìÅ output: se guardar√°n los resultados finales de nuestro an√°lisis.\nREADME.md debe contener la informaci√≥n b√°sica sobre el proyecto, la compatibilidad, los inputs que necesita, los outputs que genera y un resumen de flujo del trabajo. √âsto ayudar√° a cualquiera a comprender mejor c√≥mo es nuestro proyecto.\n\nUna de las estructuras m√°s comunes es la mostrada en la Figure¬†2.3:\n\n\n\n\n\nFigura¬†2.3: Ejemplo de estructura de un proyecto en R.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLos distintos scripts se pueden ejecutar desde otros scripts. Para ello se utiliza la funci√≥n source()\n\n\n\n\n2.1.5 Trabajar con un script de R\nPara comenzar a escribir un script nuevo nos dirigimos a File &gt; New File. Los m√°s utilizados actualmente son:\n\nüìì &gt; R Script.\nüìñ &gt; Quarto document.\nüìì &gt; R Notebook.\nüìù &gt; R Markdown.\n\n\n\n\n\n\n\nNote\n\n\n\nSi se tiene creado un proyecto Quarto mejor trabajar con un un quarto document para luego renderizar todo el documento. V√©ase Wickham and Grolemund (2016) para un estudio profundo de proyectos.\n\n\nEl editor es un editor de texto plano (sin negritas ni cursivas) pero ofrece un c√≥digo de colores del texto dependiendo de lo que se escriba (resaltado de sintaxis).\nEs extremadamente √∫til comentar el script con informaci√≥n adicional para hacer m√°s claro el proceso de programaci√≥n. Para a√±adir un comentario, simplemente hay que poner # al comienzo de la linea y lo que est√© a la derecha no ser√° ejecutado.\nFinalmente, a la hora de escribir c√≥digo en R lo m√°s conveniente es usar las buenas pr√°cticas dise√±adas por programadores expertos. Para ello, lo mejor es aplicar una gu√≠a de estilo üìñ, cuyo objetivo es hacer que nuestro c√≥digo R sea m√°s f√°cil de leer, compartir y verificar. Una de las mejores gu√≠as es The tidyverse style guide."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#tidy",
    "href": "002-toolkit-ciencia-datos.html#tidy",
    "title": "2¬† R toolkit para la ciencia de datos",
    "section": "2.2 El ecosistema tidyverse",
    "text": "2.2 El ecosistema tidyverse\n\n\n\n\n\nEl tidyverse es una colecci√≥n de paquetes de R para la ciencia de los datos. Todos los paquetes comparten una filosof√≠a de dise√±o, una gram√°tica y unas estructuras de datos subyacentes. Se basan en la idea tidy data propuesta por Hadley Wickham (Wickham et al.¬†2014) y pueden instalarse con una √∫nica orden en R:\n\ninstall.packages(\"tidyverse\")\n\nUna vez instalado, se cargan con la funci√≥n library:\n\nlibrary(tidyverse)\n\nLos paqu√©tes m√°s importantes en ciencia de datos se enumeran a continuaci√≥n.\n\n\n\n\n\n\n\n\n\n\n2.2.1 readr\nEl paquete readr roporciona una forma r√°pida y amigable de leer datos rectangulares (como csv, tsv y fwf). Est√° dise√±ado para analizar de forma flexible muchos tipos de datos que se encuentran en la naturaleza, mientras que sigue fallando limpiamente cuando los datos cambian inesperadamente.\nreadr admite los siguientes formatos de archivo asociados a estas funciones read_*():\n\nread_csv(): valores separados por coma, ficheros (CSV).\nread_tsv(): valores separados por tabulador, fichero (TSV).\nread_delim(): fichro delimitados (CSV y TSV son casos especiales).\nread_fwf(): archivos de ancho fijo.\nread_table(): archivos separados por espacios en blanco\nread_log(): archivos de logos web.\n\n\nLea la base de datos de calidad del aire de la ciudad de Madrid , air_mad.RDS, contenida en la carpeta data.\n\n(TODO): QUITAR LA FUNCI√ìN HERE PORQUE A ESTE NIVEL SI NO SE EXPLICA PUEDE LIAR\n\nair_mad &lt;- readr::read_rds(\"data/air_mad.RDS\")\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 dplyr\nEl paquete dplyr Ofrece una gram√°tica de la manipulaci√≥n de datos, proporcionando un conjunto coherente de verbos que resuelven los problemas m√°s comunes de manipulaci√≥n de datos y que pueden ser organizado tres categor√≠as basado en el dataset:\n\nFilas:\n\n\nfilter(): elige filas en funci√≥n de los valores de la columna.\nslice(): elige filas en funci√≥n de la ubicaci√≥n.\narrange(): cambia el orden de las filas.\n\n\nColumnas:\n\n\nselect(): indica cuando una columa es incluida o no.\nrename(): cambia el nombre de la columna.\nmutate(): cambia los valores de las columnas y crea nuevas columnas.\nrelocate(): cambia el orden de las columnas.\n\n\nGrupos de filas:\n\n\nsummarise(): contrae un grupo en una sola fila.\n\n\n\n\n\n\n\nEl operador pipe\n\n\n\nCanaliza la salida de una funci√≥n a la entrada de otra funci√≥n.\n\nsegundo(primero(datos))\n\nse traduce en:\n\ndatos %&gt;% \n  primero %&gt;% \n  segundo\n\nOperador pipe de {maggrit} %&gt;% Operador pipe de R base |&gt;\n\n\n\n\n\n\n\n\nEjemplo. Medidas de posici√≥n por estaci√≥n de monitoreo\n\n\n\nCalcule las medidas de posici√≥n (m√≠nimo, m√°ximo, Q1, Q3, media y mediana) por estaci√≥n de monitoreo , agrupando por id_name para Part√≠culas &lt; 2.5 ¬µm nom_abv == \"PM2.5\" utilizando las funciones de la libreria dplyr.\n\n\n\nair_mad %&gt;% # Summary por grupo usando dplyr \n  na.omit() %&gt;% # omitimos los NAs para el an√°lisis\n  filter(nom_abv == \"PM2.5\") %&gt;% # filtramos por PM2.5\n  group_by(id_name) %&gt;% \n  summarize(min = min(valor),\n            q1 = quantile(valor, 0.25),\n            median = median(valor),\n            mean = mean(valor),\n            q3 = quantile(valor, 0.75),\n            max = max(valor))\n\n# A tibble: 8 √ó 7\n  id_name            min    q1 median  mean    q3   max\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Casa de Campo        0     5      8  9.21    12   850\n2 Castellana           0     6      9  9.66    12   195\n3 Cuatro Caminos       0     6      9 10.2     13    59\n4 Escuelas Aguirre     0     7     10 11.1     14   137\n5 M√©ndez √Ålvaro        0     6      9 10.2     13    78\n6 Plaza Castilla       0     6      8  9.59    12   273\n7 Plaza El√≠ptica       0     7     10 11.0     14    61\n8 Sanchinarro          0     5      7  8.36    10    68\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 tidyr\nEl paquete tidyr proporciona un conjunto de funciones que le ayudan a obtener datos ordenados. Los datos ordenados son datos con una forma consistente. En resumen, cada variable va en una columna, y cada columna es una variable.\nLas funciones se agrupan en 5 grandes categor√≠as:\n\nPivotar. Convierte entre formas largas y anchas, pivot_longer() y pivot_wider()\nRectangling. Convierte listas profundamente anidadas (a partir de JSON) en tibbles ordenados: unnest_longer(), unnest_wider(), hoist().\nAnidar. Convierte los datos agrupados en una forma en el que cada grupo se convierte en una sola fila que contiene un data frame anidado nest() y viceversa, unnest().\nDividir y combinar columnas de caracteres. Funciones: separate(), extract() y unite()\nValores perdidos. Funciones: complete(), drop_na(), fill(), replace_na()\n\n\n\n\n\n\n\nEjemplo. ¬øCu√°l es el d√≠a con mayor y menor concentraci√≥n de NOx de todo el periodo?\n\n\n\n\n\n\n\nair_mad %&gt;% # Summary por grupo usando dplyr \n  na.omit() %&gt;% # omitimos los NAs para el an√°lisis\n  filter(nom_abv == \"NOx\") %&gt;% # filtramos por NOx\n  group_by(fecha) %&gt;% # agrupamos por fecha\n  summarize(mad_mean = mean(valor)) %&gt;% # promedio de las estaciones\n  slice(which.max(mad_mean), which.min(mad_mean)) #seleccionamos el m√°ximo y el m√≠nimo \n\n# A tibble: 2 √ó 2\n  fecha      mad_mean\n  &lt;date&gt;        &lt;dbl&gt;\n1 2013-08-10  4086.  \n2 2020-05-10     6.38\n\n\nEl valor m√°ximo, 415 ¬µg/m3 de NOx, se observa el 21 de diciembre de 2011 y el valor m√≠nimo, 6,32 ¬µg/m3 de NOx, el 10 de mayo de 2020, en pleno estado de alarma.\n\n\n\n\n\n\n\n\n\n\n\n2.2.4 ggplot2\nEl paquete ggplot2 es un sistema para crear gr√°ficos de forma ordenada, basado en The Grammar of Graphics. Al proporcionar los datos, le dice a ggplot2 c√≥mo asignar a las variables la est√©tica y qu√© gr√°ficas utilizar.\nEl template b√°sico para la funci√≥n ggplot() es:\n\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nLos argumentos m√°s importantes para la representaci√≥n de gr√°ficos son:\n\nggplot(), crea un nuevo gr√°fico.\naes(), construye la est√©tica el gr√°fico.\n+(&lt;gg&gt;), a√±ade componentes al gr√°fico.\nggsave(), guarda el gr√°fico.\n\n\n\n\n\n\n\nEjemplo. Gr√°fico de viol√≠n.\n\n\n\nRepresente, con un gr√°fico de viol√≠n, las concentraciones de NOx por a√±o en el periodo estudiado.\n\n\nUna buena forma de ver los datos es por medio de un gr√°fico de viol√≠n:\n\nair_mad %&gt;% # Summary por grupo usando dplyr \n  na.omit() %&gt;% # omitimos los NAs para el an√°lisis\n  filter(nom_abv == \"NOx\") %&gt;% # filtramos por NOx\n  group_by(year = format(fecha, \"%Y\")) %&gt;% \n  summarise(valor)  %&gt;% \n  ggplot(aes(factor(year), valor))+\n  geom_violin() +\n  geom_jitter(height = 0, width = 0.01) +\n  aes(x=factor(year), y=valor, fill=year)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLibro de referencia obligatoria: ggplot2: elegant graphics for data analysis.\nEs interesante conocer las Extensiones de ggplot2 as√≠ como las ggplot2 Cheat sheet disponible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.5 purrr\nEl paquete purrr mejora el conjunto de herramientas de programaci√≥n funcional (PF) de R proporcionando un conjunto completo y coherente de herramientas para trabajar con funciones y vectores. Una vez que se dominan los conceptos b√°sicos, purrr permite sustituir muchos bucles for por un c√≥digo m√°s f√°cil de escribir y m√°s expresivo.\n\n\n\n\n\n\nNote\n\n\n\nConsulte la purrr Cheat sheet disponible para una descripcion de la librer√≠a.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.6 tibble\nEl paquete tibble es una reimaginaci√≥n moderna del marco de datos, manteniendo lo que el tiempo ha demostrado que es eficaz, y desechando lo que no. Los Tibbles suelen conducir a un c√≥digo m√°s limpio y expresivo.\n\n\n\n\n\n\nTip\n\n\n\nConsulte el cap√≠tulo 10 de R for data science disponible online para una primera aproximaci√≥n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.7 stringr\nEl paquete stringr proporciona un conjunto cohesivo de funciones dise√±adas para facilitar al m√°ximo el trabajo con cadenas.\n\n\n\n\n\n\nTip\n\n\n\nConsulte el cap√≠tulo 14 de R for data science disponible online para una primera aproximaci√≥n.\n\n\n\n\n\n\n\n\n\n2.2.8 forcats\nEl paquete forcats proporciona un conjunto de herramientas √∫tiles que resuelven problemas comunes con los factores. R utiliza factores para manejar variables categ√≥ricas, variables que tienen un conjunto fijo y conocido de valores posibles.\n\n\n\n\n\n\nNote\n\n\n\nConsulte el cap√≠tulo 14 de R for data science disponible online para una primera aproximaci√≥n."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#quarto",
    "href": "002-toolkit-ciencia-datos.html#quarto",
    "title": "2¬† R toolkit para la ciencia de datos",
    "section": "2.3 Informes reproducibles con Quarto/Rmarkdown",
    "text": "2.3 Informes reproducibles con Quarto/Rmarkdown\nEste es un documento R Quarto. Quarto es una sintaxis simple de formato para la creaci√≥n de documentos HTML, PDF y MS Word. Para obtener m√°s detalles sobre c√≥mo usar R Quarto consulta https://quarto.org/.\nRMarkdown (el antecesor de Quarto) y Quarto son herramientas permiten combinar texto formateado con c√≥digo y resultados en un mismo documento. Quarto facilita la interoperabilidad entre R, Python, Julia.\nLos documentos Quarto constan principalmetne de 3 partes: + el YAML (o cabecera del documento),\n\nel texto (cuerpo escrito del documento),\nlas chunks (tozos de c√≥digo, en este caso, c√≥digo R)\n\nA continuaci√≥n se exponen, brevemente, las principales ideas para entender y trabajar sintaxis quarto.\n\n2.3.1 Introducci√≥n\n\n2.3.1.1 Markdown: un lenguaje de marcado ligero\nMarkdown es un lenguaje de marcado ligero y f√°cil de aprender que permite dar formato al texto de manera sencilla. Con Markdown, es posible aplicar negrita, cursiva, encabezados, listas y enlaces, entre otras opciones de formato, simplemente utilizando una sintaxis intuitiva. La facilidad de uso del lenguaje Markdown hace que sea ampliamente utilizado, sobre todo para la documentaci√≥n.\n\n\n2.3.1.2 Integraci√≥n de c√≥digo y resultados\nUna de las caracter√≠sticas principales de Quarto/Rmarkdown es la capacidad de integrar c√≥digo en el informe. Esto significa que se pueden incluir bloques de c√≥digo o chunks en lenguajes como R, Python o Julia directamente en el documento. Estos bloques de c√≥digo se ejecutan en tiempo real y los resultados, como tablas o gr√°ficos, se incrustan autom√°ticamente en el informe. Esto permite a los lectores ver tanto el c√≥digo como los resultados generados, lo que promueve la transparencia y la reproducibilidad.\n\n\n2.3.1.3 Generaci√≥n de m√∫ltiples formatos de salida\nQuarto ofrece la posibilidad de generar informes en diferentes formatos de salida, como por ejemplo: HTML, PDF, Word o presentaciones de diapositivas. Esto es especialmente √∫til cuando se necesita compartir el informe con diferentes audiencias o cuando se requiere una presentaci√≥n visualmente atractiva. La conversi√≥n entre formatos se realiza de manera autom√°tica y se puede personalizar para que se adapte a las necesidades espec√≠ficas de cada proyecto.\n\n\n2.3.1.4 Facilidad de uso y colaboraci√≥n\nQuarto se integra con RStudio, lo que proporciona una experiencia de usuario fluida y amigable. Adem√°s, los archivos Rmarkdown son archivos de texto plano, lo que hace que sean f√°ciles para el control de versiones como Git. Esto facilita el trabajo en equipo y la colaboraci√≥n en proyectos.\n\n\n\n\n\n\nTip\n\n\n\nLa mayor√≠a de los paquetes tiene una amplia documentaci√≥n y los llamados Cheatsheets (chuletas). Son un buen m√©todo para ampliar los conocimientos. Algunas de ellas son: - Documentaci√≥n de Quarto - Cheatsheet de RMarkdown - Colecci√≥n de otros cheatsheets de RStudio\n\n\n\n\n\n2.3.2 Instalaci√≥n\nPara poder utilizar Quarto/Rmarkdown en R y RStudio, es necesario realizar algunos pasos de instalaci√≥n. A continuaci√≥n, se detallan los pasos necesarios:\n\nInstalar R (v√©ase (instal-r-rstudio?)).\nInstalar RStudio (v√©ase (instal-r-rstudio?)).\nIntalar Quarto: para ello hay que dirigirse a la p√°gina oficial de Quarto, acceder al apartado Get Started y seleccionar la instalaci√≥n acorde al sistema operativo. POsteriormente hay que seguir los pasos del instalador.\nInstalar el paquete Quarto: Una vez que R, RStudio y Quarto, se debe instalar la librer√≠a de Quarto para habilitar la funcionalidad de Quarto en R y RStudio. Para ello, se puede ejecutar el siguiente comando en la consola de RStudio: install.packages(\"quarto\")\n\nDespu√©s de la instalaci√≥n, se debe configurar RStudio para utilizar Quarto como el motor de renderizado de Rmarkdown. Para ello, se debe abrir la pesta√±a Tools (Herramientas) en la barra de men√∫ de RStudio y seleccionar Global Options (Opciones globales). En la ventana emergente, selecciona la pesta√±a R Markdown y aseg√∫rate de que la opci√≥n Quarto est√© seleccionada en la secci√≥n Render.\nCon estos pasos completados, ya se puede comenzar a utilizar Quarto en RStudio para crear informes reproducibles. Se puede crear un nuevo archivo Rmarkdown y comenzar a combinar texto y c√≥digo en un solo documento.\n\n\n2.3.3 El YAML\nLa sintaxis b√°sica de YAML utiliza pares clave-valor en el formato clave: valor. Otros campos YAML com√∫nmente encontrados en los encabezados de los documentos incluyen metadatos como author (autor), subtitle (subt√≠tulo), date (fecha), as√≠ como opciones de personalizaci√≥n como theme (tema), fontcolor (color de la fuente), fig-width (ancho de la figura), etc. Se puede encontrar informaci√≥n sobre todos los campos YAML disponibles para documentos HTML en la p√°gina oficial de Quarto. Los campos YAML disponibles var√≠an seg√∫n el formato del documento, por ejemplo,los campos YAML para documentos PDF y para MS Word.\n---\ntitle: \"My Document\"\nauthor:\n    - Gema Fern√°ndez-Avil√©s\n    - Michal Kinel\nformat: \n  html:\n    toc: true\n    code-fold: true\n---\n\n\n\n2.3.4 Flujo de trabajo y el concepto renderizar\nUn informe en Quarto/Rmarkdown se compone de bloques de texto y bloques de c√≥digo. El texto se escribe en formato Markdown. Los bloques de c√≥digo, chunks se delimitan con etiquetas especiales y pueden contener c√≥digo en R, Python u otros lenguajes compatibles.\n\n\n\n\n\n\nNote\n\n\n\nrenderizar en RMarkdown/Quarto se refiere al proceso de transformar un documento escrito en lenguaje Markdown, que combina texto y c√≥digo, en un formato final legible y presentable, como un informe, un documento PDF o una p√°gina web interactiva.\nEl proceso de renderizado se realiza mediante la ejecuci√≥n de un motor de renderizado, como Quarto, que interpreta el documento RMarkdown y produce el resultado final en el formato deseado, como un archivo HTML, PDF, Word, entre otros. Durante el proceso de renderizado, el motor ejecuta el c√≥digo R, genera los gr√°ficos y tablas correspondientes, y aplica el formato y estilo definidos en el documento.\n\n\nLa mejor manera de tejer un html_document es mediante el bot√≥n Render o la funci√≥n quarto::quarto_render().\n\n\n\n\n\nDiagrama de transformaci√≥n de un documento Quarto a HTML, PDF, etc.\n\n\n\n\n\n2.3.4.1 Trozos de c√≥digo chunks\nLos bloques de c√≥digo en Quarto/Rmarkdown se utilizan para incorporar c√≥digo fuente en el informe. Estos bloques se delimitan mediante etiquetas especiales, como {r} para c√≥digo en R o {python} para c√≥digo en Python. Dentro de estos bloques, se puede escribir y ejecutar c√≥digo de la misma manera que se har√≠a en un entorno de programaci√≥n normal.\nLa ejecuci√≥n del c√≥digo se realiza en tiempo real, lo que significa que los resultados generados, como tablas, gr√°ficos o textos formateados, se incrustan directamente en el informe cuando se renderiza. Esto permite a los lectores ver los resultados obtenidos y verificar la reproducibilidad del an√°lisis.\nSe puede insertar r√°pidamente bloques de c√≥digo con el comando Agregar Bloque en la barra de herramientas del editor o escribiendo los delimitadores de bloque:\n\n2+2\n\n[1] 4\n\n\nLa salida de una chunk se puede personalizar con las opciones de knitr. Algunas de las m√°s usadas son:\n\n|# include: false evita que el c√≥digo y los resultados aparezcan en el archivo final. R Markdown sigue ejecutando el c√≥digo en el chunk, y los resultados pueden ser utilizados por otros chunks.\n|# echo : false evita que el c√≥digo, pero no los resultados, aparezcan en el archivo final. Es una forma √∫til de incrustar cifras.\n|# message : false evita que los mensajes generados por el c√≥digo aparezcan en el archivo final.\n|# warning : false evita que las advertencias generadas por el c√≥digo aparezcan en el acabado.\n|# fig.cap : \"...\" a√±ade una leyenda a los resultados gr√°ficos.\n\nM√°s informaci√≥n sobre la opci√≥n de trozos de c√≥digo: Quarto chunks.\n\n\n2.3.4.2 Formato de texto\nEl texto en formato Markdown se utiliza para proporcionar explicaciones, descripciones y contexto en el documento Escribir texto en Markdown es sencillo, ya que no requiere conocimientos avanzados de programaci√≥n. Con Markdown, se pueden aplicar diferentes estilos de formato utilizando una sintaxis sencilla y legible.\nSe pueden a√±adir enlaces, escribir en negrita o cursiva. Consulta la hoja de trucos de Rstudio descargar aqu√≠.\n\nHacer citas\n\nbloque de l√≠nea\nOtros formatos de texto:\n\nsubrayado\ntachar\nsuper√≠ndice\nsub√≠ndice\nversalitas\n\nLa forma de a√±adir notas a pie de p√°gina es [^number]2\nTambi√©n se puede a√±adir emojis incre√≠bles como üòª insert√°ndolo en el editor visual markdown o mediante su c√≥digo :heart_eyes_cat:.\n\nA veces es necesario separar el texto con una l√≠nea horizontal, para ello simplemente se introduce ***\n\no saltar una l√≠nea con c√≥digo html &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;.\n\n\n2.3.4.3 A√±adir una imagen\nPara a√±adir im√°genes basta con escribir ![image_name](image_path)\n\n\n\nR learners. Cr√©ditos: Allison Horst\n\n\nSi desea centrar la imagen, un poco de c√≥digo htm le ayudar√°, s√≥lo tiene que utilizar &lt;center&gt; &lt;/center&gt;\n\n\n\n\nR & RStudio\n\n\n\nPara ajustar las im√°genes puedes usar r chunk a√±adiendo la imagen con una funci√≥n knitr::include_graphics\n\nknitr::include_graphics(\"https://bookdown.org/oscar_teach/estadistica_aplicada_con_r/r-rstudio.png\")\n\n\n\n\nMi super foto\n\n\n\n\n\nplot(cars)\nplot(pressure)\n\n\n\n\n\n\n\n(a) Plot 1: eed and Stopping Distances of Cars\n\n\n\n\n\n\n\n(b) Plot 2: por Pressure of Mercury as a Function of Temperature\n\n\n\n\nFigura¬†2.4: Plots\n\n\n\nV√©anse ejemplos en Figure¬†2.4. En particular, Figure¬†2.4 (b).\n\n\n2.3.4.4 Ecuaciones\nInsertar ecuaciones en un documento Quarteo es f√°cil utilizando la escritura Latex + en l√≠nea: \\(A = (\\pi * \\lambda \\times r^{4}) / \\alpha\\)\n\ncentradas: \\[A = (\\pi * \\lambda \\times r^{4}) / \\alpha\\]\n\n\n\n2.3.4.5 Diagramas\nQuarto tiene soporte nativo para incrustar diagramas Mermaid y Graphviz. Esto te permite crear diagramas de flujo, diagramas de secuencia, diagramas de estado, diagramas gnatt, y m√°s usando una sintaxis de texto plano inspirada en markdown.\nPor ejemplo, aqu√≠ se incrusta un diagrama de flujo creado con Mermaid:\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\nGantt diagram:\n\n\n\n\ngantt\ndateFormat  YYYY-MM-DD\ntitle Adding GANTT diagram to mermaid\nexcludes weekdays 2014-01-10\n\nsection A section\nCompleted task            :done,    des1, 2014-01-06,2014-01-08\nActive task               :active,  des2, 2014-01-09, 3d\nFuture task               :         des3, after des2, 5d\nFuture task2               :         des4, after des3, 5d\n\n\n\n\n\n\n\n\n2.3.4.6 Bloques de llamada\n\n\n\n\n\n\nNote\n\n\n\nTenga en cuenta que hay cinco tipos de llamadas, incluyendo: note, tip, warning, caution, and important.\n\n\n\n\n2.3.4.7 Listas\n\n\n\n\n√≠tem X\n√≠tem Y\n√≠tem Z\n\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur gravida eu erat et fring. Morbi congue augue vel eros ullamcorper, eget convallis tortor sagittis. Fusce sodales viverra mauris a fringilla. Donec feugiat, justo eu blandit placerat, enim dui volutpat turpis, eu dictum lectus urna eu urna. Mauris sed massa ornare, interdum ipsum a, semper massa.\n\n\n\n\n\n2.3.4.8 Ap√©ndice\nLorem ipsum dolor sit amet, consectetur adipiscing elit.\n\n\n2.3.4.9 Citaciones\nR Markdown The Definitive Guide (Xie, Allaire, and Grolemund 2018)\n\n\n\n2.3.5 Renderizado\nUtiliza el bot√≥n de Render en el entorno de desarrollo integrado (IDE) de RStudio para renderizar el archivo y previsualizar el resultado con un solo clic o combinaci√≥n de teclas (‚áß‚åòK).\n\nknitr::include_graphics(\"img/rstudio-render.png\")\n\n\n\n\n\n\n\n\nSi se prefiere renderizar autom√°ticamente cada vez que se guarda el documento, se puede activar la opci√≥n de Renderizar al Guardar (Render on Save) en la barra de herramientas del editor. La previsualizaci√≥n se actualizar√° cada vez que vuelvas a renderizar el documento. La vista de previsualizaci√≥n en paralelo funciona tanto para salidas en HTML como en PDF."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#buscar-ayuda-en-r-o-sobre-r-funciones-paquetes-errores",
    "href": "002-toolkit-ciencia-datos.html#buscar-ayuda-en-r-o-sobre-r-funciones-paquetes-errores",
    "title": "2¬† R toolkit para la ciencia de datos",
    "section": "2.4 Buscar ayuda en R o sobre R (funciones, paquetes, errores)",
    "text": "2.4 Buscar ayuda en R o sobre R (funciones, paquetes, errores)\nLas funciones de distintos paquetes de R tienen su ayuda y es muy f√°cil acceder a ella desde la consola poniendo un signo de interrogaci√≥n delante de la funci√≥n, por ejemplo al ejecutar ?mean en la consola se abrir√° la ventana de Help con la descripci√≥n de la funci√≥n, su uso, los argumentos y otros datos relevantes que nos permitir√°n entender mejor la funci√≥n que ejecutamos. Tambi√©n, existe una amplia informaci√≥n online sobre las funciones y distintos paquetes disponible en Rdocumentation.org.\n\n2.4.1 Haz preguntas\nUno de los foros m√°s amplio de b√∫squeda de preguntas y respuestas sobre la programaci√≥n es StackOverflow. En StackOverflow se registraron hasta el momento m√°s 450.000 preguntas. Se puede navegar por los archivos de StackOverflow y ver qu√© respuestas han sido votadas por los usuarios, o puedes hacer tus propias preguntas relacionadas con R y esperar una respuesta.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.2 Sigue la comunidad de R\nExisten diversas p√°ginas web que contienen art√≠culos sobre la programaci√≥n en R, como por ejemplo:\n\nR bloggers: un agregador de blogs que vuelve a publicar art√≠culos relacionados con R de toda la web. Un buen lugar para encontrar tutoriales de R,\nRWeekly: R Weekly se fund√≥ el 20 de mayo de 2016. R est√° creciendo muy r√°pidamente, y hay un mont√≥n de grandes blogs, tutoriales y otros formatos de recursos que salen cada d√≠a. R Weekly quiere hacer un seguimiento de estas grandes cosas en la comunidad de R y hacerla m√°s accesible para todos.\nRPubs: RStudio permite aprovechar el poder de R Markdown para crear documentos que entrelazan su escritura y la salida de su c√≥digo R. En RPubs se puede publicar esos documentos en la web en un click.\nMedium: un sitio web de blogs donde se puede encontrar mucha tem√°tica sobre el Data Science."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#git-y-github",
    "href": "002-toolkit-ciencia-datos.html#git-y-github",
    "title": "2¬† R toolkit para la ciencia de datos",
    "section": "2.5 Git y GitHub",
    "text": "2.5 Git y GitHub\n¬øGit y GitHub? En primer lugar, son dos cosas distintas:\n\nGit es un software de c√≥digo abierto para el control de versiones. Con Git puedes hacer cosas como ver todas las versiones anteriores del c√≥digo que has creado en un proyecto.\nGitHub es el servicio m√°s popular (otros son GitLab y BitBucket) para colaborar en el c√≥digo utilizando Git.\n\n\n2.5.1 ¬øPor qu√© deber√≠a usar Git y GitHub?\nLas tres principales ventajas de utilizar Git y GitHub son:\n\nEl uso de Git y GitHub sirve como copia de seguridad. Dado que GitHub tiene una copia de todo el c√≥digo que tienes localmente, si algo le ocurriera a tu ordenador, seguir√≠as teniendo acceso a tu c√≥digo.\nEl uso de Git y GitHub te permite utilizar el control de versiones. ¬øAlguna vez has tenido documentos llamados trabajo_final.pdf, trabajo_final_2.pdf o este_es_el_final.pdf? En lugar de hacer copias de los archivos por miedo a perder el trabajo, el control de versiones permite ver lo que se hizo en el pasado, todo ello manteniendo versiones √∫nicas de los documentos.\nEl uso de Git y GitHub hace posible trabajar en el mismo proyecto al mismo tiempo con distintos colaboradores. Se puede ver el autor de los cambios y se fuera necesario volver a la versi√≥n anterior.\n\nPara poner Git y GitHub a punto lo mejor es seguir la gu√≠a de Happy Git and GitHub for the useR en la que se explica paso a paso como registrarse en GitHub e instalar Git, adem√°s de integrarlo en RStuidio.\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O‚ÄôReilly Media, Inc.\".\n\n\nXie, Yihui, Joseph J Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. CRC Press."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#footnotes",
    "href": "002-toolkit-ciencia-datos.html#footnotes",
    "title": "2¬† R toolkit para la ciencia de datos",
    "section": "",
    "text": "La ‚ÄúComprehensive R Archive Network‚Äù (CRAN) es una colecci√≥n de sitios que contienen material id√©ntico, consistente en la(s) distribuci√≥n(es) de R, las extensiones aportadas, la documentaci√≥n para R y los binarios. El sitio principal de CRAN en la WU (Wirtschaftsuniversit√§t Wien) en Austria puede encontrarse en la URL https://CRAN.R-project.org/ y se refleja diariamente en muchos sitios de todo el mundo.‚Ü©Ô∏é\nla nota a pie de p√°gina‚Ü©Ô∏é"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#compren-negocio",
    "href": "003-crispdm-air-madrid.html#compren-negocio",
    "title": "3¬† An√°lisis de datos de contaminaci√≥n del aire en la ciudad de Madid",
    "section": "3.1 Entendimiento del negocio",
    "text": "3.1 Entendimiento del negocio\n\n\n\n\n\n\nFase I: Comprensi√≥n de los problemas y desaf√≠os asociados con la contaminaci√≥n del aire\n\n\n\n\nIdentificar las fuentes de contaminantes, evaluar los impactos en la salud humana y el medio ambiente y la formular pol√≠ticas y regulaciones. Definici√≥n de los objetivos y requisitos:\nEstablecer los objetivos espec√≠ficos del an√°lisis desde la perspectiva de valor a√±adido para la empresa o entidad p√∫blica: predecir los niveles de contaminaci√≥n, identificar √°reas de alto riesgo o evaluar la efectividad de medidas de control existentes.\nDeterminar los requisitos de datos, como la disponibilidad de datos espaciales de calidad, datos meteorol√≥gicos y datos demogr√°ficos.\n\n\n\nSe comienza formulando las preguntas de investigaci√≥n que se consideran clave en el an√°lisis. Por ejemplo:\n\n¬øCu√°les son las √°reas con mayor nivel de contaminaci√≥n del aire?\n¬øQu√© factores contribuyen a la contaminaci√≥n del aire en estas √°reas?\n¬øC√≥mo ha evolucionado la contaminaci√≥n del aire en estas √°reas a lo largo del tiempo?\n\nUna vez identificadas las preguntas clave, se definen los objetivos del an√°lisis. Por ejemplo, en este caso, el objetivo principal es proporcionar mapas de predicci√≥n de calidad del aire para todo el municipio de Madrid.\nEs importante tener en cuenta que los objetivos deben ser espec√≠ficos, medibles y realistas. Tambi√©n deben estar alineados con los objetivos empresariales o p√∫blicos para proporcionar valor a√±adido."
  },
  {
    "objectID": "003-crispdm-air-madrid.html#compren-datos",
    "href": "003-crispdm-air-madrid.html#compren-datos",
    "title": "3¬† An√°lisis de datos de contaminaci√≥n del aire en la ciudad de Madid",
    "section": "3.2 Comprensi√≥n de los datos",
    "text": "3.2 Comprensi√≥n de los datos\n\n\n\n\n\n\nFase II: entendimimiento de los datos de calidad del aire\n\n\n\n\nExploraci√≥n de los datos:\n\nAnalizar la estructura de los datos, como las variables disponibles y sus tipos.\nRealizar res√∫menes estad√≠sticos y an√°lisis exploratorios, como la identificaci√≥n de valores at√≠picos y la distribuci√≥n de los datos.\n\nVisualizaci√≥n de los datos espaciales:\n\nCrear gr√°ficos y mapas para visualizar la distribuci√≥n espacial de la contaminaci√≥n del aire.\nUtilizar t√©cnicas de visualizaci√≥n interactiva, como la creaci√≥n de mapas interactivos y paneles de control.\n\nEvaluaci√≥n de la calidad de los datos:\n\nEvaluar la calidad de los datos, como la integridad espacial, la consistencia y la completitud.\nTratar los valores faltantes y los valores at√≠picos de manera apropiada.\n\n\n\n\n\n\n\n\n\n\nLibrer√≠as y funciones en R\n\n\n\nEl paquete integrado de tidyverse es un buen aliado en esta tarea, ya que recoge las librer√≠as para lectura, transformaci√≥n y representaci√≥n de los datos. Las librer√≠as a tener en cuenta son:\n\nLectura: reader.\nManejo de datos: dplyr.\nDatos temporales: lubridate. \nVisualizaci√≥n: ggplot2, leaflet.\n\n\n\n\n3.2.1 Obtenci√≥n de los datos\nEl primer paso siempre es la obtenci√≥n y carga de los datos. Los datos de la calidad de la ciudad de Madrid se encuentran disponibles en la p√°gina datos abiertos. Aqu√≠, se seleccionan aquellos que tienen frecuencia diaria (disponibles aqu√≠). Para obtener los datos de forma automatizada se utiliza la API proporcionada por la web de datos abiertos. Como base de obtenci√≥n de los enlaces de descarga, se copia el enlace de la descarga en DECAT https://datos.madrid.es/egob/catalogo/201410-0-calidad-aire-diario.dcat. La definici√≥n y la estructura de los datos se encuentra en Int√©rprete de ficheros de calidad del aire.\n\nlibrary(tidyverse)\nlibrary(xml2)\nlibrary(vroom)\n\n# Se guarda la URL RDF de DECAT de los datos\nurl &lt;- \"https://datos.madrid.es/egob/catalogo/201410-0-calidad-aire-diario.dcat\"\n\n# Se carga la p√°gina\npage &lt;- url %&gt;%\n  read_xml() %&gt;%\n  as_list()\n\n# Se convierte en una lista e inspecciona para identificar la localizaci√≥n de los datos\nlocation &lt;- page[[\"RDF\"]][[\"Catalog\"]][[\"dataset\"]][[\"Dataset\"]]\nlocation &lt;- location[names(location) == \"distribution\"]\n\n\n# Tras identificar se genera un tibble con el a√±o como identificaci√≥n y el link a los datos\nlinks &lt;-\n  tibble(\n    year = sapply(X = location, function(x) unname(unlist(\n      x[[\"Distribution\"]][[\"title\"]][1]))),\n    link = sapply(X = location, function(x) unname(unlist(\n      x[[\"Distribution\"]][[\"accessURL\"]][1])))\n  )\n\n# Seleccionar el formato csv\nlinks &lt;- links %&gt;% filter(str_detect(link, pattern = \".csv\"))\n\n# A√±adir el nombre de fichero a descargar\nlinks &lt;- links %&gt;% \n  mutate(file_name = paste0(\"datos_aire_madrid_\", year, \".csv\"))\n\n# Descarga de los ficheros csv desde 2013\nyears &lt;- links %&gt;% filter(year &gt;= \"2013\") %&gt;% .$year\n\nlapply(years, function(x) {\n  file_x &lt;- links %&gt;%\n    filter(year == x & str_detect(link, \".csv\"))\n  \n  if (x == max(years)) {\n    download.file(url = file_x$link,\n                  destfile = paste0(\"data/\",\n                                    file_x$file_name))\n  }\n  \n  if (!file.exists(paste0(\"data/\", file_x$file_name))) {\n    download.file(url = file_x$link,\n                  destfile = paste0(\"data/\",\n                                    file_x$file_name))\n  }\n})\n\n# Lectura de los ficheros\ndata &lt;- vroom(paste0(\"data/\", links %&gt;% filter(year %in% years) %&gt;% .$file_name))\n\n# Conversi√≥n de viariables a num√©ricas\ncols_to_numeric &lt;-\n  c(\n    \"PROVINCIA\",\n    \"MUNICIPIO\",\n    \"ESTACION\",\n    \"MAGNITUD\",\n    \"PUNTO_MUESTREO\",\n    \"ANO\",\n    \"MES\",\n    str_subset(names(data), pattern = '^D')\n  )\ndata &lt;- data %&gt;%\n  mutate(across(all_of(cols_to_numeric), as.numeric))\n\n# Guardar datos brutos\nwrite_rds(data, \"data/data_raw.RDS\")\n\n# Transformar de datos transversales a longitudinales\nair_mad &lt;- data %&gt;%\n  gather(v, valor, D01:V31) %&gt;%\n  mutate(DIA = str_sub(v, 2, 3),\n         v = str_sub(v, 1, 1))\n\nair_mad &lt;- air_mad %&gt;%\n  mutate(id = ESTACION,\n         fecha = as.Date(paste(ANO, MES, DIA, sep = \"-\"))) %&gt;%\n  select(id, MAGNITUD, fecha, v, valor)\n\nair_mad &lt;- air_mad %&gt;%\n  unique() %&gt;%\n  pivot_wider(names_from = v, values_from = valor)\nair_mad &lt;- air_mad %&gt;%\n  mutate(valor = as.numeric(D)) %&gt;%\n  select(-D)\n\n# A√±adir informaci√≥n de diccionarios\nstation_names &lt;- read_csv(\"dictionaries/station_names.csv\")\nmagnitudes_names &lt;- read_csv(\"dictionaries/magnitudes_names.csv\")\n\nair_mad &lt;- left_join(air_mad, station_names, by = \"id\")\nair_mad &lt;- left_join(air_mad, magnitudes_names, by = \"MAGNITUD\")\nair_mad &lt;- air_mad %&gt;% select(-MAGNITUD)\n\n# Guardar los datos\nwrite_rds(air_mad, \"data/air_mad.RDS\")\n\nSe procede a la lectura de los datos obtenidos y procesados que se han guardado en el objeto air_mad.RDS. Posteriormente se consulta la clase del objeto.\n\nlibrary(tidyverse)\nair_mad &lt;- readr::read_rds(\"data/air_mad.RDS\")\nclass(air_mad)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nDe acuerdo con la salida, se trata de un objeto de clase tibble, que forma parte del universo tidyverse. A continuaci√≥n, se explora el conjunto de datos.\n\nhead(air_mad)\n\n# A tibble: 6 √ó 12\n     id fecha      V     valor id_name      longitud latitud zona  tipo  nom_mag\n  &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  \n1     4 2023-01-01 V       1   Plaza de Es‚Ä¶    -3.71    40.4 Inte‚Ä¶ tr√°f‚Ä¶ Di√≥xid‚Ä¶\n2     4 2023-02-01 V       3   Plaza de Es‚Ä¶    -3.71    40.4 Inte‚Ä¶ tr√°f‚Ä¶ Di√≥xid‚Ä¶\n3     4 2023-03-01 V       3   Plaza de Es‚Ä¶    -3.71    40.4 Inte‚Ä¶ tr√°f‚Ä¶ Di√≥xid‚Ä¶\n4     4 2023-04-01 V       1   Plaza de Es‚Ä¶    -3.71    40.4 Inte‚Ä¶ tr√°f‚Ä¶ Di√≥xid‚Ä¶\n5     4 2023-05-01 V       1   Plaza de Es‚Ä¶    -3.71    40.4 Inte‚Ä¶ tr√°f‚Ä¶ Di√≥xid‚Ä¶\n6     4 2023-01-01 V       0.4 Plaza de Es‚Ä¶    -3.71    40.4 Inte‚Ä¶ tr√°f‚Ä¶ Mon√≥xi‚Ä¶\n# ‚Ñπ 2 more variables: nom_abv &lt;chr&gt;, ud_med &lt;chr&gt;\n\n\n\ndim(air_mad)\n\n[1] 559009     12\n\n\n\nsummary(air_mad)\n\n       id            fecha                 V                 valor         \n Min.   : 4.00   Min.   :2013-01-01   Length:559009      Min.   :   -2.00  \n 1st Qu.:18.00   1st Qu.:2015-07-16   Class :character   1st Qu.:    2.00  \n Median :38.00   Median :2018-01-26   Mode  :character   Median :   13.00  \n Mean   :34.95   Mean   :2018-02-03                      Mean   :   26.19  \n 3rd Qu.:54.00   3rd Qu.:2020-08-02                      3rd Qu.:   36.00  \n Max.   :60.00   Max.   :2023-05-31                      Max.   :97396.00  \n                 NA's   :157                                               \n   id_name             longitud         latitud          zona          \n Length:559009      Min.   :-3.775   Min.   :40.35   Length:559009     \n Class :character   1st Qu.:-3.713   1st Qu.:40.41   Class :character  \n Mode  :character   Median :-3.689   Median :40.42   Mode  :character  \n                    Mean   :-3.683   Mean   :40.43                     \n                    3rd Qu.:-3.652   3rd Qu.:40.46                     \n                    Max.   :-3.580   Max.   :40.52                     \n                                                                       \n     tipo             nom_mag            nom_abv             ud_med         \n Length:559009      Length:559009      Length:559009      Length:559009     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n\n\n\nstr(air_mad)\n\ntibble [559,009 √ó 12] (S3: tbl_df/tbl/data.frame)\n $ id      : num [1:559009] 4 4 4 4 4 4 4 4 4 4 ...\n $ fecha   : Date[1:559009], format: \"2023-01-01\" \"2023-02-01\" ...\n $ V       : chr [1:559009] \"V\" \"V\" \"V\" \"V\" ...\n $ valor   : num [1:559009] 1 3 3 1 1 0.4 0.5 0.4 0.3 0.1 ...\n $ id_name : chr [1:559009] \"Plaza de Espa√±a\" \"Plaza de Espa√±a\" \"Plaza de Espa√±a\" \"Plaza de Espa√±a\" ...\n $ longitud: num [1:559009] -3.71 -3.71 -3.71 -3.71 -3.71 ...\n $ latitud : num [1:559009] 40.4 40.4 40.4 40.4 40.4 ...\n $ zona    : chr [1:559009] \"Interior M30\" \"Interior M30\" \"Interior M30\" \"Interior M30\" ...\n $ tipo    : chr [1:559009] \"tr√°fico\" \"tr√°fico\" \"tr√°fico\" \"tr√°fico\" ...\n $ nom_mag : chr [1:559009] \"Di√≥xido de Azufre\" \"Di√≥xido de Azufre\" \"Di√≥xido de Azufre\" \"Di√≥xido de Azufre\" ...\n $ nom_abv : chr [1:559009] \"SO2\" \"SO2\" \"SO2\" \"SO2\" ...\n $ ud_med  : chr [1:559009] \"¬µg/m3\" \"¬µg/m3\" \"¬µg/m3\" \"¬µg/m3\" ...\n\n\n\n\n\n\n\n\nWarning\n\n\n\nExisten paquetes como DataExplorer y dlookr que generan informes autom√°ticos con los principales descriptivos.\n\n\n¬øC√≥mo han evolucionado la concentraci√≥n de contaminantes del aire en la ciudad de Madrid en el periodo considerado? Realizar un primer gr√°fico de lineas para responder a esta pregunta ser√≠a muy esclarecedor.\n\nlibrary(lubridate)\nair_mad %&gt;%\n  filter(V == \"V\") %&gt;% \n  group_by(semana = floor_date(fecha, unit = \"week\"), nom_mag) %&gt;%\n  summarise(media_estaciones = mean(valor, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = semana, y = media_estaciones)) +\n  geom_line(aes(color = nom_mag)) +\n  geom_smooth(size = 0.5, color = \"red\") +\n  labs(\n    x = NULL, y = \"(¬µg/m3)\", title = \"Evoluci√≥n de contaminantes en Madrid\",\n    subtitle = \"Concentraci√≥n media semanal en las estaciones de medici√≥n\",\n    caption = \"Fuente: Portal de datos abiertos del Ayuntamiento de Madrid\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_wrap(~nom_mag, scales = \"free_y\", ncol = 2)\n\n\n\n\nFigura¬†3.1: Evoluci√≥n de contaminantes en Madrid (2013-2023)\n\n\n\n\nLas emisiones de la mayor√≠a de los contaminantes del aire han disminuido significativamente en las √∫ltimas d√©cadas (v√©ase Figure¬†3.1), sin embargo, sus concentraciones todav√≠a superan los l√≠mites legales en la mayor√≠a de los pa√≠ses, lo que indica que el control de la contaminaci√≥n del aire sigue siendo un desaf√≠o para las sociedades modernas. Este an√°lisis se centra en el di√≥xido de nitr√≥geno (NO2) por sus implicaciones directas en la ciudad de Madrid, adem√°s de por las siguientes razones de peso:\n\nImpacto en la salud humana: el di√≥xido de nitr√≥geno son un contaminante del aire asociado a diversos problemas de salud, especialmente afecciones respiratorias. La exposici√≥n prolongada al NO2 puede causar irritaci√≥n en las v√≠as respiratorias, exacerbaci√≥n de enfermedades pulmonares como el asma y aumentar el riesgo de infecciones respiratorias.\nFuente de emisi√≥n: el tr√°fico rodado es una de las principales fuentes de emisi√≥n de di√≥xido de nitr√≥geno en √°reas urbanas. En ciudades con altos vol√∫menes de tr√°fico, como Madrid, se produce una considerable liberaci√≥n de NO2 debido a la combusti√≥n de combustibles f√≥siles en los veh√≠culos.\nRegulaciones y l√≠mites: el di√≥xido de nitr√≥geno est√° sujeto a regulaciones y l√≠mites establecidos tanto a nivel nacional como internacional. Estos l√≠mites buscan controlar y reducir la concentraci√≥n de NO2 en el aire para proteger la salud de la poblaci√≥n y mejorar la calidad del aire en general.\nDisponibilidad de datos: existen sistemas de monitoreo de calidad del aire que recopilan datos precisos sobre la concentraci√≥n de di√≥xido de nitr√≥geno en tiempo real. En el caso de Madrid, el Ayuntamiento dispone de datos abiertos proporcionados por su red de estaciones de monitoreo, lo que facilita el an√°lisis y seguimiento de los niveles de NO2 en la ciudad.\n\nTeniendo en cuenta estas razones, el an√°lisis de datos de contaminaci√≥n se centrar√° en el di√≥xido de nitr√≥geno (NO2) para obtener informaci√≥n relevante sobre su impacto en la salud p√∫blica y evaluar el cumplimiento de los l√≠mites establecidos para este contaminante en la ciudad de Madrid (y concretamente, en la zonas de calidad del aire definidas por el ayuntamiento).\n\nplot_air_mad &lt;- air_mad %&gt;%\n  filter(V == \"V\" & nom_abv == \"NO2\" & fecha &gt;= \"2018-01-01\") %&gt;% \n  group_by(fecha, nom_mag, zona) %&gt;%\n  summarise(media_estaciones = mean(valor, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = fecha, y = media_estaciones)) +\n  geom_line(aes(color = zona)) +\n  geom_smooth(size = 0.5, color = \"red\") +\n  labs(\n    x = NULL, y = \"(¬µg/m3)\", title = \"Evoluci√≥n de NO2 en Madrid\",\n    subtitle = \"Concentraci√≥n por zonas en las estaciones de medici√≥n\",\n    caption = \"Fuente: Portal de datos abiertos del Ayuntamiento de Madrid\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_wrap(~zona, ncol = 1)\n\nplot_air_mad"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#prep-datos",
    "href": "003-crispdm-air-madrid.html#prep-datos",
    "title": "3¬† An√°lisis de datos de contaminaci√≥n del aire en la ciudad de Madid",
    "section": "3.3 Preparaci√≥n de los datos",
    "text": "3.3 Preparaci√≥n de los datos\nEn esta fase se abordan la limpieza y transformaci√≥n de los datos de contaminaci√≥n del aire para su posterior an√°lisis.\nEn esta secci√≥n, se explorar√°n las t√©cnicas y m√©todos para limpiar los datos de contaminaci√≥n del aire y abordar los problemas de calidad, como los valores faltantes, los valores at√≠picos y los errores de medici√≥n. Se describir√°n las siguientes actividades:\n\n\n\n\n\n\nFase III: limpieza y transformaci√≥n de los datos de calidad del aire\n\n\n\n\nTratamiento de valores faltantes, por aver√≠as en las estaciones de monitoreo, por ejemplo. Imputaci√≥n o eliminaci√≥n los valores faltantes de manera adecuada.\nIdentificar y analizar los valores at√≠picos en los datos de contaminaci√≥n del aire, en d√≠as de calima, por ejemplo. Evaluar si los valores at√≠picos son errores de medici√≥n o representan informaci√≥n relevante.\nResoluci√≥n de errores de medici√≥n, si los hubiera.\nIngenier√≠a de variables: normalizaci√≥n, codificaci√≥n de variables categ√≥ricas, agregaci√≥n (pasar de frecuencia horaria a frecuencia diaria en la medici√≥n delos contaminantes).\n\n\n\n\n\n\n\n\n\nLibrar√≠as y funciones en R\n\n\n\n\nManipulaci√≥n de datos (valores at√≠picos y datos faltantes): dplyr\nManipulaci√≥n de datos a lo ancho y largo: tidyr\n\n\n\n\nno2 &lt;- air_mad %&gt;% \n  filter(nom_abv == \"NO2\")\n\n\nno2 %&gt;% \n  count(V)\n\n# A tibble: 2 √ó 2\n  V         n\n  &lt;chr&gt; &lt;int&gt;\n1 N       819\n2 V     90267\n\n\nHay un total de 819 datos no v√°lidos y 90267 v√°lidos. Se seleccionan solamente los datos v√°lidos:\n\nno2 &lt;- no2 %&gt;% \n  filter(V == \"V\")\n\n\n3.3.1 Valores faltantes (NA)\n¬øExisten datos con valores faltantes, codificados como NA (not available)?:\n\nno2 %&gt;%\n  count(is.na(valor))\n\n# A tibble: 1 √ó 2\n  `is.na(valor)`     n\n  &lt;lgl&gt;          &lt;int&gt;\n1 FALSE          90267\n\n\nNo se encuentran valores v√°lidos con NA en el campo valor.\nComo parte del proceso de preparaci√≥n de los datos, a continuaci√≥n, se agrupan los datos seg√∫n la variable zona y se genera la variable valor_zona que representa la media por zona y d√≠a. Se crea el objeto no2_zona para tal efecto.\n\nno2_zona &lt;- no2 %&gt;% \n  group_by(zona, fecha) %&gt;% \n  summarise(valor_zona = mean(valor, na.rm = T))\n\n\n\n3.3.2 Valores at√≠picos (outliers)\nLa detecci√≥n de valores at√≠picos es fundamental en esta tercera fase. A modo de ejemplo, se ilustra como detectar valores at√≠picos en la zona Interior de la M30 con el criterio de 1,5 veces el rango intercuart√≠lico.\n\nanomalias &lt;- lapply(unique(no2_zona$zona), function(x) {\n  aux_data &lt;- no2_zona %&gt;%\n    filter(zona == x) %&gt;%\n    select(zona, fecha, valor_zona)\n  \n  aux_data &lt;- aux_data %&gt;%\n    group_by(month(fecha), year(fecha)) %&gt;%\n    mutate(anomaly = !between(\n      valor_zona,\n      quantile(aux_data$valor_zona, probs = c(.25)) - 1.5 * IQR(valor_zona),\n      quantile(aux_data$valor_zona, probs = c(.75)) + 1.5 * IQR(valor_zona)\n    )) %&gt;% \n    ungroup() %&gt;% \n    select(zona, fecha, anomaly)\n})\nanomalias &lt;- do.call(rbind, anomalias)\n  \nno2_zona &lt;- left_join(no2_zona, anomalias, by = c(\"zona\", \"fecha\"))\n\n\nno2_zona %&gt;% \n  filter(zona == \"Interior M30\") %&gt;% \n  ggplot(aes(fecha, valor_zona, color = anomaly)) +\n  geom_point() +\n  scale_x_date() + \n  scale_color_manual(values = c(\"black\", \"red\")) +\n  labs(title = \"Anomal√≠as NO2 por zona de interes\",\n       subtitle = \"Interior M30\",\n       x = NULL, \n       y = \"(¬µg/m3)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n3.3.3 Ingenier√≠a de variables\nUna vez se han realizado los ajustes en los datos se procede a creaci√≥n de variables de inter√©s. De acuerdo con el DECRETO 140/2017, de 21 de noviembre, del Consejo de Gobierno, por el que se aprueba el protocolo marco de actuaci√≥n durante episodios de alta contaminaci√≥n por di√≥xido de nitr√≥geno (NO2) en la Comunidad de Madrid, el objetivo de la media anual se fija en los 40 ¬µg/m3. En consecuencia, se genera la variable l√≥gica objetivo_anual, que tomar√° el valor TRUE cuando los valores son inferiores o iguales a 40 ¬µg/m3 del objetivo anual y FALSE en caso contrario.\n\nno2_zona &lt;- no2_zona %&gt;%\n  mutate(objetivo_anual = valor_zona &lt;= 40)\n\nno2_zona %&gt;% \n  ggplot(aes(zona, fill = objetivo_anual)) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Cumplimiento del tope de 40 ¬µg/m3 de NO2\",\n       subtitle = \"porcentaje de cumplimiento por zona\",\n       x = NULL,\n       y = \"%\", \n       fill = \"Cumplimiento del objetivo\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nSe observa que la zona que se encuentra m√°s cerca del cumplimiento de los objetivos es la zona Noroeste, mientras la que se encuentra m√°s alejada es la zona Suroeste. En General las zonas del Norte obtienen mejores resultados.\nDado que se trata de un objetivo anual se aplica a cada zona una media m√≥vil con ventana de un a√±o para ver la evoluci√≥n de los datos:\n\nlibrary(zoo) \nno2_zona &lt;- no2_zona %&gt;% \n  group_by(zona) %&gt;%\n  mutate(valor_zona_anual = rollmean(valor_zona, k = 365, fill = NA, align = 'right'))\n\nno2_zona %&gt;% \n  filter(fecha &gt;= \"2014-01-01\") %&gt;% \n  ggplot(aes(fecha, valor_zona_anual, color = zona)) +\n  geom_line() +\n  scale_x_date() +\n  geom_hline(yintercept = 40, color = \"red\") +\n  labs(title = \"Cumplimiento del tope de 40 ¬µg/m3 de NO2\",\n       subtitle = \"media m√≥vil anual por zona\",\n       x = NULL,\n       y = \"¬µg/m3 de NO2\", \n       color = \"Zona\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nSe observa un claro cambio en los niveles de la contaminaci√≥n desde 2020, el a√±o del comienzo de la pandemia de COVID-19.\nEl siguiente paso ser√° centrar el estudio en la zona de calidad del aire con mayores niveles de contaminaci√≥n, el interior de la M30.\nSe filtra la zona de calidad del aire con mayores niveles de contaminaci√≥n, la zona ‚ÄúInterior M30‚Äù y se crea el subconjunto no2_zona_m30.\n\nno2_zona_m30 &lt;- no2_zona %&gt;%\n  filter(zona == \"Interior M30\" & anomaly == FALSE) %&gt;% \n  ungroup() %&gt;% \n  select(fecha, valor_zona)\n\nSe visualizan los datos de NO2 en el interior de la M30\n\nlibrary(timetk)\n\nno2_zona_m30 %&gt;%\n  plot_time_series(fecha, valor_zona)\n\n\n\n\n\n\n\n3.3.4 Ingenier√≠a de variables\nSe generan una serie de variables √∫tiles para la predicci√≥n y se define h como el horizonte de predicci√≥n y que indica, tambi√©n, el n√∫mero de lags.\n\nfecha_num: valor num√©rico normalizado de la fecha.\ndow: valor num√©rico indicativo del d√≠a de la semana (siendo lunes el primer d√≠a).\nmonth: valor num√©rico indicativo del mes.\nquarter: valor num√©rico indicativo del trimestre.\nyear: a√±o.\n\nLa creaci√≥n de variables auxiliares como d√≠a de la semana, mes o a√±o en los modelos de machine learning para la predicci√≥n de series temporales es necesaria debido a varias razones:\n\nTendencia estacional: Muchas series temporales exhiben patrones estacionales que se repiten en intervalos regulares, como estacionalidad diaria, mensual o anual. Al incluir variables como d√≠a de la semana, mes o a√±o, los modelos de machine learning pueden capturar mejor estos patrones y ajustar las predicciones en consecuencia.\nDiferenciaci√≥n de comportamiento: Las series temporales pueden tener diferentes comportamientos dependiendo del d√≠a de la semana o el mes. Por ejemplo, las ventas minoristas pueden ser m√°s altas los fines de semana o durante las vacaciones. Al incluir variables auxiliares, los modelos pueden aprender estas diferencias y mejorar la precisi√≥n de las predicciones.\nCambios estacionales y tendencias a largo plazo: Al considerar variables como el a√±o, los modelos pueden capturar cambios estacionales y tendencias a largo plazo en los datos. Por ejemplo, una serie de tiempo que muestra un crecimiento anual constante puede beneficiarse de la inclusi√≥n del a√±o como variable para predecir futuros incrementos.\nAjuste a patrones no lineales: Al utilizar variables auxiliares, los modelos de machine learning pueden capturar patrones no lineales en las series temporales. Por ejemplo, puede haber interacciones complejas entre el d√≠a de la semana y el mes que influyen en el comportamiento de la serie.\n\nEn resumen, la inclusi√≥n de variables auxiliares como d√≠a de la semana, mes o a√±o en los modelos de machine learning para la predicci√≥n de series temporales permite capturar patrones estacionales, diferencias de comportamiento, cambios estacionales y tendencias a largo plazo, as√≠ como ajustar a patrones no lineales. Esto mejora la capacidad del modelo para realizar pron√≥sticos precisos y m√°s s√≥lidos en series temporales.\nAdem√°s de las variables auxiliares mencionadas anteriormente, la inclusi√≥n de lags o retardos en los modelos de machine learning para la predicci√≥n de series temporales es otra t√©cnica importante. Los lags representan la observaci√≥n pasada de la serie en diferentes puntos de tiempo y se utilizan como variables predictoras en el modelo.\nLa inclusi√≥n de lags permite capturar la dependencia temporal en los datos. Las observaciones pasadas de la serie pueden contener informaci√≥n relevante para predecir el valor futuro. Al incluir lags en el modelo, se proporciona al algoritmo de machine learning informaci√≥n sobre c√≥mo las observaciones anteriores afectan a las futuras.\nLa elecci√≥n adecuada de los lags es importante. Depender√° de la naturaleza de la serie y de su periodicidad. Por ejemplo, en una serie de tiempo mensual, se pueden incluir lags correspondientes a los meses anteriores. En una serie de tiempo diaria, se pueden considerar lags que representen los d√≠as anteriores.\nEn el caso de los modelos seleccionados se aplica lags de 1 al 14 por medio de la funci√≥n lag_transformer() creada en el chunk siguiente.\nEn definitiva inclusi√≥n de lags permite al modelo capturar patrones de autocorrelaci√≥n y estacionalidad en la serie temporal. Adem√°s, puede ayudar a capturar cambios en la din√°mica de la serie y a mejorar la precisi√≥n de las predicciones.\n\n\n\n\n\n\nAutocorrelaci√≥n y estacionalidad\n\n\n\nLa autocorrelaci√≥n temporal se refiere a la correlaci√≥n entre una observaci√≥n y sus observaciones pasadas a lo largo del tiempo. Es una medida que indica c√≥mo una observaci√≥n en un momento dado est√° relacionada con las observaciones anteriores en la misma serie (Shumway and Stoffer 2017). Se trata un concepto fundamental en el an√°lisis de series temporales, ya que proporciona informaci√≥n sobre la dependencia temporal y los patrones de comportamiento en los datos. Permite identificar si hay una relaci√≥n lineal o no lineal entre las observaciones pasadas y presentes.\nLa estacionalidad se refiere a patrones regulares y repetitivos que se observan en los datos a lo largo del tiempo. Estos patrones pueden ser de corto plazo, como fluctuaciones diarias o semanales, o de largo plazo, como fluctuaciones estacionales o anuales (Hyndman 2021). Se trata de una caracter√≠stica com√∫n en muchos tipos de datos, como las ventas minoristas, la demanda de productos, el tr√°fico web, el clima y muchos otros.\n\n\nSin embargo, es importante tener en cuenta que la inclusi√≥n de demasiados lags puede aumentar la complejidad del modelo y llevar a problemas de sobreajuste1. Por lo tanto, se debe encontrar un equilibrio adecuado al seleccionar los lags relevantes y evitar la inclusi√≥n de lags redundantes.\nPara mitigasr los problemas de sobreajuste existen varias t√©cnicas como:\n\nRegularizaci√≥n: Se aplica una penalizaci√≥n a los coeficientes o par√°metros del modelo para evitar que tomen valores extremos.\nValidaci√≥n cruzada: Se divide el conjunto de datos en subconjuntos de entrenamiento y validaci√≥n para evaluar el rendimiento del modelo en datos no vistos durante el entrenamiento.\nReducci√≥n de la complejidad del modelo: Se puede reducir el n√∫mero de caracter√≠sticas o la complejidad del modelo para evitar un ajuste excesivo.\nRecopilaci√≥n de m√°s datos: Obtener m√°s datos puede ayudar a reducir el sobreajuste al proporcionar una visi√≥n m√°s completa y representativa del problema.\n\nEn el caso de este proyecto se aplica el ajuste de par√°metros y validaci√≥n cruzada.\n\n# el horizonte\nh &lt;- 21\n\n# funci√≥n auxilar para hacer los lags\nlag_transformer &lt;- function(data){\n  data %&gt;%\n    tk_augment_lags(valor_zona, .lags = 1:14) %&gt;%\n    ungroup()\n}\n\n# Extensi√≥n de los datos en h\nno2_zona_m30_ext &lt;- no2_zona_m30 %&gt;%\n  future_frame(\n    .date_var = \"fecha\", \n    .length_out = h,\n    .bind_data  = TRUE\n  ) %&gt;%\n  ungroup()\n\n# Generaci√≥n de variables\nno2_zona_m30_ext &lt;- no2_zona_m30_ext %&gt;% \n  mutate(\n    fecha_num = normalize_vec(as.numeric(fecha)),\n    dow = wday(fecha, week_start = 1), \n    month = month(fecha),\n    quarter = quarter(fecha),\n    year = year(fecha)\n  ) %&gt;%\n  ungroup()\n\n# A√±adir lags\nno2_zona_m30_ext &lt;- no2_zona_m30_ext %&gt;%\n  lag_transformer()\n\n\n\n3.3.5 Divisi√≥n del dataset (entrenamamineto-validaci√≥n y predicci√≥n)\nA continuaci√≥n se divide el conjunto de datos en dos subconjuntos: uno para el entrenamiento y validaci√≥n y otro para la predicci√≥n.\n\n# Datos de entrenamiento\ntrain_data &lt;- no2_zona_m30_ext %&gt;%\n  drop_na()\n\n# Datos para la prediccci√≥n\nfuture_data &lt;- no2_zona_m30_ext %&gt;%\n  filter(is.na(valor_zona))\n\nLos datos para el entrenamiento.\n\nstr(train_data)\n\ntibble [3,674 √ó 21] (S3: tbl_df/tbl/data.frame)\n $ fecha           : Date[1:3674], format: \"2013-01-15\" \"2013-01-16\" ...\n $ valor_zona      : num [1:3674] 33.6 31.6 36.2 25.7 28.3 ...\n $ fecha_num       : num [1:3674] 0.00366 0.00392 0.00419 0.00445 0.00471 ...\n $ dow             : num [1:3674] 2 3 4 5 6 7 1 2 3 4 ...\n $ month           : num [1:3674] 1 1 1 1 1 1 1 1 1 1 ...\n $ quarter         : int [1:3674] 1 1 1 1 1 1 1 1 1 1 ...\n $ year            : num [1:3674] 2013 2013 2013 2013 2013 ...\n $ valor_zona_lag1 : num [1:3674] 42.2 33.6 31.6 36.2 25.7 ...\n $ valor_zona_lag2 : num [1:3674] 27.2 42.2 33.6 31.6 36.2 ...\n $ valor_zona_lag3 : num [1:3674] 29.2 27.2 42.2 33.6 31.6 ...\n $ valor_zona_lag4 : num [1:3674] 57.2 29.2 27.2 42.2 33.6 31.6 36.2 25.7 28.3 19.7 ...\n $ valor_zona_lag5 : num [1:3674] 61.5 57.2 29.2 27.2 42.2 33.6 31.6 36.2 25.7 28.3 ...\n $ valor_zona_lag6 : num [1:3674] 52.5 61.5 57.2 29.2 27.2 42.2 33.6 31.6 36.2 25.7 ...\n $ valor_zona_lag7 : num [1:3674] 54.2 52.5 61.5 57.2 29.2 27.2 42.2 33.6 31.6 36.2 ...\n $ valor_zona_lag8 : num [1:3674] 53.1 54.2 52.5 61.5 57.2 29.2 27.2 42.2 33.6 31.6 ...\n $ valor_zona_lag9 : num [1:3674] 75.4 53.1 54.2 52.5 61.5 57.2 29.2 27.2 42.2 33.6 ...\n $ valor_zona_lag10: num [1:3674] 85 75.4 53.1 54.2 52.5 61.5 57.2 29.2 27.2 42.2 ...\n $ valor_zona_lag11: num [1:3674] 81.2 85 75.4 53.1 54.2 52.5 61.5 57.2 29.2 27.2 ...\n $ valor_zona_lag12: num [1:3674] 66.1 81.2 85 75.4 53.1 54.2 52.5 61.5 57.2 29.2 ...\n $ valor_zona_lag13: num [1:3674] 59.1 66.1 81.2 85 75.4 53.1 54.2 52.5 61.5 57.2 ...\n $ valor_zona_lag14: num [1:3674] 40.7 59.1 66.1 81.2 85 75.4 53.1 54.2 52.5 61.5 ...\n\n\nLos datos para la predicci√≥n:\n\nstr(future_data)\n\ntibble [21 √ó 21] (S3: tbl_df/tbl/data.frame)\n $ fecha           : Date[1:21], format: \"2023-06-01\" \"2023-06-02\" ...\n $ valor_zona      : num [1:21] NA NA NA NA NA NA NA NA NA NA ...\n $ fecha_num       : num [1:21] 0.995 0.995 0.995 0.996 0.996 ...\n $ dow             : num [1:21] 4 5 6 7 1 2 3 4 5 6 ...\n $ month           : num [1:21] 6 6 6 6 6 6 6 6 6 6 ...\n $ quarter         : int [1:21] 2 2 2 2 2 2 2 2 2 2 ...\n $ year            : num [1:21] 2023 2023 2023 2023 2023 ...\n $ valor_zona_lag1 : num [1:21] 25.3 NA NA NA NA NA NA NA NA NA ...\n $ valor_zona_lag2 : num [1:21] 31.5 25.3 NA NA NA NA NA NA NA NA ...\n $ valor_zona_lag3 : num [1:21] 28.2 31.5 25.3 NA NA NA NA NA NA NA ...\n $ valor_zona_lag4 : num [1:21] 15.9 28.2 31.5 25.3 NA NA NA NA NA NA ...\n $ valor_zona_lag5 : num [1:21] 16.4 15.9 28.2 31.5 25.3 NA NA NA NA NA ...\n $ valor_zona_lag6 : num [1:21] 19.6 16.4 15.9 28.2 31.5 25.3 NA NA NA NA ...\n $ valor_zona_lag7 : num [1:21] 25.4 19.6 16.4 15.9 28.2 31.5 25.3 NA NA NA ...\n $ valor_zona_lag8 : num [1:21] 18.3 25.4 19.6 16.4 15.9 28.2 31.5 25.3 NA NA ...\n $ valor_zona_lag9 : num [1:21] 15.1 18.3 25.4 19.6 16.4 15.9 28.2 31.5 25.3 NA ...\n $ valor_zona_lag10: num [1:21] 18.2 15.1 18.3 25.4 19.6 16.4 15.9 28.2 31.5 25.3 ...\n $ valor_zona_lag11: num [1:21] 10.5 18.2 15.1 18.3 25.4 19.6 16.4 15.9 28.2 31.5 ...\n $ valor_zona_lag12: num [1:21] 9.9 10.5 18.2 15.1 18.3 25.4 19.6 16.4 15.9 28.2 ...\n $ valor_zona_lag13: num [1:21] 12.5 9.9 10.5 18.2 15.1 18.3 25.4 19.6 16.4 15.9 ...\n $ valor_zona_lag14: num [1:21] 10.7 12.5 9.9 10.5 18.2 15.1 18.3 25.4 19.6 16.4 ...\n\n\n\n\n3.3.6 Validaci√≥n cruzada para evaluaci√≥n de los modelos\nLa librer√≠a timek con la funci√≥n time_series_cv() crea conjuntos de validaci√≥n cruzada de rsample para series temporales. Esta funci√≥n genera un plan de muestreo comenzando con las observaciones m√°s recientes de la serie temporal y avanzando hacia atr√°s. El procedimiento de muestreo es similar a rsample::rolling_origin(), pero se enfoca en los datos m√°s recientes de la serie temporal para la validaci√≥n cruzada.\nEs por ello, que el conjunto de datos de entrenamiento se refiere a la serie completa conocida ya que internamente va a dividir los datos en varias partes con sus subconjuntos de entrenamiento y test.\n\nresamples_tscv &lt;- time_series_cv(\n  data        = train_data,\n  assess      = h,\n  initial     = 12 * 4 * h,\n  skip        = 12 * 3 * h,\n  slice_limit = 4\n)\n\nresamples_tscv\n\n# Time Series Cross Validation Plan \n# A tibble: 4 √ó 2\n  splits            id    \n  &lt;list&gt;            &lt;chr&gt; \n1 &lt;split [1008/21]&gt; Slice1\n2 &lt;split [1008/21]&gt; Slice2\n3 &lt;split [1008/21]&gt; Slice3\n4 &lt;split [1008/21]&gt; Slice4\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa validaci√≥n cruzada es una t√©cnica utilizada en el an√°lisis de series de tiempo para evaluar la estabilidad de los modelos a lo largo del tiempo. Consiste en dividir los datos de la serie de tiempo en diferentes subconjuntos, aplicar un modelo predictivo a cada subconjunto y luego comparar las predicciones generadas por cada modelo. Es importante por varias razones:\n\nEval√∫a la estabilidad del modelo.\nValida el rendimiento del modelo\nPermite la comparaci√≥n de modelos.\n\n\n\nEn el gr√°fico inferior se observa c√≥mo se gener√≥ la divisi√≥n de los datos para la validaci√≥n cruzada. En color azul se muestra la parte de entrenamiento y en rojo la del test. Posteriormente los modelos seleccionados se aplicar√°n a cada trozo de entrenamiento por separado y gracias a la parte del test se generar√°n las estad√≠sticas que evaluar√°n los modelos en cada trozo. De esta forma, se obtendr√°n 4 estad√≠sticas de rendimiento de cada modelo, lo que ayudar√° a no caer en el sobreajuste.\n\nresamples_tscv %&gt;%\n  tk_time_series_cv_plan() %&gt;%\n  plot_time_series_cv_plan(fecha, valor_zona, .facet_ncol = 2, .interactive = FALSE)"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#modelado",
    "href": "003-crispdm-air-madrid.html#modelado",
    "title": "3¬† An√°lisis de datos de contaminaci√≥n del aire en la ciudad de Madid",
    "section": "3.4 Modelado",
    "text": "3.4 Modelado\n\n\n\n\n\n\nFase IV: selecci√≥n de t√©cnicas de modelado\n\n\n\n\nObjetivos del an√°lisis: identificar si el objetivo es la predicci√≥n, clasificaci√≥n, agrupaci√≥n u otra tarea espec√≠fica.\nNaturaleza de los datos: evaluar la estructura de los datos, la presencia de variables dependientes e independientes, y si se trata de datos espaciales, temporales, etc.\nDisponibilidad de datos: considerar la cantidad y calidad de los datos disponibles, as√≠ como los recursos computacionales y el tiempo requerido para entrenar los modelos.\nConocimiento del dominio: utilizar el conocimiento experto en contaminaci√≥n del aire para seleccionar t√©cnicas relevantes y realizar ajustes apropiados.\n\n\n\n\n\n\n\n\n\nLibrer√≠as y funciones en R\n\n\n\nTidymodeling + tidymodels + modeltime + modeltime.resample\nBase Models + earth + glmnet + xgboost + lightgbm\nCore Packages + tidyverse + lubridate + timetk\n+ bonsai\nset.seed(160191)\n\n\n\n\n\n\n\nProceso de modelado en ciencia de datos\n\n\n\n\nEl principal paquete que se utilzar√° en el modelado de los datos es el paquete modeltime de R, apropiado para series temporales como es la distribuci√≥n del NO2 durante 10 a√±os con mediciones diarias. modeltime ofrece una variedad de herramientas y t√©cnicas avanzadas para el pron√≥stico de series temporales. Est√° dise√±ado para simplificar el proceso de construcci√≥n, evaluaci√≥n y despliegue de modelos de series temporales.\nCon modeltime, los usuarios pueden realizar tareas como la selecci√≥n autom√°tica de modelos, la construcci√≥n de modelos en paralelo, la agregaci√≥n de modelos, la validaci√≥n cruzada y la generaci√≥n de pron√≥sticos. Tambi√©n ofrece una interfaz intuitiva para ajustar y personalizar modelos de series temporales, lo que facilita su adaptaci√≥n a diferentes escenarios. Adem√°s, permite la integraci√≥n con otras bibliotecas populares de R, como tidyverse, lo que brinda un enfoque coherente para el preprocesamiento de datos y el an√°lisis exploratorio.\nPara m√°s informaci√≥n sobre la biblioteca v√©ase la documentaci√≥n oficial en sobre modeltime.\n\n# Tidymodeling\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(modeltime.resample)\n\n# Base Models\nlibrary(earth)\nlibrary(glmnet)\nlibrary(xgboost)\nlibrary(lightgbm)\n\n# Core Packages\nlibrary(tidyverse)\nlibrary(lubridate)\n#library(timetk)\nlibrary(bonsai)\n\n\nset.seed(160191)\n\n\n3.4.1 La ‚Äúreceta‚Äù del modelo\nEl paquete recipes nace de la analog√≠a entre preparar una receta de cocina y preprocesar los datos (como si de una receta de libro de cocina se tratase) y el cocinado (modelado).\nSe crea la receta del modelo, siendo la variable dependiente valor_zona y los predictores el resto de valiables del dataset.\n\n#library(recipes)\nrecipe_spec &lt;- recipe(valor_zona ~ ., train_data)\n\n# recipe_spec &lt;- recipe(valor_zona ~ fecha_num + dow + month + quarter +  year + valor_zona_lag1 + valor_zona_lag2 + ... valor_zona_lag21\", train_data)\n\n\n\n3.4.2 Modelos propuestos\nüìâ Modelo 1: Generalized Linear Model with Elastic Net regularization (GLMNET): es una t√©cnica de modelado que combina la regresi√≥n lineal generalizada con la regularizaci√≥n elastic net.\nGLMNET es √∫til cuando se trabaja con conjuntos de datos con un gran n√∫mero de variables predictoras, ya que ayuda a seleccionar y ajustar las variables m√°s relevantes. GLMNET utiliza una combinaci√≥n de la regularizaci√≥n L1 (LASSO) y L2 (Ridge) para penalizar los coeficientes de las variables y controlar la complejidad del modelo. Esto permite seleccionar autom√°ticamente las variables m√°s relevantes y reducir la tendencia al sobreajuste. Por ejemplo, supongamos que se desea predecir el precio de la vivienda en funci√≥n de m√∫ltiples variables, como el tama√±o, el n√∫mero de habitaciones, la ubicaci√≥n, etc. Aplicando GLMNET, se identifican las variables m√°s importantes para predecir el precio y ajustar el modelo de regresi√≥n lineal generalizada al mismo tiempo.\nEn el caso NO2 se procede con la siguiente especificaci√≥n:\n\n# Especificaci√≥n\nmodel_spec_glmnet &lt;- linear_reg(penalty = 0.04) %&gt;%\n  set_engine(\"glmnet\")\n\n# Ajuste\nworkflow_fit_glmnet &lt;- workflow() %&gt;%\n  add_model(model_spec_glmnet) %&gt;%\n  add_recipe(recipe_spec %&gt;% step_rm(fecha)) %&gt;%\n  fit(train_data) %&gt;%\n    recursive(\n      transform  = lag_transformer,\n      train_tail = tail(train_data, h))\n\n\n\n\n\n\n\nNote\n\n\n\nN√≥tese que cuando se determina el workflow del modelo se desestima la variable fecha aplicando una modificaci√≥n en la receta: add_recipe(recipe_spec %&gt;% step_rm(fecha))\nEsto es debido a que el modelo no soporta variables en formato de fecha (Date).\n\n\nüìä Modelo 2: Multivariate Adaptive Regression Splines (MARS). es una t√©cnica de modelado que utiliza funciones de base para aproximar relaciones no lineales entre variables predictoras y una variable objetivo. Se adapta a los datos mediante la construcci√≥n de una red de segmentos de regresi√≥n que capturan cambios en la relaci√≥n entre las variables.\nPor ejemplo, supongamos que queremos predecir el precio de una vivienda en funci√≥n de variables como el tama√±o, el n√∫mero de habitaciones y la ubicaci√≥n. Al aplicar el modelo MARS, √©ste identificar√° los segmentos de regresi√≥n √≥ptimos para cada variable y construir√° un modelo no lineal que se ajuste mejor a los datos. Esto permite capturar relaciones complejas y no lineales entre las variables predictoras y la variable objetivo.\nEste tipo de modelos tambi√©n puede aplicarse para la modelizaci√≥n de la calidad de aire. Garc√≠a Nieto and √Ålvarez Ant√≥n (2014) modeliz√≥ la calidad del aire de Gij√≥n con el modelo MARS. El estudio explora el uso de este algoritmo de regresi√≥n no param√©trico que puede aproximar la relaci√≥n entre las variables de entrada y salida y expresarla matem√°ticamente. Se recopil√≥ un conjunto de datos experimental de contaminantes del aire peligrosos durante tres a√±os (2006-2008) y se utiliz√≥ para crear un modelo altamente no lineal de la calidad del aire en el √°rea urbana de Gij√≥n basado en la t√©cnica de MARS.\nEn el caso NO2 se procede con la siguiente especificaci√≥n:\n\n# Especificaci√≥n\nmodel_spec_mars &lt;- mars(mode = \"regression\") %&gt;%\n  set_engine(\"earth\")\n\n# Ajuste\nworkflow_fit_mars &lt;- workflow() %&gt;%\n  add_model(model_spec_mars) %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  fit(train_data) %&gt;%\n    recursive(\n      transform  = lag_transformer,\n      train_tail = tail(train_data, h))\n\nüìâ Modelo 3: LightGBM: es un modelo de aprendizaje autom√°tico basado en √°rboles de decisi√≥n que se destaca por su eficiencia y velocidad en el procesamiento de grandes vol√∫menes de datos. Utiliza el algoritmo de refuerzo (boosting) para construir una serie de √°rboles de decisi√≥n que se combinan para mejorar la precisi√≥n de las predicciones.\nUna de las principales caracter√≠sticas de LightGBM es su capacidad para manejar datos con alta dimensionalidad y realizar una selecci√≥n autom√°tica de caracter√≠sticas importantes. Tambi√©n utiliza un enfoque de partici√≥n de datos basado en hojas, lo que permite una mayor eficiencia en el proceso de entrenamiento.\nUn ejemplo de aplicaci√≥n de LightGBM puede ser en la predicci√≥n de la satisfacci√≥n del cliente en una empresa de comercio electr√≥nico. Se pueden utilizar caracter√≠sticas como el historial de compras, el tiempo de respuesta del servicio al cliente y la interacci√≥n en redes sociales para predecir la satisfacci√≥n del cliente. Al entrenar un modelo LightGBM con estos datos, se puede obtener un modelo eficiente y preciso que permita identificar patrones y factores clave que influyen en la satisfacci√≥n del cliente.\nEn el caso NO2 se procede con la siguiente especificaci√≥n:\n\n# Especificaci√≥n\nmodel_spec_lightgbm &lt;- boost_tree(mode = \"regression\",\n                                  trees = 400,\n                                  learn_rate = 0.008,\n                                  tree_depth = 12,\n                                  min_n = 50) %&gt;%\n  set_engine(\"lightgbm\")\n\n# Ajuste\nworkflow_fit_lightgbm &lt;- workflow() %&gt;%\n  add_model(model_spec_lightgbm) %&gt;%\n  add_recipe(recipe_spec %&gt;% step_rm(fecha)) %&gt;%\n  fit(train_data) %&gt;%\n    recursive(\n      transform  = lag_transformer,\n      train_tail = tail(train_data, h))\n\n\n\n\n\n\n\nNote\n\n\n\nN√≥tese que cuando se determina el workflow del modelo se desestima la variable fecha aplicando una modificaci√≥n en la receta: add_recipe(recipe_spec %&gt;% step_rm(fecha))\nEsto es debido a que el modelo no soporta variables en formato de fecha (Date).\n\n\nüìä Modelo 4: Prophet boost con crecimiento logar√≠tmico: es una herramienta de predicci√≥n de series temporales desarrollada por Facebook. Al especificar un crecimiento logar√≠tmico en Prophet, se asume que la tasa de crecimiento de la serie temporal disminuye con el tiempo. Esto es √∫til cuando los datos muestran un crecimiento inicial r√°pido seguido de una desaceleraci√≥n.\nPor ejemplo, supongamos que deseamos predecir las ventas de un producto a lo largo del tiempo. Si aplicamos el modelo Prophet con crecimiento logar√≠tmico, capturar√° la tendencia de crecimiento inicial acelerado y luego la desaceleraci√≥n esperada. Esto puede ser √∫til para ajustar mejor la serie temporal y realizar pron√≥sticos m√°s precisos.\nEl modelo Prophet tambi√©n se utiliza en la predicci√≥n del la contaminaci√≥n del aire. Un ejemplo es la predicci√≥n de la contaminaci√≥n en Se√∫l (Shen, Valagolam, and McCalla 2020). El modelo logr√≥ predecir con precisi√≥n la concentraci√≥n de varios contaminantes hasta un a√±o de antelaci√≥n, superando a otros modelos similares.\nEn el caso NO2 se procede con la siguiente especificaci√≥n:\n\n# Especificaci√≥n\nmodel_spec_prophet_boost_log &lt;- prophet_boost(\n  mode = 'regression',\n  growth = 'linear',\n  seasonality_yearly = T,\n  seasonality_weekly = T,\n  seasonality_daily = F,\n  logistic_floor = min(train_data$valor_zona),\n  logistic_cap = max(train_data$valor_zona),\n  changepoint_num = .8,\n  learn_rate = .001,\n  tree_depth = 4,\n  trees = 2000\n) %&gt;%\n  set_engine(\"prophet_xgboost\")\n\n# Ajuste\nworkflow_fit_prophet_boost_log &lt;- workflow() %&gt;%\n  add_model(model_spec_prophet_boost_log) %&gt;%\n  add_recipe(recipe_spec %&gt;% step_rm(fecha_num, dow, month, quarter, year)) %&gt;%\n  fit(train_data) %&gt;%\n    recursive(\n      transform  = lag_transformer,\n      train_tail = tail(train_data, h))\n\n\n\n\n\n\n\nNote\n\n\n\nN√≥tese que cuando se determina el workflow del modelo se desestima la variable fecha_num, dow, month, quarter, year aplicando una modificaci√≥n en la receta: add_recipe(recipe_spec %&gt;% step_rm(fecha_num, dow, month, quarter, year))\nEsto se debe a que el modelo Prophet boost con crecimiento logar√≠tmico ya incluye en su estrucura variables que determinan la estacionalidad de los datos como: seasonality_yearly = T o seasonality_weekly = T.\n\n\nEl siguiente paso es agregar cada uno de los modelos a una tabla de Modeltime utilizando modeltime_table().\n\nmodels_tbl &lt;- modeltime_table(\n  workflow_fit_glmnet,\n  workflow_fit_mars,\n  workflow_fit_lightgbm,\n  workflow_fit_prophet_boost_log\n)\n\nmodels_tbl\n\n# Modeltime Table\n# A tibble: 4 √ó 3\n  .model_id .model     .model_desc                        \n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                              \n1         1 &lt;workflow&gt; RECURSIVE GLMNET                   \n2         2 &lt;workflow&gt; RECURSIVE EARTH                    \n3         3 &lt;workflow&gt; RECURSIVE LIGHTGBM                 \n4         4 &lt;workflow&gt; RECURSIVE PROPHET W/ XGBOOST ERRORS\n\n\nPor √∫ltimo, se realizan las predicciones entrenando los modelos seleccionados para los 4 trozos de datos del plan de validaci√≥n cruzada con el fin de poder comparar los datos de predicci√≥n con los datos reales y obtener una buena evaluaci√≥n de los modelos propuestos.\n\nresamples_fitted &lt;- models_tbl %&gt;%\n  modeltime_fit_resamples(\n    resamples = resamples_tscv,\n    control   = control_resamples(verbose = FALSE)\n  )\n\nresamples_fitted\n\n# Modeltime Table\n# A tibble: 4 √ó 4\n  .model_id .model     .model_desc                         .resample_results\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                               &lt;list&gt;           \n1         1 &lt;workflow&gt; RECURSIVE GLMNET                    &lt;rsmp[+]&gt;        \n2         2 &lt;workflow&gt; RECURSIVE EARTH                     &lt;rsmp[+]&gt;        \n3         3 &lt;workflow&gt; RECURSIVE LIGHTGBM                  &lt;rsmp[+]&gt;        \n4         4 &lt;workflow&gt; RECURSIVE PROPHET W/ XGBOOST ERRORS &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#eval-vali",
    "href": "003-crispdm-air-madrid.html#eval-vali",
    "title": "3¬† An√°lisis de datos de contaminaci√≥n del aire en la ciudad de Madid",
    "section": "3.5 Evaluaci√≥n",
    "text": "3.5 Evaluaci√≥n\n\n\n\n\n\n\nFase V: M√©tricas de evaluaci√≥n de modelos\n\n\n\n\nMAE (Mean Absolute Error): Es la media de las diferencias absolutas entre las predicciones y los valores reales. Un valor m√°s bajo de MAE indica un mejor ajuste del modelo.\nMAPE (Mean Absolute Percentage Error): Es la media de los porcentajes absolutos de error entre las predicciones y los valores reales. Un valor m√°s bajo de MAPE indica una mayor precisi√≥n en las predicciones.\nMASE (Mean Absolute Scaled Error): Es una m√©trica que compara el error absoluto promedio del modelo con el error absoluto promedio de un modelo ingenuo, generalmente un modelo de caminata aleatoria. Un valor de MASE inferior a 1 indica que el modelo es mejor que el modelo de referencia.\nSMAPE (Symmetric Mean Absolute Percentage Error): Es similar al MAPE, pero utiliza el promedio sim√©trico de los porcentajes absolutos de error entre las predicciones y los valores reales. Un valor bajo de SMAPE indica una mayor precisi√≥n en las predicciones.\nRMSE (Root Mean Squared Error): Es la ra√≠z cuadrada de la media de los errores al cuadrado entre las predicciones y los valores reales. Un valor m√°s bajo de RMSE indica una menor dispersi√≥n y un mejor ajuste del modelo.\nRSQ (R-squared): Tambi√©n conocido como coeficiente de determinaci√≥n, es una medida que indica la proporci√≥n de la variabilidad de la variable de respuesta que puede explicar el modelo. Un valor m√°s alto de RSQ indica un mejor ajuste del modelo a los datos observados.\n\n\n\nEn el campo de la ciencia de datos, la evaluaci√≥n de modelos es fundamental para garantizar la calidad y el rendimiento de los modelos utilizados. A medida que los datos se vuelven cada vez m√°s complejos y abundantes, los modelos de machine learning se han convertido en una herramienta poderosa para extraer informaci√≥n y tomar decisiones basadas en datos.\nLa evaluaci√≥n de modelos de machine learning permite determinar la precisi√≥n, la eficacia y la capacidad de generalizaci√≥n de un modelo en funci√≥n de los datos utilizados. Es crucial evaluar los modelos de forma adecuada para evitar problemas como el sobreajuste (overfitting) o el subajuste (underfitting) y para identificar cualquier sesgo o error en los resultados.\nAdem√°s, la evaluaci√≥n de modelos proporciona informaci√≥n valiosa sobre la robustez y la confiabilidad de un modelo en diferentes escenarios y conjuntos de datos. Permite comparar diferentes modelos y enfoques, seleccionar el mejor modelo para una tarea espec√≠fica y realizar mejoras iterativas para optimizar el rendimiento.\nEvaluaci√≥n de la precisi√≥n:\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = FALSE\n    )\n\n\n\n\n\nresamples_fitted %&gt;%\n    modeltime_resample_accuracy(summary_fns = mean) %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      n\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    1\nRECURSIVE GLMNET\nResamples\n4\n7.57\n26.16\n0.88\n23.00\n9.71\n0.38\n    2\nRECURSIVE EARTH\nResamples\n4\n9.78\n36.55\n1.18\n29.75\n11.73\n0.34\n    3\nRECURSIVE LIGHTGBM\nResamples\n4\n7.90\n31.70\n1.04\n26.01\n9.87\n0.41\n    4\nRECURSIVE PROPHET W/ XGBOOST ERRORS\nResamples\n4\n10.77\n39.32\n1.27\n37.83\n13.19\n0.27\n  \n  \n  \n\n\n\n\nLos tres primeros modelos parecen los que mejor se ajustan a los datos. Sin embargo todos los modelos presentan unas m√©tricas con un MAPE bastante elevado. EN este punto habr√≠a que reconsiderar la inclusi√≥n de m√°s regresores.\nEl modelo con las m√©tricas m√°s bajas de error es el GLMNET.\nUna vez seleccionado el modelo, el siguiente paso deseado ser√≠a la predicci√≥n a futuro del valor del NO2.\n\nworkflow_fit_lightgbm %&gt;% \n  modeltime_table() %&gt;%\n  modeltime_forecast(\n    new_data    = future_data,\n    actual_data = no2_zona_m30_ext,\n    keep_data   = TRUE\n  ) %&gt;%\n  filter(fecha &gt;= \"2023-01-01\") %&gt;% \n  plot_modeltime_forecast(\n    .conf_interval_show = FALSE\n  )"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#despliegue",
    "href": "003-crispdm-air-madrid.html#despliegue",
    "title": "3¬† An√°lisis de datos de contaminaci√≥n del aire en la ciudad de Madid",
    "section": "3.6 Despliegue",
    "text": "3.6 Despliegue\n\n\n\n\n\n\nDespliegue de resultados\n\n\n\nPresentaci√≥n de los resultados del an√°lisis de manera clara y comprensible.\n\nCreaci√≥n de una ShinyApp: se puede construir una aplicaci√≥n interactiva que permita a los usuarios explorar y visualizar los datos de contaminaci√≥n del aire, as√≠ como interactuar con los modelos y los resultados obtenidos.\nImplementaci√≥n de un bot de Twitter: es posible desarrollar un bot de Twitter (rtweet, por ejemplo) que publique actualizaciones y resultados relacionados con la contaminaci√≥n del aire, lo que permite difundir la informaci√≥n de manera efectiva.\nComunicaci√≥n de los hallazgos y recomendaciones: Utilizando RMarkdown y RStudio, se pueden crear informes reproducibles que combinen el c√≥digo, los resultados y las visualizaciones en un documento HTML, PDF u otro formato, lo que facilita la comunicaci√≥n y la presentaci√≥n de los hallazgos del an√°lisis.\n\n\n\n\n\n\n\n\n\nLibrar√≠as y funciones en R\n\n\n\n\nshyni\nflexdasboard\n\n\n\nEjemplo de creaci√≥n de una üì∫ ShinyApp en R:\n# Carga de la librer√≠a shiny\nlibrary(shiny)\n\n# Definici√≥n de la interfaz de la ShinyApp\nui &lt;- fluidPage(\n  titlePanel(\"An√°lisis de Contaminaci√≥n del Aire\"),\n  sidebarLayout(\n    sidebarPanel(\n      # Aqu√≠ se pueden agregar controles para interactuar con los datos y modelos\n    ),\n    mainPanel(\n      # Aqu√≠ se pueden mostrar las visualizaciones y los resultados obtenidos\n    )\n  )\n)\n\n# Definici√≥n de la l√≥gica de la ShinyApp\nserver &lt;- function(input, output) {\n  # Aqu√≠ se puede incluir el c√≥digo para cargar los datos, entrenar modelos, etc.\n}\n\n# Creaci√≥n de la ShinyApp\nshinyApp(ui = ui, server = server)\nLa etapa de ‚ÄúImplementaci√≥n‚Äù es el sexto y √∫ltimo paso en el proceso de CRISP-DM. En esta etapa, debemos implementar el modelo seleccionado y seguir sus resultados. Esto puede incluir la generaci√≥n de informes y visualizaciones para comunicar los resultados del an√°lisis.\nEn el caso de un an√°lisis de datos sobre la contaminaci√≥n del aire utilizando R, podr√≠amos utilizar diferentes t√©cnicas para implementar y comunicar los resultados de nuestros modelos. Por ejemplo, si hemos ajustado un modelo de regresi√≥n, podr√≠amos utilizar funciones como predict para generar predicciones con nuestro modelo y librer√≠as como ggplot2 para visualizar los resultados. Por ejemplo:\npredicciones &lt;- predict(modelo, newdata = datos_test)\ndatos_test$predicciones &lt;- predicciones\n\nggplot(datos_test, aes(x = fecha, y = contaminacion)) +\n  geom_point() +\n  geom_line(aes(y = predicciones), color = \"red\")\nSi hemos ajustado un modelo de series temporales, podr√≠amos utilizar funciones como forecast para generar predicciones con nuestro modelo y librer√≠as como ggplot2 para visualizar los resultados. Por ejemplo:\npredicciones &lt;- forecast(modelo, h = length(datos_test$contaminacion))\ndatos_test$predicciones &lt;- predicciones$mean\n\nggplot(datos_test, aes(x = fecha, y = contaminacion)) +\n  geom_point() +\n  geom_line(aes(y = predicciones), color = \"red\")\nTambi√©n podemos generar informes y visualizaciones para comunicar los resultados de nuestro an√°lisis a otras personas. Por ejemplo, podr√≠amos utilizar librer√≠as como rmarkdown o shiny para crear informes din√°micos o aplicaciones interactivas que muestren los resultados de nuestro an√°lisis.\n\n\n\n\nGarc√≠a Nieto, P. J., and J. C. √Ålvarez Ant√≥n. 2014. ‚ÄúNonlinear Air Quality Modeling Using Multivariate Adaptive Regression Splines in Gij√≥n Urban Area (Northern Spain) at Local Scale.‚Äù Applied Mathematics and Computation 235 (May): 50‚Äì65. https://doi.org/10.1016/j.amc.2014.02.096.\n\n\nHyndman, & Athanasopoulos, R. J. 2021. Forecasting: Principles and Practice, 3rd Edition. OTexts. http://OTexts.com/fpp3.\n\n\nNordhausen, Klaus. 2009. ‚ÄúThe Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition by Trevor Hastie, Robert Tibshirani, Jerome Friedman.‚Äù International Statistical Review 77 (3): 482‚Äì82. https://doi.org/10.1111/j.1751-5823.2009.00095_18.x.\n\n\nShen, Justin, Davesh Valagolam, and Serena McCalla. 2020. ‚ÄúProphet Forecasting Model: A Machine Learning Approach to Predict the Concentration of Air Pollutants (PM2.5, PM10, O3, NO2, SO2, CO) in Seoul, South Korea.‚Äù PeerJ 8 (September): e9961. https://doi.org/10.7717/peerj.9961.\n\n\nShumway, Robert H., and David S. Stoffer. 2017. Time Series Analysis and Its Applications. Springer International Publishing. https://doi.org/10.1007/978-3-319-52452-8."
  },
  {
    "objectID": "003-crispdm-air-madrid.html#footnotes",
    "href": "003-crispdm-air-madrid.html#footnotes",
    "title": "3¬† An√°lisis de datos de contaminaci√≥n del aire en la ciudad de Madid",
    "section": "",
    "text": "El sobreajuste (overfitting en ingl√©s) en el contexto del machine learning se refiere a un escenario en el cual un modelo se ajusta demasiado a los datos de entrenamiento y pierde su capacidad de generalizaci√≥n en datos nuevos e invisibles. En otras palabras, el modelo aprende y se ajusta demasiado a las particularidades y el ruido presentes en los datos de entrenamiento, en lugar de capturar los patrones y relaciones generales que se aplicar√≠an a otros conjuntos de datos (Nordhausen 2009).‚Ü©Ô∏é"
  },
  {
    "objectID": "00a-pre-curso.html#instalaci√≥n-de-r-y-rstudio",
    "href": "00a-pre-curso.html#instalaci√≥n-de-r-y-rstudio",
    "title": "Appendix A ‚Äî Preparaci√≥n del entorno de trabajo",
    "section": "A.1 Instalaci√≥n de R y RStudio",
    "text": "A.1 Instalaci√≥n de R y RStudio\nSe proporcionar√°n instrucciones detalladas sobre c√≥mo instalar R y RStudio en diferentes sistemas operativos, como Windows, macOS y Linux. Se explicar√° el proceso paso a paso y se destacar√°n los enlaces de descarga y recursos adicionales para obtener m√°s informaci√≥n.\n\nA.1.1 Descarga e instalaci√≥n de R\nR est√° mantenido por un equipo internacional de desarrolladores que hacen que el lenguaje est√© disponible a trav√©s de la p√°gina web de The Comprehensive R Archive Network. En la parte superior de la p√°gina web se encuentran tres enlaces para descargar R. Simplemente debes hacer clic en el enlace que corresponda a tu sistema operativo: Windows, macOS o Linux.\n\nA.1.1.1 Windows\nPara instalar R en Windows, haz clic en el enlace Download R for Windows. Luego, haz clic en el enlace base. A continuaci√≥n, haz clic en el primer enlace en la parte superior de la nueva p√°gina. Este enlace deber√≠a decir algo como Download R-4.3.1 for Windows, excepto que el 4.3.1 ser√° reemplazado por la versi√≥n m√°s actualizada de R. El enlace descargar√° un programa de instalaci√≥n que instalar√° la versi√≥n m√°s reciente de R para Windows. Ejecuta este programa y sigue los pasos del asistente de instalaci√≥n que aparece. El asistente instalar√° R en las carpetas de tus archivos de programa y colocar√° un acceso directo en tu men√∫ de inicio. Ten en cuenta que necesitar√°s tener todos los privilegios de administraci√≥n apropiados para instalar nuevo software en tu m√°quina. Adem√°s se recomienda la instalaci√≥n de Rtools.\n\n\n\n\n\n\nNote\n\n\n\nRtools es un conjunto de herramientas utilizado para construir paquetes de R desde el c√≥digo fuente (aquellos que requieren la compilaci√≥n de c√≥digo C/C++ o Fortran) y para construir R en s√≠ mismo. Rtools43 se utiliza para R 4.3.x y para R-devel, la versi√≥n de desarrollo de R.\n\n\n\n\n\n\n\n\nInstalaci√≥n de Rtools43\n\n\n\n\n\nRtools43 solo es necesario para la instalaci√≥n de paquetes de R desde el c√≥digo fuente o la construcci√≥n de R desde el c√≥digo fuente. R se puede instalar desde el instalador binario de R y, de forma predeterminada, se instalar√°n versiones binarias de los paquetes de CRAN, lo cual no requiere Rtools43.\nAdem√°s, existen servicios de construcci√≥n en l√≠nea disponibles para verificar y construir paquetes de R para Windows, para los cuales tampoco es necesario instalar Rtools43 localmente. El servicio de verificaci√≥n de Winbuilder utiliza una configuraci√≥n id√©ntica a la de las verificaciones de paquetes entrantes de CRAN y ya tiene preinstalados todos los paquetes de CRAN y Bioconductor.\nRtools43 se puede instalar desde el instalador de Rtools43. Se recomienda utilizar los valores predeterminados, incluida la ubicaci√≥n de instalaci√≥n predeterminada en C:\\rtools43.\nCuando se utiliza R instalado mediante el instalador, no es necesario realizar ninguna configuraci√≥n adicional despu√©s de instalar Rtools43 para construir paquetes de R desde el c√≥digo fuente. Cuando se utiliza la ubicaci√≥n de instalaci√≥n predeterminada, R y Rtools43 se pueden instalar en cualquier orden y Rtools43 se puede instalar incluso si R ya est√° en ejecuci√≥n.\n\n\n\n\n\nA.1.1.2 macOS\nPara instalar R en una Mac, haz clic en el enlace Download R for macOS. A continuaci√≥n, haz clic en el enlace del paquete R-4.3.1 (o el enlace del paquete para la versi√≥n m√°s actualizada de R). Se descargar√° un instalador que te guiar√° a trav√©s del proceso de instalaci√≥n, el cual es muy f√°cil. El instalador te permite personalizar tu instalaci√≥n, pero la configuraci√≥n predeterminada ser√° adecuada para la mayor√≠a de los usuarios.\n\n\nA.1.1.3 Linux - Ubuntu\nLos paquetes para la versi√≥n actual de R 4.2 est√°n disponibles para la mayor√≠a de las versiones estables de Ubuntu para escritorio hasta su fecha oficial de fin de vida. Sin embargo, solo se brinda soporte completo para la √∫ltima versi√≥n de Soporte a Largo Plazo (LTS). A partir del 2 de mayo de 2022, las versiones compatibles son:\n\nJammy Jellyfish (22.04, solo amd64)\nImpish Indri (21.10, solo amd64)\nFocal Fossa (20.04, solo LTS y amd64)\nBionic Beaver (18.04, LTS)\nXenial Xerus (16.04, LTS)\n\nEjecuta estas l√≠neas (si eres root, omite el comando sudo) para informar a Ubuntu sobre los archivos binarios de R en CRAN.\n\n\nTerminal\n\n# update indices\nsudo apt update -qq\n# install two helper packages we need\nsudo apt install --no-install-recommends software-properties-common dirmngr\n# add the signing key (by Michael Rutter) for these repos\n# To verify key, run gpg --show-keys /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc \n# Fingerprint: E298A3A825C0D65DFD57CBB651716619E084DAB9\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\n# add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed\nsudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\"\n\nAqu√≠ utilizamos el comando ‚Äúlsb_release -cs‚Äù para acceder a la versi√≥n de Ubuntu que est√°s utilizando, que puede ser una de las siguientes: ‚Äújammy‚Äù, ‚Äúimpish‚Äù, ‚Äúfocal‚Äù, ‚Äúbionic‚Äù, ‚Ä¶\nLuego ejecuta:\n\n\nTerminal\n\nsudo apt install --no-install-recommends r-base\n\npara instalar R y sus dependencias.\nPara obtener instrucciones m√°s detalladas, incluyendo informaci√≥n sobre la administraci√≥n y el mantenimiento de paquetes de R, consulta el README completo.\nPara versiones antiguas de R, consulta el README correspondiente.\n\n\n\nA.1.2 Descarga e instalaci√≥n de RStudio\nRStudio es un entorno de desarrollo integrado (IDE) dise√±ado espec√≠ficamente para trabajar con el lenguaje de programaci√≥n R. Es una aplicaci√≥n que proporciona una interfaz f√°cil de usar y funciones adicionales para facilitar la escritura, depuraci√≥n y ejecuci√≥n de c√≥digo en R. RStudio ofrece un entorno de trabajo organizado con paneles que incluyen una consola interactiva para ejecutar comandos en tiempo real, una ventana para ver y editar archivos de c√≥digo, herramientas de visualizaci√≥n de datos y mucho m√°s. Es una herramienta muy popular y ampliamente utilizada por profesionales y estudiantes en el an√°lisis de datos y la programaci√≥n en R.\n\nA.1.2.1 Windows\n\nVe al sitio web de RStudio en https://www.rstudio.com/products/rstudio/download/.\nEn la secci√≥n ‚ÄúRStudio Desktop‚Äù, haz clic en ‚ÄúDownload‚Äù debajo de la versi√≥n gratuita de RStudio.\nSelecciona la descarga correspondiente a tu sistema operativo de Windows (32 o 64 bits).\nUna vez descargado, ejecuta el archivo de instalaci√≥n.\nSigue las instrucciones del asistente de instalaci√≥n y acepta los t√©rminos y condiciones.\nDespu√©s de completar la instalaci√≥n, podr√°s abrir RStudio desde el men√∫ de inicio o mediante un acceso directo en el escritorio.\n\n\n\nA.1.2.2 macOS\n\nVisita el sitio web de RStudio en https://www.rstudio.com/products/rstudio/download/.\nEn la secci√≥n ‚ÄúRStudio Desktop‚Äù, haz clic en ‚ÄúDownload‚Äù debajo de la versi√≥n gratuita de RStudio.\nDescarga el archivo de instalaci√≥n correspondiente a macOS.\nUna vez descargado, abre el archivo de instalaci√≥n.\nArrastra el √≠cono de RStudio a la carpeta ‚ÄúApplications‚Äù para completar la instalaci√≥n.\nPuedes abrir RStudio desde Launchpad o desde la carpeta ‚ÄúApplications‚Äù.\n\n\n\nA.1.2.3 Linux\n\nEn el sitio web de RStudio en https://www.rstudio.com/products/rstudio/download/, selecciona la descarga adecuada para tu distribuci√≥n de Linux.\nDependiendo de tu distribuci√≥n, puedes seguir las instrucciones espec√≠ficas para instalar RStudio en Linux. Estas instrucciones suelen incluir comandos de terminal.\nUna vez instalado, puedes abrir RStudio desde el men√∫ de aplicaciones o utilizando el comando ‚Äúrstudio‚Äù en la terminal.\n\n\n\n\n\n\n\nCaution\n\n\n\nRecuerda que RStudio requiere tener previamente instalado R en tu sistema operativo, ya que RStudio es un entorno que se utiliza para trabajar con el lenguaje R. Si a√∫n no tienes instalado R, puedes descargarlo desde el sitio web de CRAN (Comprehensive R Archive Network) y luego instalar RStudio."
  },
  {
    "objectID": "00a-pre-curso.html#configuraci√≥n-de-librer√≠as-que-tienen-que-venir-instalados",
    "href": "00a-pre-curso.html#configuraci√≥n-de-librer√≠as-que-tienen-que-venir-instalados",
    "title": "Appendix A ‚Äî Preparaci√≥n del entorno de trabajo",
    "section": "A.2 Configuraci√≥n de librer√≠as que tienen que venir instalados",
    "text": "A.2 Configuraci√≥n de librer√≠as que tienen que venir instalados\nEste apartado, se centra en las las librer√≠as esenciales que se van a necesitar para seguir estelibro. Estas librer√≠as son ampliamente utilizadas en el an√°lisis de datos, el modelado estad√≠stico y el aprendizaje autom√°tico. Se agrupan por tipolog√≠a para facilitar su comprensi√≥n. A continuaci√≥n, se explica c√≥mo obtener e instalar cada una de ellas, adem√°s de proporcionar una breve descripci√≥n de su funcionalidad.\n\nLibrer√≠as para el manejo de series temporales:\n\n\nmodeltime: Esta librer√≠a proporciona herramientas para la creaci√≥n, manipulaci√≥n y visualizaci√≥n de modelos de series temporales en R. Puedes obtener m√°s informaci√≥n y descargar la librer√≠a desde el sitio oficial en GitHub\nmodeltime.resample: Es una extensi√≥n de la librer√≠a modeltime que ofrece funciones para el muestreo y la reescalada de series temporales.\n\n\nLibrer√≠as para el modelado estad√≠stico:\n\n\ntidymodels: Es un conjunto de librer√≠as dise√±adas para facilitar el flujo de trabajo en el modelado estad√≠stico. Proporciona una interfaz consistente para ajustar, evaluar y comparar diferentes modelos.\nearth: Esta librer√≠a implementa modelos de regresi√≥n basados en funciones suaves y proporciona una interpretaci√≥n m√°s f√°cil de los modelos\nglmnet: Es una librer√≠a que implementa algoritmos eficientes para el ajuste de modelos de regresi√≥n lineal y log√≠stica con regularizaci√≥n L1 y L2.\nxgboost: Es una librer√≠a de aprendizaje autom√°tico que implementa el algoritmo de Gradient Boosting. Proporciona una implementaci√≥n r√°pida y eficiente del algoritmo.\nlightgbm: Es otra librer√≠a de Gradient Boosting que se centra en la eficiencia y la velocidad de entrenamiento. Puedes instalarla con el comando install.packages(‚Äúlightgbm‚Äù).\n\n\nLibrer√≠as para el manejo de datos y visualizaci√≥n:\n\n\ntidyverse: Es un conjunto de librer√≠as que incluye ggplot2, dplyr, tidyr y otras herramientas para el manejo de datos y la visualizaci√≥n en R.\nlubridate: Esta librer√≠a proporciona funciones para el manejo de fechas y horas en R. Puedes instalarla con el comando install.packages(‚Äúlubridate‚Äù).\nbonsai: Es una librer√≠a para el an√°lisis de datos de series temporales y la creaci√≥n de modelos predictivos. Puedes obtener m√°s informaci√≥n y descargarla desde el sitio oficial en GitHub: https://github.com/bonsai-team/bonsai\n\n\nLibrer√≠as adicionales:\n\n\nknitr: Es una librer√≠a utilizada para la generaci√≥n de informes din√°micos en R.\nxml2: Es una librer√≠a para el manejo de datos XML en R.\nvroom: Esta librer√≠a permite la lectura r√°pida de grandes conjuntos de datos en R.\ntimetk: Es una librer√≠a para la manipulaci√≥n de series temporales en R. Proporciona funciones para la limpieza, transformaci√≥n y visualizaci√≥n de datos temporales.\n\n\n# Instalaci√≥n de las bibliotecas necesarias\ninstall.packages(\"modeltime\")\ninstall.packages(\"modeltime.resample\")\ninstall.packages(\"tidymodels\")\ninstall.packages(\"earth\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"xgboost\")\ninstall.packages(\"lightgbm\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"bonsai\")\ninstall.packages(\"knitr\")\ninstall.packages(\"xml2\")\ninstall.packages(\"vroom\")\ninstall.packages(\"timetk\")"
  }
]