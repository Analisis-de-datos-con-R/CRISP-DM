[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introducción al análisis estadístico de datos con R",
    "section": "",
    "text": "Prologo\nEn los últimos años, la cantidad de datos generados en todo el mundo ha crecido exponencialmente, abriendo un mundo de posibilidades para la toma de decisiones en base información valiosa extraída de los datos. En este contexto, la ciencia de datos se ha convertido en una disciplina fundamental para descubrir patrones, tendencias y relaciones ocultas en grandes conjuntos de datos."
  },
  {
    "objectID": "index.html#objetivos-del-libro",
    "href": "index.html#objetivos-del-libro",
    "title": "Introducción al análisis estadístico de datos con R",
    "section": "Objetivos del libro",
    "text": "Objetivos del libro\nEl objetivo principal de este libro es proporcionar a los lectores una guía práctica llevara a cabo un análisis estadístico en ciencia de datos basado en la metodología Cross-Industry Standard Process for Data Mining (CRISP-DM), un modelo de proceso de minería de datos de estándar abierto.\nCentrado en el uso del lenguaje de programación R y el IDE RStudio, este libro pretende proporcionar a los lectores las herramientas necesarias para llevar a cabo un análisis estadístico con datos de contaminación del aire en la ciudad de Madrid. El enfoque estará orientado hacia la reproducibilidad y la gobernanza del dato, garantizando así la integridad y la transparencia en el análisis."
  },
  {
    "objectID": "index.html#estructura-del-libro",
    "href": "index.html#estructura-del-libro",
    "title": "Introducción al análisis estadístico de datos con R",
    "section": "Estructura del libro",
    "text": "Estructura del libro\nEl libro se divide en tres capítulos:\n\nIntroducción: las bases de un análisis de datos con R.\nToolkit para la ciencia de datos con R.\nAnálisis de datos de contaminación del aire en la ciudad de Madid."
  },
  {
    "objectID": "index.html#enfoque-del-manual",
    "href": "index.html#enfoque-del-manual",
    "title": "Introducción al análisis estadístico de datos con R",
    "section": "Enfoque del manual",
    "text": "Enfoque del manual\nEl enfoque teórico-práctico adoptado en este libro permitirá a los lectores adquirir los conocimientos fundamentales para aplicar eficazmente las técnicas de minería de datos en el contexto de la contaminación atmosférica. Desde la comprensión de los fundamentos de CRISP-DM hasta la aplicación de modelos predictivos y la validación de los resultados, cada paso del proceso se aborda de forma clara y concisa.\nUno de los puntos fuertes de esta obra es su énfasis en la reproducibilidad y la gobernanza de los datos. Se ofrecen ejemplos prácticos en R y RStudio que permiten a los lectores aplicar los conceptos teóricos y seguir el proceso de análisis de forma rigurosa y transparente. Además, se aborda el despliegue de los resultados a través de diferentes enfoques, como el desarrollo de una ShinyApp o la generación de informes reproducibles.\n\n¡Bienvenidos a este apasionante viaje de descubrimiento y análisis de datos sobre contaminación atmosférica utilizando CRISP-DM y el potente lenguaje de programación R!\nGema Fernández-Avilés Michal Kinel"
  },
  {
    "objectID": "index.html#licencia-y-copyright",
    "href": "index.html#licencia-y-copyright",
    "title": "Introducción al análisis estadístico de datos con R",
    "section": "Licencia y copyright",
    "text": "Licencia y copyright\nGuía práctica para el análisis de datos con R mediante la aplicación de la metodología CRISP-DM en R © 2023 de Gema Fernández-Avilés y Michal Kinel está licenciado bajo una licencia de Creative Commons Reconocimiento-NoComercial-CompartirIgual 4.0 Internacional.\nTodos los logotipos y marcas comerciales que puedan aparecer en este texto son propiedad de sus respectivos dueños y se incluyen en este texto únicamente con fines didácticos."
  },
  {
    "objectID": "001-Introduccion.html#qué-vas-a-apernder",
    "href": "001-Introduccion.html#qué-vas-a-apernder",
    "title": "1  Introducción: las bases de un análisis de datos con R",
    "section": "1.1 ¿Qué vas a apernder?",
    "text": "1.1 ¿Qué vas a apernder?\n\nLos conceptos básicos para llevar a cabo un análisis en ciencia de datos.\nUn overview de las herramientas más importantes del software estadístico R y su potencial no solo para el análisis de datos.\nUna de las metodologías más utilizada en ciencia de datos: la metodología Cross Industry Standard Process for Data Mining (CRISP-DM por sus siglas en inglés).\nCómo desarrollar un caso de uso en ciencia de datos aplicando la metodologías CRISP-DM a un problema actual de importante repercusión económica, sanitaria y social, el control de la calidad del aire."
  },
  {
    "objectID": "001-Introduccion.html#qué-no-vas-a-aprender",
    "href": "001-Introduccion.html#qué-no-vas-a-aprender",
    "title": "1  Introducción: las bases de un análisis de datos con R",
    "section": "1.2 ¿Qué no vas a aprender?",
    "text": "1.2 ¿Qué no vas a aprender?\n\nProgramación en R\nEscritura en LaTeX.\nContenidos teóricos de estadística y ciencia de datos."
  },
  {
    "objectID": "001-Introduccion.html#qué-es-la-ciencia-de-datos",
    "href": "001-Introduccion.html#qué-es-la-ciencia-de-datos",
    "title": "1  Introducción: las bases de un análisis de datos con R",
    "section": "1.3 ¿Qué es la ciencia de datos?",
    "text": "1.3 ¿Qué es la ciencia de datos?\nLa ciencia de datos es una disciplina emergente que ha sido etiquetada como la profesión más sexy del siglo XXI (Davenport and Patil 2012). Mediante la combinación de modelos matemáticos y estadísticos, la programación computacional y el conocimiento del negocio o área de aplicación (véase Figure 1.1), la ciencia de datos obtiene el máximo valor de los datos para apoyar los procesos de toma de decisiones. Es decir, transforma los datos en información y la información en conocimiento.\n\n\n\n\n\nFigura 1.1: Diagrama de Venn de la ciencia de datos"
  },
  {
    "objectID": "001-Introduccion.html#cuáles-son-las-herramientas-básicas-de-r-para-ciencia-de-datos",
    "href": "001-Introduccion.html#cuáles-son-las-herramientas-básicas-de-r-para-ciencia-de-datos",
    "title": "1  Introducción: las bases de un análisis de datos con R",
    "section": "1.4 ¿Cuáles son las herramientas básicas de R para ciencia de datos?",
    "text": "1.4 ¿Cuáles son las herramientas básicas de R para ciencia de datos?\nR es una caja de herramientas en el sentido más amplio (tal y com ilustra la Figure 1.2), pues permite llevar a cabo muchas aplicaciones y muy diferentes. Entre ellas, para hacer una idea al lector, con R es posible: sumar o restar, ejecutar modelos ya programados, desarrollar funciones nuevas, dibujar datos y resultados de forma estática y/o interactiva, crear informes y fomentar la reproducibilidad, diseñar y publicar blogs, páginas webs, dashboards, cuadros de mando, etc…\n\n\n\n\n\nFigura 1.2: La caja de herramientas de R\n\n\n\n\nR tiene tres grandes pilares que son:\n\nRstudio: un entorno de desarrollo integrado (IDE) para R (y Python) dedicado a la computación estadística y gráficos. Incluye (Rstudio):\n\nuna consola,\nun editor sintaxis en color que apoya la ejecución de código,\nun entrono para las variables y\nun conjunto de utilidades.\n\nTidyverse: una colección de paquetes coherentes, que comparten gramática, filosofía y estructura y están diseñados para realizar juntos como una canalización completa (pipeline). Todos se basan en la idea de tidy data propuesta por Hadley Wickham\n\ny pueden instalarse con un único comando en R:\n\n\n\ninstall.packages(\"tidyverse\")\n\nLos paquetes que forman parte del tidyverse son:\n\nreadr: importación de datos.\ndplyr: manipulación de datos.\ntidyr: ordenación de datos.\nggplot2 visualización de datos.\npurrr: programación.\ntibble: para tibbles, un nuevo formato de data frames.\nstringr: para caracteres.\nforcats: para factores.\n\n\nQuarto/Rmarkdown: un marco de escritura para ciencia de datos, que combina código, resultados y comentarios. Los documentos de Quarto (*.qmd) y R Markdown (*.Rmd) son completamente reproducibles y soportan docenas de formatos de salida tales como PDFs, archivos de Word, presentaciones, artículos científicos,… Materiral de recomendable lectura R Markdown Cookbook, Rmarkdown y el nuevo formato multiplataforma integrado de Quarto\n\n\n\n\n\n\n\nNote\n\n\n\nLibro de referencia obligatoria: R for Data Science (en español)"
  },
  {
    "objectID": "001-Introduccion.html#fundamentos",
    "href": "001-Introduccion.html#fundamentos",
    "title": "1  Introducción: las bases de un análisis de datos con R",
    "section": "1.5 Importancia de una metodología en ciencia de datos",
    "text": "1.5 Importancia de una metodología en ciencia de datos\nLa metodología en la ciencia de datos proporciona un camino para encontrar soluciones a un problema específico. Este es un proceso cíclico que sufre un comportamiento crítico y que guía a los analistas de negocios y científicos de datos a actuar en consecuencia.\nCRISP-DM es un modelo estándar ampliamente utilizado en la minería de datos que proporciona un enfoque estructurado para llevar a cabo proyectos de análisis de datos (Wirth and Hipp (2000)). Este modelo se compone de seis fases interconectadas, que abarcan desde la comprensión del negocio hasta la implementación de los resultados del análisis. Las fases de CRISP-DM son:\n\nEntendimiento del negocio.\nComprensión de los datos.\nPreparación de los datos.\nModelado.\nEvaluación.\nDespliegue.\n\n\n\n\n\n\nCiclo de vida de un proceso de ciencia de datos"
  },
  {
    "objectID": "001-Introduccion.html#caso-de-estudio-la-calidad-del-aire-en-la-ciudad-de-madrid",
    "href": "001-Introduccion.html#caso-de-estudio-la-calidad-del-aire-en-la-ciudad-de-madrid",
    "title": "1  Introducción: las bases de un análisis de datos con R",
    "section": "1.6 Caso de estudio: la calidad del aire en la ciudad de Madrid",
    "text": "1.6 Caso de estudio: la calidad del aire en la ciudad de Madrid\nLa contaminación del aire exterior es uno de los principales problemas que afectan a la salud humana en las zonas urbanas de todo el mundo (Sanchis-Marco, Montero, and Fernandez-Aviles (2022)). Aunque las emisiones de la mayoría de los contaminantes del aire han disminuido sustancialmente en las últimas décadas, sus concentraciones aún superan los límites legales en la mayoría de los países, lo que indica que el control de la contaminación del aire sigue siendo un desafío para las sociedades modernas. Cada año, más de 4,2 millones de personas sufren una muerte prematura a causa de la contaminación del aire exterior (OMS, 2016). Los principales culpables son el ozono (O3), el dióxido de nitrógeno (NOx) y, sobre todo, las partículas finas o material particulado (PM) con un diámetro de 10 micrómetros o menos (PM10).\nPor otra parte, Madrid es la tercera ciudad más poblada de la Unión Europea después de Londres y Berlín y cuenta con una gran área metropolitana periférica con más de cinco millones de habitantes. Su potente actividad económica, incluso en tiempos de pandemia de la Covid-19, se traduce en niveles de PM10 y NO2 superiores a los deseados a causa del transporte —en concreto, del tráfico rodado— y de la actividad industrial, que son las principales fuentes de emisión de PM10 (véase Montero and Fernández-Avilés (2018) y Montero, Fernández-Avilés, and Laureti (2021)).\nEn este contexto, el análisis de datos desempeña un papel crucial en la comprensión de la contaminación del aire y sus efectos en la salud humana y el medio ambiente. La disponibilidad de datos geoespaciales permite identificar patrones espaciales, detectar áreas de alta contaminación y evaluar el impacto de las emisiones en diferentes regiones. Resulta muy importante la fácil accesibilidad de los datos para la democratización de la información. Un ejemplo de ello es la página de Datos abiertos del Gobierno de España. Gracias a los datos públicos y el análisis adecuado se facilita la toma de decisiones informadas para abordar y mitigar los problemas de contaminación del aire.\nLa mejor forma para comprender y entender nuevos desarrollos es viendo su utilidad a través de un caso de uso, y en este manual se ha escogido un tema tan importante como es la contaminación del aire en la ciudad de Madrid. Ello quedó de manifiesto los días 14-16 de marzo de 2022 cuando una gran calima cubrió no sólo a Madrid (Figure 1.3) sino a España entera tiñiendo el país de polvo rojo.\n\n\n\n\n\nFigura 1.3: Calidad del aire 15-Marzo-2022\n\n\n\n\nAdemás, el Ayuntamiento de Madrid dispone datos abiertos proporcionados a traves delPortal de datos abiertos del Ayuntamiento de Madrid que permiten su análisis sin coste alguno.\nConcretamente, a través del Sistema Integral de la Calidad del Aire del Ayuntamiento de Madrid (véase Figure 1.4), se pueden descargar los datos de los contaminantes registrados en las estaciones de monitoreo de la ciudad desde 2001 en distintos formatos.\n\n\n\n\n\nFigura 1.4: Web Ayuntamiento de Madrid\n\n\n\n\nTodo hace que quede más que justificado el interés del caso de estudio que se presenta.\n\n1.6.1 Los datos\nDe acuerdo con Wickham and Grolemund (2016), el cual es considerada una referencia clave en el la ciencia de datos (Data Science with R), las herramientas necesarias para un proyecto típico de ciencia de datos sigue el siguiente esquema: importar, ordenar, transformar, visualizar, modelar y comunicar (véase Figure 1.5)\n\n\n\n\n\nFigura 1.5: Ciclo de vida de la ciencia de datos\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPartir con la ingesta y orden de los datos no sería optimo para llevar a cabo el objetivo de este manual, pues el 80% del tiempo es un proceso rutinario y aburrido y el 20% restante es extraño y frustrante.\n\n\nEn el caso que nos ocupa, se recurre al repositorio de Github michal0091/aire_madrid de Kinel (2022a, 2022b) que trata los datos brutos de calidad del aire del Ayuntamiento de Madrid, los organiza y los codifica, facilitando así su uso (Figure 1.6). La salida del conjunto de datos dt_daily_mean_2011.RDS que aquí se proporciona será la entrada para el análisis que se presenta.\n(TODO) descripción breve de lo que contienen estas carpetas, datos procesasdos, sin procear, diccionarios.\n\n\n\n\n\nFigura 1.6: Repositorio Github con el código\n\n\n\n\n\n\n\n\nDavenport, Thomas H, and DJ Patil. 2012. “Data Scientist.” Harvard Business Review 90 (5): 70–76.\n\n\nMontero, José-Marı́a, and Gema Fernández-Avilés. 2018. “Functional Kriging Prediction of Atmospheric Particulate Matter Concentrations in Madrid, Spain: Is the New Monitoring System Masking Potential Public Health Problems?” Journal of Cleaner Production 175: 283–93.\n\n\nMontero, José-Marı́a, Gema Fernández-Avilés, and Tiziana Laureti. 2021. “A Local Spatial STIRPAT Model for Outdoor NOx Concentrations in the Community of Madrid, Spain.” Mathematics 9 (6): 677.\n\n\nSanchis-Marco, Lidia, José-Marı́a Montero, and Gema Fernandez-Aviles. 2022. “An Extended CAViaR Model for Early-Warning of Exceedances of the Air Pollution Standards. The Case of PM10 in the City of Madrid.” Atmospheric Pollution Research 13 (4): 101355.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\".\n\n\nWirth, Rüdiger, and Jochen Hipp. 2000. “CRISP-DM: Towards a Standard Process Model for Data Mining.” In Proceedings of the 4th International Conference on the Practical Applications of Knowledge Discovery and Data Mining, 1:29–39. Manchester."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#r-y-rstudio",
    "href": "002-toolkit-ciencia-datos.html#r-y-rstudio",
    "title": "2  R toolkit para la ciencia de datos",
    "section": "2.1 R y RStudio",
    "text": "2.1 R y RStudio\n\n2.1.1 Instalación de R y RStudio\nR es un entorno de software libre para la computación estadística y los gráficos. Se compila y ejecuta en una amplia variedad de plataformas UNIX, Windows y MacOS. Las fuentes, los binarios y la documentación de R pueden obtenerse a través de CRAN1. Para descargar R elija su espejo CRAN preferido en www.r-project.org.\nRStudio es un entorno de desarrollo integrado (IDE) para R. Incluye una consola, un editor que resalta la sintaxis y admite la ejecución directa del código, así como herramientas para el trazado, el historial, la depuración y la gestión del espacio de trabajo. RStudio está disponible en ediciones comerciales y de código abierto y se ejecuta en el escritorio (Windows, Mac y Linux) o en un navegador conectado a RStudio Server o RStudio Workbench (Debian/Ubuntu, Red Hat/CentOS y SUSE Linux), está disponible en www.rstudio.com.\n\n\n\n\n\n\nNote\n\n\n\nEl orden es importante:\n\nInstale R.\nInstale RStudio.\n\n\n\nCompruebe que tiene instalados los iconos de la Figure 2.1 y abra RStudio.\n\n\n\n\n\nFigura 2.1: Iconos de R y RStudio\n\n\n\n\n\n\n2.1.2 Interfaz de RStudio\nAl abrir el programa se observan cuatro paneles (véase Figure 2.2).\n\nSripts: para escribir código.\nConsola: contiene la Consola, Terminal y los Jobs.\nEntorno: contiene el Environment, History, conexiones y Git, entre otros.\nFiles, Plots, Packages, Help y Viewer\n\nLa posición de los paneles se puede personalizar al gusto del usuario, al igual que la apariencia del IDE.\n\n\n\n\n\nFigura 2.2: Paneles de RStudio\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa posición relativa y el contenido de cada panel pueden personalizarse desde el menú: Tools &gt; Global Options &gt; Pane Layout\n\n\n\n\n2.1.3 Instalación los paquetes\nLos paquetes son adiciones modulares al software R que añaden funcionalidad en forma de nuevas funciones, conjuntos de datos, documentación, etc. El repositorio estándar de paquetes R es “Comprehensive R Archive Network” (CRAN).\n\n\n\n\n\n\nNote\n\n\n\nCRAN Task Views:\nOfrecen un breve resumen de los paquetes incluidos.\n\n\nLos paquetes se instalan con la función base de R install.packages():\n\ninstall.packages(\"tidyverse\")\n\n\n\n\n\n\n\nNote\n\n\n\nLos paquetes se instalan una única vez y se leen cada vez que se utilizan.\n\n\nAlternativamente, se pueden instalar desde el panel de Paquetes en RStudio desde una interfaz gráfica.\n\n\n2.1.4 Proyectos\nSer organizado desde el principio es uno de los mejores hábitos y en R esto se consigue trabajando con proyectos. Un proyecto de R agrupa todo los ficheros del trabajo en una carpeta de forma que se facilita su manejo, el trabajo colaborativo y su reproducibilidad.\n\n\n\n\n\n\nNote\n\n\n\nCrear un proyecto Quarto en R:\nDesde el menú: File &gt; New Project y luego elija:\n\nNuevo Directorio\nProyecto Vacío\nElija un nombre para el directorio, por ejemplo, mi_proyecto\nHaga clic en Crear Proyecto.\n\n\n\n\n📁 data: para los datos.\n📁 img: para las imágenes.\n📂 src: contiene los scripts de R para llevar a cabo las distintas tareas del análisis.\n📁 output: se guardarán los resultados finales de nuestro análisis.\nREADME.md debe contener la información básica sobre el proyecto, la compatibilidad, los inputs que necesita, los outputs que genera y un resumen de flujo del trabajo. Ésto ayudará a cualquiera a comprender mejor cómo es nuestro proyecto.\n\nUna de las estructuras más comunes es la mostrada en la Figure 2.3:\n\n\n\n\n\nFigura 2.3: Ejemplo de estructura de un proyecto en R.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLos distintos scripts se pueden ejecutar desde otros scripts. Para ello se utiliza la función source()\n\n\n\n\n2.1.5 Trabajar con un script de R\nPara comenzar a escribir un script nuevo nos dirigimos a File &gt; New File. Los más utilizados actualmente son:\n\n📓 &gt; R Script.\n📖 &gt; Quarto document.\n📓 &gt; R Notebook.\n📝 &gt; R Markdown.\n\n\n\n\n\n\n\nNote\n\n\n\nSi se tiene creado un proyecto Quarto mejor trabajar con un un quarto document para luego renderizar todo el documento. Véase Wickham and Grolemund (2016) para un estudio profundo de proyectos.\n\n\nEl editor es un editor de texto plano (sin negritas ni cursivas) pero ofrece un código de colores del texto dependiendo de lo que se escriba (resaltado de sintaxis).\nEs extremadamente útil comentar el script con información adicional para hacer más claro el proceso de programación. Para añadir un comentario, simplemente hay que poner # al comienzo de la linea y lo que esté a la derecha no será ejecutado.\nFinalmente, a la hora de escribir código en R lo más conveniente es usar las buenas prácticas diseñadas por programadores expertos. Para ello, lo mejor es aplicar una guía de estilo 📖, cuyo objetivo es hacer que nuestro código R sea más fácil de leer, compartir y verificar. Una de las mejores guías es The tidyverse style guide."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#tidy",
    "href": "002-toolkit-ciencia-datos.html#tidy",
    "title": "2  R toolkit para la ciencia de datos",
    "section": "2.2 El ecosistema tidyverse",
    "text": "2.2 El ecosistema tidyverse\n\n\n\n\n\nEl tidyverse es una colección de paquetes de R para la ciencia de los datos. Todos los paquetes comparten una filosofía de diseño, una gramática y unas estructuras de datos subyacentes. Se basan en la idea tidy data propuesta por Hadley Wickham (Wickham et al. 2014) y pueden instalarse con una única orden en R:\n\ninstall.packages(\"tidyverse\")\n\nUna vez instalado, se cargan con la función library:\n\nlibrary(tidyverse)\n\nLos paquétes más importantes en ciencia de datos se enumeran a continuación.\n\n\n\n\n\n\n\n\n\n\n2.2.1 readr\nEl paquete readr roporciona una forma rápida y amigable de leer datos rectangulares (como csv, tsv y fwf). Está diseñado para analizar de forma flexible muchos tipos de datos que se encuentran en la naturaleza, mientras que sigue fallando limpiamente cuando los datos cambian inesperadamente.\nreadr admite los siguientes formatos de archivo asociados a estas funciones read_*():\n\nread_csv(): valores separados por coma, ficheros (CSV).\nread_tsv(): valores separados por tabulador, fichero (TSV).\nread_delim(): fichro delimitados (CSV y TSV son casos especiales).\nread_fwf(): archivos de ancho fijo.\nread_table(): archivos separados por espacios en blanco\nread_log(): archivos de logos web.\n\n\nLea la base de datos de calidad del aire de la ciudad de Madrid , air_mad.RDS, contenida en la carpeta data.\n\n(TODO): QUITAR LA FUNCIÓN HERE PORQUE A ESTE NIVEL SI NO SE EXPLICA PUEDE LIAR\n\nair_mad &lt;- readr::read_rds(\"data/air_mad.RDS\")\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 dplyr\nEl paquete dplyr Ofrece una gramática de la manipulación de datos, proporcionando un conjunto coherente de verbos que resuelven los problemas más comunes de manipulación de datos y que pueden ser organizado tres categorías basado en el dataset:\n\nFilas:\n\n\nfilter(): elige filas en función de los valores de la columna.\nslice(): elige filas en función de la ubicación.\narrange(): cambia el orden de las filas.\n\n\nColumnas:\n\n\nselect(): indica cuando una columa es incluida o no.\nrename(): cambia el nombre de la columna.\nmutate(): cambia los valores de las columnas y crea nuevas columnas.\nrelocate(): cambia el orden de las columnas.\n\n\nGrupos de filas:\n\n\nsummarise(): contrae un grupo en una sola fila.\n\n\n\n\n\n\n\nEl operador pipe\n\n\n\nCanaliza la salida de una función a la entrada de otra función.\n\nsegundo(primero(datos))\n\nse traduce en:\n\ndatos %&gt;% \n  primero %&gt;% \n  segundo\n\nOperador pipe de {maggrit} %&gt;% Operador pipe de R base |&gt;\n\n\n\n\n\n\n\n\nEjemplo. Medidas de posición por estación de monitoreo\n\n\n\nCalcule las medidas de posición (mínimo, máximo, Q1, Q3, media y mediana) por estación de monitoreo , agrupando por id_name para Partículas &lt; 2.5 µm nom_abv == \"PM2.5\" utilizando las funciones de la libreria dplyr.\n\n\n\nair_mad %&gt;% # Summary por grupo usando dplyr \n  na.omit() %&gt;% # omitimos los NAs para el análisis\n  filter(nom_abv == \"PM2.5\") %&gt;% # filtramos por PM2.5\n  group_by(id_name) %&gt;% \n  summarize(min = min(valor),\n            q1 = quantile(valor, 0.25),\n            median = median(valor),\n            mean = mean(valor),\n            q3 = quantile(valor, 0.75),\n            max = max(valor))\n\n# A tibble: 8 × 7\n  id_name            min    q1 median  mean    q3   max\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Casa de Campo        0     5      8  9.21    12   850\n2 Castellana           0     6      9  9.66    12   195\n3 Cuatro Caminos       0     6      9 10.2     13    59\n4 Escuelas Aguirre     0     7     10 11.1     14   137\n5 Méndez Álvaro        0     6      9 10.2     13    78\n6 Plaza Castilla       0     6      8  9.59    12   273\n7 Plaza Elíptica       0     7     10 11.0     14    61\n8 Sanchinarro          0     5      7  8.36    10    68\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 tidyr\nEl paquete tidyr proporciona un conjunto de funciones que le ayudan a obtener datos ordenados. Los datos ordenados son datos con una forma consistente. En resumen, cada variable va en una columna, y cada columna es una variable.\nLas funciones se agrupan en 5 grandes categorías:\n\nPivotar. Convierte entre formas largas y anchas, pivot_longer() y pivot_wider()\nRectangling. Convierte listas profundamente anidadas (a partir de JSON) en tibbles ordenados: unnest_longer(), unnest_wider(), hoist().\nAnidar. Convierte los datos agrupados en una forma en el que cada grupo se convierte en una sola fila que contiene un data frame anidado nest() y viceversa, unnest().\nDividir y combinar columnas de caracteres. Funciones: separate(), extract() y unite()\nValores perdidos. Funciones: complete(), drop_na(), fill(), replace_na()\n\n\n\n\n\n\n\nEjemplo. ¿Cuál es el día con mayor y menor concentración de NOx de todo el periodo?\n\n\n\n\n\n\n\nair_mad %&gt;% # Summary por grupo usando dplyr \n  na.omit() %&gt;% # omitimos los NAs para el análisis\n  filter(nom_abv == \"NOx\") %&gt;% # filtramos por NOx\n  group_by(fecha) %&gt;% # agrupamos por fecha\n  summarize(mad_mean = mean(valor)) %&gt;% # promedio de las estaciones\n  slice(which.max(mad_mean), which.min(mad_mean)) #seleccionamos el máximo y el mínimo \n\n# A tibble: 2 × 2\n  fecha      mad_mean\n  &lt;date&gt;        &lt;dbl&gt;\n1 2013-08-10  4086.  \n2 2020-05-10     6.38\n\n\nEl valor máximo, 415 µg/m3 de NOx, se observa el 21 de diciembre de 2011 y el valor mínimo, 6,32 µg/m3 de NOx, el 10 de mayo de 2020, en pleno estado de alarma.\n\n\n\n\n\n\n\n\n\n\n\n2.2.4 ggplot2\nEl paquete ggplot2 es un sistema para crear gráficos de forma ordenada, basado en The Grammar of Graphics. Al proporcionar los datos, le dice a ggplot2 cómo asignar a las variables la estética y qué gráficas utilizar.\nEl template básico para la función ggplot() es:\n\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nLos argumentos más importantes para la representación de gráficos son:\n\nggplot(), crea un nuevo gráfico.\naes(), construye la estética el gráfico.\n+(&lt;gg&gt;), añade componentes al gráfico.\nggsave(), guarda el gráfico.\n\n\n\n\n\n\n\nEjemplo. Gráfico de violín.\n\n\n\nRepresente, con un gráfico de violín, las concentraciones de NOx por año en el periodo estudiado.\n\n\nUna buena forma de ver los datos es por medio de un gráfico de violín:\n\nair_mad %&gt;% # Summary por grupo usando dplyr \n  na.omit() %&gt;% # omitimos los NAs para el análisis\n  filter(nom_abv == \"NOx\") %&gt;% # filtramos por NOx\n  group_by(year = format(fecha, \"%Y\")) %&gt;% \n  summarise(valor)  %&gt;% \n  ggplot(aes(factor(year), valor))+\n  geom_violin() +\n  geom_jitter(height = 0, width = 0.01) +\n  aes(x=factor(year), y=valor, fill=year)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLibro de referencia obligatoria: ggplot2: elegant graphics for data analysis.\nEs interesante conocer las Extensiones de ggplot2 así como las ggplot2 Cheat sheet disponible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.5 purrr\nEl paquete purrr mejora el conjunto de herramientas de programación funcional (PF) de R proporcionando un conjunto completo y coherente de herramientas para trabajar con funciones y vectores. Una vez que se dominan los conceptos básicos, purrr permite sustituir muchos bucles for por un código más fácil de escribir y más expresivo.\n\n\n\n\n\n\nNote\n\n\n\nConsulte la purrr Cheat sheet disponible para una descripcion de la librería.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.6 tibble\nEl paquete tibble es una reimaginación moderna del marco de datos, manteniendo lo que el tiempo ha demostrado que es eficaz, y desechando lo que no. Los Tibbles suelen conducir a un código más limpio y expresivo.\n\n\n\n\n\n\nTip\n\n\n\nConsulte el capítulo 10 de R for data science disponible online para una primera aproximación.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.7 stringr\nEl paquete stringr proporciona un conjunto cohesivo de funciones diseñadas para facilitar al máximo el trabajo con cadenas.\n\n\n\n\n\n\nTip\n\n\n\nConsulte el capítulo 14 de R for data science disponible online para una primera aproximación.\n\n\n\n\n\n\n\n\n\n2.2.8 forcats\nEl paquete forcats proporciona un conjunto de herramientas útiles que resuelven problemas comunes con los factores. R utiliza factores para manejar variables categóricas, variables que tienen un conjunto fijo y conocido de valores posibles.\n\n\n\n\n\n\nNote\n\n\n\nConsulte el capítulo 14 de R for data science disponible online para una primera aproximación."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#quarto",
    "href": "002-toolkit-ciencia-datos.html#quarto",
    "title": "2  R toolkit para la ciencia de datos",
    "section": "2.3 Informes reproducibles con Quarto/Rmarkdown",
    "text": "2.3 Informes reproducibles con Quarto/Rmarkdown\nEste es un documento R Quarto. Quarto es una sintaxis simple de formato para la creación de documentos HTML, PDF y MS Word. Para obtener más detalles sobre cómo usar R Quarto consulta https://quarto.org/.\nRMarkdown (el antecesor de Quarto) y Quarto son herramientas permiten combinar texto formateado con código y resultados en un mismo documento. Quarto facilita la interoperabilidad entre R, Python, Julia.\nLos documentos Quarto constan principalmetne de 3 partes: + el YAML (o cabecera del documento),\n\nel texto (cuerpo escrito del documento),\nlas chunks (tozos de código, en este caso, código R)\n\nA continuación se exponen, brevemente, las principales ideas para entender y trabajar sintaxis quarto.\n\n2.3.1 Introducción\n\n2.3.1.1 Markdown: un lenguaje de marcado ligero\nMarkdown es un lenguaje de marcado ligero y fácil de aprender que permite dar formato al texto de manera sencilla. Con Markdown, es posible aplicar negrita, cursiva, encabezados, listas y enlaces, entre otras opciones de formato, simplemente utilizando una sintaxis intuitiva. La facilidad de uso del lenguaje Markdown hace que sea ampliamente utilizado, sobre todo para la documentación.\n\n\n2.3.1.2 Integración de código y resultados\nUna de las características principales de Quarto/Rmarkdown es la capacidad de integrar código en el informe. Esto significa que se pueden incluir bloques de código o chunks en lenguajes como R, Python o Julia directamente en el documento. Estos bloques de código se ejecutan en tiempo real y los resultados, como tablas o gráficos, se incrustan automáticamente en el informe. Esto permite a los lectores ver tanto el código como los resultados generados, lo que promueve la transparencia y la reproducibilidad.\n\n\n2.3.1.3 Generación de múltiples formatos de salida\nQuarto ofrece la posibilidad de generar informes en diferentes formatos de salida, como por ejemplo: HTML, PDF, Word o presentaciones de diapositivas. Esto es especialmente útil cuando se necesita compartir el informe con diferentes audiencias o cuando se requiere una presentación visualmente atractiva. La conversión entre formatos se realiza de manera automática y se puede personalizar para que se adapte a las necesidades específicas de cada proyecto.\n\n\n2.3.1.4 Facilidad de uso y colaboración\nQuarto se integra con RStudio, lo que proporciona una experiencia de usuario fluida y amigable. Además, los archivos Rmarkdown son archivos de texto plano, lo que hace que sean fáciles para el control de versiones como Git. Esto facilita el trabajo en equipo y la colaboración en proyectos.\n\n\n\n\n\n\nTip\n\n\n\nLa mayoría de los paquetes tiene una amplia documentación y los llamados Cheatsheets (chuletas). Son un buen método para ampliar los conocimientos. Algunas de ellas son: - Documentación de Quarto - Cheatsheet de RMarkdown - Colección de otros cheatsheets de RStudio\n\n\n\n\n\n2.3.2 Instalación\nPara poder utilizar Quarto/Rmarkdown en R y RStudio, es necesario realizar algunos pasos de instalación. A continuación, se detallan los pasos necesarios:\n\nInstalar R (véase (instal-r-rstudio?)).\nInstalar RStudio (véase (instal-r-rstudio?)).\nIntalar Quarto: para ello hay que dirigirse a la página oficial de Quarto, acceder al apartado Get Started y seleccionar la instalación acorde al sistema operativo. POsteriormente hay que seguir los pasos del instalador.\nInstalar el paquete Quarto: Una vez que R, RStudio y Quarto, se debe instalar la librería de Quarto para habilitar la funcionalidad de Quarto en R y RStudio. Para ello, se puede ejecutar el siguiente comando en la consola de RStudio: install.packages(\"quarto\")\n\nDespués de la instalación, se debe configurar RStudio para utilizar Quarto como el motor de renderizado de Rmarkdown. Para ello, se debe abrir la pestaña Tools (Herramientas) en la barra de menú de RStudio y seleccionar Global Options (Opciones globales). En la ventana emergente, selecciona la pestaña R Markdown y asegúrate de que la opción Quarto esté seleccionada en la sección Render.\nCon estos pasos completados, ya se puede comenzar a utilizar Quarto en RStudio para crear informes reproducibles. Se puede crear un nuevo archivo Rmarkdown y comenzar a combinar texto y código en un solo documento.\n\n\n2.3.3 El YAML\nLa sintaxis básica de YAML utiliza pares clave-valor en el formato clave: valor. Otros campos YAML comúnmente encontrados en los encabezados de los documentos incluyen metadatos como author (autor), subtitle (subtítulo), date (fecha), así como opciones de personalización como theme (tema), fontcolor (color de la fuente), fig-width (ancho de la figura), etc. Se puede encontrar información sobre todos los campos YAML disponibles para documentos HTML en la página oficial de Quarto. Los campos YAML disponibles varían según el formato del documento, por ejemplo,los campos YAML para documentos PDF y para MS Word.\n---\ntitle: \"My Document\"\nauthor:\n    - Gema Fernández-Avilés\n    - Michal Kinel\nformat: \n  html:\n    toc: true\n    code-fold: true\n---\n\n\n\n2.3.4 Flujo de trabajo y el concepto renderizar\nUn informe en Quarto/Rmarkdown se compone de bloques de texto y bloques de código. El texto se escribe en formato Markdown. Los bloques de código, chunks se delimitan con etiquetas especiales y pueden contener código en R, Python u otros lenguajes compatibles.\n\n\n\n\n\n\nNote\n\n\n\nrenderizar en RMarkdown/Quarto se refiere al proceso de transformar un documento escrito en lenguaje Markdown, que combina texto y código, en un formato final legible y presentable, como un informe, un documento PDF o una página web interactiva.\nEl proceso de renderizado se realiza mediante la ejecución de un motor de renderizado, como Quarto, que interpreta el documento RMarkdown y produce el resultado final en el formato deseado, como un archivo HTML, PDF, Word, entre otros. Durante el proceso de renderizado, el motor ejecuta el código R, genera los gráficos y tablas correspondientes, y aplica el formato y estilo definidos en el documento.\n\n\nLa mejor manera de tejer un html_document es mediante el botón Render o la función quarto::quarto_render().\n\n\n\n\n\nDiagrama de transformación de un documento Quarto a HTML, PDF, etc.\n\n\n\n\n\n2.3.4.1 Trozos de código chunks\nLos bloques de código en Quarto/Rmarkdown se utilizan para incorporar código fuente en el informe. Estos bloques se delimitan mediante etiquetas especiales, como {r} para código en R o {python} para código en Python. Dentro de estos bloques, se puede escribir y ejecutar código de la misma manera que se haría en un entorno de programación normal.\nLa ejecución del código se realiza en tiempo real, lo que significa que los resultados generados, como tablas, gráficos o textos formateados, se incrustan directamente en el informe cuando se renderiza. Esto permite a los lectores ver los resultados obtenidos y verificar la reproducibilidad del análisis.\nSe puede insertar rápidamente bloques de código con el comando Agregar Bloque en la barra de herramientas del editor o escribiendo los delimitadores de bloque:\n\n2+2\n\n[1] 4\n\n\nLa salida de una chunk se puede personalizar con las opciones de knitr. Algunas de las más usadas son:\n\n|# include: false evita que el código y los resultados aparezcan en el archivo final. R Markdown sigue ejecutando el código en el chunk, y los resultados pueden ser utilizados por otros chunks.\n|# echo : false evita que el código, pero no los resultados, aparezcan en el archivo final. Es una forma útil de incrustar cifras.\n|# message : false evita que los mensajes generados por el código aparezcan en el archivo final.\n|# warning : false evita que las advertencias generadas por el código aparezcan en el acabado.\n|# fig.cap : \"...\" añade una leyenda a los resultados gráficos.\n\nMás información sobre la opción de trozos de código: Quarto chunks.\n\n\n2.3.4.2 Formato de texto\nEl texto en formato Markdown se utiliza para proporcionar explicaciones, descripciones y contexto en el documento Escribir texto en Markdown es sencillo, ya que no requiere conocimientos avanzados de programación. Con Markdown, se pueden aplicar diferentes estilos de formato utilizando una sintaxis sencilla y legible.\nSe pueden añadir enlaces, escribir en negrita o cursiva. Consulta la hoja de trucos de Rstudio descargar aquí.\n\nHacer citas\n\nbloque de línea\nOtros formatos de texto:\n\nsubrayado\ntachar\nsuperíndice\nsubíndice\nversalitas\n\nLa forma de añadir notas a pie de página es [^number]2\nTambién se puede añadir emojis increíbles como 😻 insertándolo en el editor visual markdown o mediante su código :heart_eyes_cat:.\n\nA veces es necesario separar el texto con una línea horizontal, para ello simplemente se introduce ***\n\no saltar una línea con código html &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;.\n\n\n2.3.4.3 Añadir una imagen\nPara añadir imágenes basta con escribir ![image_name](image_path)\n\n\n\nR learners. Créditos: Allison Horst\n\n\nSi desea centrar la imagen, un poco de código htm le ayudará, sólo tiene que utilizar &lt;center&gt; &lt;/center&gt;\n\n\n\n\nR & RStudio\n\n\n\nPara ajustar las imágenes puedes usar r chunk añadiendo la imagen con una función knitr::include_graphics\n\nknitr::include_graphics(\"https://bookdown.org/oscar_teach/estadistica_aplicada_con_r/r-rstudio.png\")\n\n\n\n\nMi super foto\n\n\n\n\n\nplot(cars)\nplot(pressure)\n\n\n\n\n\n\n\n(a) Plot 1: eed and Stopping Distances of Cars\n\n\n\n\n\n\n\n(b) Plot 2: por Pressure of Mercury as a Function of Temperature\n\n\n\n\nFigura 2.4: Plots\n\n\n\nVéanse ejemplos en Figure 2.4. En particular, Figure 2.4 (b).\n\n\n2.3.4.4 Ecuaciones\nInsertar ecuaciones en un documento Quarteo es fácil utilizando la escritura Latex + en línea: \\(A = (\\pi * \\lambda \\times r^{4}) / \\alpha\\)\n\ncentradas: \\[A = (\\pi * \\lambda \\times r^{4}) / \\alpha\\]\n\n\n\n2.3.4.5 Diagramas\nQuarto tiene soporte nativo para incrustar diagramas Mermaid y Graphviz. Esto te permite crear diagramas de flujo, diagramas de secuencia, diagramas de estado, diagramas gnatt, y más usando una sintaxis de texto plano inspirada en markdown.\nPor ejemplo, aquí se incrusta un diagrama de flujo creado con Mermaid:\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\nGantt diagram:\n\n\n\n\ngantt\ndateFormat  YYYY-MM-DD\ntitle Adding GANTT diagram to mermaid\nexcludes weekdays 2014-01-10\n\nsection A section\nCompleted task            :done,    des1, 2014-01-06,2014-01-08\nActive task               :active,  des2, 2014-01-09, 3d\nFuture task               :         des3, after des2, 5d\nFuture task2               :         des4, after des3, 5d\n\n\n\n\n\n\n\n\n2.3.4.6 Bloques de llamada\n\n\n\n\n\n\nNote\n\n\n\nTenga en cuenta que hay cinco tipos de llamadas, incluyendo: note, tip, warning, caution, and important.\n\n\n\n\n2.3.4.7 Listas\n\n\n\n\nítem X\nítem Y\nítem Z\n\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur gravida eu erat et fring. Morbi congue augue vel eros ullamcorper, eget convallis tortor sagittis. Fusce sodales viverra mauris a fringilla. Donec feugiat, justo eu blandit placerat, enim dui volutpat turpis, eu dictum lectus urna eu urna. Mauris sed massa ornare, interdum ipsum a, semper massa.\n\n\n\n\n\n2.3.4.8 Apéndice\nLorem ipsum dolor sit amet, consectetur adipiscing elit.\n\n\n2.3.4.9 Citaciones\nR Markdown The Definitive Guide (Xie, Allaire, and Grolemund 2018)\n\n\n\n2.3.5 Renderizado\nUtiliza el botón de Render en el entorno de desarrollo integrado (IDE) de RStudio para renderizar el archivo y previsualizar el resultado con un solo clic o combinación de teclas (⇧⌘K).\n\nknitr::include_graphics(\"img/rstudio-render.png\")\n\n\n\n\n\n\n\n\nSi se prefiere renderizar automáticamente cada vez que se guarda el documento, se puede activar la opción de Renderizar al Guardar (Render on Save) en la barra de herramientas del editor. La previsualización se actualizará cada vez que vuelvas a renderizar el documento. La vista de previsualización en paralelo funciona tanto para salidas en HTML como en PDF."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#buscar-ayuda-en-r-o-sobre-r-funciones-paquetes-errores",
    "href": "002-toolkit-ciencia-datos.html#buscar-ayuda-en-r-o-sobre-r-funciones-paquetes-errores",
    "title": "2  R toolkit para la ciencia de datos",
    "section": "2.4 Buscar ayuda en R o sobre R (funciones, paquetes, errores)",
    "text": "2.4 Buscar ayuda en R o sobre R (funciones, paquetes, errores)\nLas funciones de distintos paquetes de R tienen su ayuda y es muy fácil acceder a ella desde la consola poniendo un signo de interrogación delante de la función, por ejemplo al ejecutar ?mean en la consola se abrirá la ventana de Help con la descripción de la función, su uso, los argumentos y otros datos relevantes que nos permitirán entender mejor la función que ejecutamos. También, existe una amplia información online sobre las funciones y distintos paquetes disponible en Rdocumentation.org.\n\n2.4.1 Haz preguntas\nUno de los foros más amplio de búsqueda de preguntas y respuestas sobre la programación es StackOverflow. En StackOverflow se registraron hasta el momento más 450.000 preguntas. Se puede navegar por los archivos de StackOverflow y ver qué respuestas han sido votadas por los usuarios, o puedes hacer tus propias preguntas relacionadas con R y esperar una respuesta.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.2 Sigue la comunidad de R\nExisten diversas páginas web que contienen artículos sobre la programación en R, como por ejemplo:\n\nR bloggers: un agregador de blogs que vuelve a publicar artículos relacionados con R de toda la web. Un buen lugar para encontrar tutoriales de R,\nRWeekly: R Weekly se fundó el 20 de mayo de 2016. R está creciendo muy rápidamente, y hay un montón de grandes blogs, tutoriales y otros formatos de recursos que salen cada día. R Weekly quiere hacer un seguimiento de estas grandes cosas en la comunidad de R y hacerla más accesible para todos.\nRPubs: RStudio permite aprovechar el poder de R Markdown para crear documentos que entrelazan su escritura y la salida de su código R. En RPubs se puede publicar esos documentos en la web en un click.\nMedium: un sitio web de blogs donde se puede encontrar mucha temática sobre el Data Science."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#git-y-github",
    "href": "002-toolkit-ciencia-datos.html#git-y-github",
    "title": "2  R toolkit para la ciencia de datos",
    "section": "2.5 Git y GitHub",
    "text": "2.5 Git y GitHub\n¿Git y GitHub? En primer lugar, son dos cosas distintas:\n\nGit es un software de código abierto para el control de versiones. Con Git puedes hacer cosas como ver todas las versiones anteriores del código que has creado en un proyecto.\nGitHub es el servicio más popular (otros son GitLab y BitBucket) para colaborar en el código utilizando Git.\n\n\n2.5.1 ¿Por qué debería usar Git y GitHub?\nLas tres principales ventajas de utilizar Git y GitHub son:\n\nEl uso de Git y GitHub sirve como copia de seguridad. Dado que GitHub tiene una copia de todo el código que tienes localmente, si algo le ocurriera a tu ordenador, seguirías teniendo acceso a tu código.\nEl uso de Git y GitHub te permite utilizar el control de versiones. ¿Alguna vez has tenido documentos llamados trabajo_final.pdf, trabajo_final_2.pdf o este_es_el_final.pdf? En lugar de hacer copias de los archivos por miedo a perder el trabajo, el control de versiones permite ver lo que se hizo en el pasado, todo ello manteniendo versiones únicas de los documentos.\nEl uso de Git y GitHub hace posible trabajar en el mismo proyecto al mismo tiempo con distintos colaboradores. Se puede ver el autor de los cambios y se fuera necesario volver a la versión anterior.\n\nPara poner Git y GitHub a punto lo mejor es seguir la guía de Happy Git and GitHub for the useR en la que se explica paso a paso como registrarse en GitHub e instalar Git, además de integrarlo en RStuidio.\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\".\n\n\nXie, Yihui, Joseph J Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. CRC Press."
  },
  {
    "objectID": "002-toolkit-ciencia-datos.html#footnotes",
    "href": "002-toolkit-ciencia-datos.html#footnotes",
    "title": "2  R toolkit para la ciencia de datos",
    "section": "",
    "text": "La “Comprehensive R Archive Network” (CRAN) es una colección de sitios que contienen material idéntico, consistente en la(s) distribución(es) de R, las extensiones aportadas, la documentación para R y los binarios. El sitio principal de CRAN en la WU (Wirtschaftsuniversität Wien) en Austria puede encontrarse en la URL https://CRAN.R-project.org/ y se refleja diariamente en muchos sitios de todo el mundo.↩︎\nla nota a pie de página↩︎"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#compren-negocio",
    "href": "003-crispdm-air-madrid.html#compren-negocio",
    "title": "3  Análisis de datos de contaminación del aire en la ciudad de Madid",
    "section": "3.1 Entendimiento del negocio",
    "text": "3.1 Entendimiento del negocio\n\n\n\n\n\n\nFase I: Comprensión de los problemas y desafíos asociados con la contaminación del aire\n\n\n\n\nIdentificar las fuentes de contaminantes, evaluar los impactos en la salud humana y el medio ambiente y la formular políticas y regulaciones. Definición de los objetivos y requisitos:\nEstablecer los objetivos específicos del análisis desde la perspectiva de valor añadido para la empresa o entidad pública: predecir los niveles de contaminación, identificar áreas de alto riesgo o evaluar la efectividad de medidas de control existentes.\nDeterminar los requisitos de datos, como la disponibilidad de datos espaciales de calidad, datos meteorológicos y datos demográficos.\n\n\n\nSe comienza formulando las preguntas de investigación que se consideran clave en el análisis. Por ejemplo:\n\n¿Cuáles son las áreas con mayor nivel de contaminación del aire?\n¿Qué factores contribuyen a la contaminación del aire en estas áreas?\n¿Cómo ha evolucionado la contaminación del aire en estas áreas a lo largo del tiempo?\n\nUna vez identificadas las preguntas clave, se definen los objetivos del análisis. Por ejemplo, en este caso, el objetivo principal es proporcionar mapas de predicción de calidad del aire para todo el municipio de Madrid.\nEs importante tener en cuenta que los objetivos deben ser específicos, medibles y realistas. También deben estar alineados con los objetivos empresariales o públicos para proporcionar valor añadido."
  },
  {
    "objectID": "003-crispdm-air-madrid.html#compren-datos",
    "href": "003-crispdm-air-madrid.html#compren-datos",
    "title": "3  Análisis de datos de contaminación del aire en la ciudad de Madid",
    "section": "3.2 Comprensión de los datos",
    "text": "3.2 Comprensión de los datos\n\n\n\n\n\n\nFase II: entendimimiento de los datos de calidad del aire\n\n\n\n\nExploración de los datos:\n\nAnalizar la estructura de los datos, como las variables disponibles y sus tipos.\nRealizar resúmenes estadísticos y análisis exploratorios, como la identificación de valores atípicos y la distribución de los datos.\n\nVisualización de los datos espaciales:\n\nCrear gráficos y mapas para visualizar la distribución espacial de la contaminación del aire.\nUtilizar técnicas de visualización interactiva, como la creación de mapas interactivos y paneles de control.\n\nEvaluación de la calidad de los datos:\n\nEvaluar la calidad de los datos, como la integridad espacial, la consistencia y la completitud.\nTratar los valores faltantes y los valores atípicos de manera apropiada.\n\n\n\n\n\n\n\n\n\n\nLibrerías y funciones en R\n\n\n\nEl paquete integrado de tidyverse es un buen aliado en esta tarea, ya que recoge las librerías para lectura, transformación y representación de los datos. Las librerías a tener en cuenta son:\n\nLectura: reader.\nManejo de datos: dplyr.\nDatos temporales: lubridate. \nVisualización: ggplot2, leaflet.\n\n\n\n\n3.2.1 Obtención de los datos\nEl primer paso siempre es la obtención y carga de los datos. Los datos de la calidad de la ciudad de Madrid se encuentran disponibles en la página datos abiertos. Aquí, se seleccionan aquellos que tienen frecuencia diaria (disponibles aquí). Para obtener los datos de forma automatizada se utiliza la API proporcionada por la web de datos abiertos. Como base de obtención de los enlaces de descarga, se copia el enlace de la descarga en DECAT https://datos.madrid.es/egob/catalogo/201410-0-calidad-aire-diario.dcat. La definición y la estructura de los datos se encuentra en Intérprete de ficheros de calidad del aire.\n\nlibrary(tidyverse)\nlibrary(xml2)\nlibrary(vroom)\n\n# Se guarda la URL RDF de DECAT de los datos\nurl &lt;- \"https://datos.madrid.es/egob/catalogo/201410-0-calidad-aire-diario.dcat\"\n\n# Se carga la página\npage &lt;- url %&gt;%\n  read_xml() %&gt;%\n  as_list()\n\n# Se convierte en una lista e inspecciona para identificar la localización de los datos\nlocation &lt;- page[[\"RDF\"]][[\"Catalog\"]][[\"dataset\"]][[\"Dataset\"]]\nlocation &lt;- location[names(location) == \"distribution\"]\n\n\n# Tras identificar se genera un tibble con el año como identificación y el link a los datos\nlinks &lt;-\n  tibble(\n    year = sapply(X = location, function(x) unname(unlist(\n      x[[\"Distribution\"]][[\"title\"]][1]))),\n    link = sapply(X = location, function(x) unname(unlist(\n      x[[\"Distribution\"]][[\"accessURL\"]][1])))\n  )\n\n# Seleccionar el formato csv\nlinks &lt;- links %&gt;% filter(str_detect(link, pattern = \".csv\"))\n\n# Añadir el nombre de fichero a descargar\nlinks &lt;- links %&gt;% \n  mutate(file_name = paste0(\"datos_aire_madrid_\", year, \".csv\"))\n\n# Descarga de los ficheros csv desde 2013\nyears &lt;- links %&gt;% filter(year &gt;= \"2013\") %&gt;% .$year\n\nlapply(years, function(x) {\n  file_x &lt;- links %&gt;%\n    filter(year == x & str_detect(link, \".csv\"))\n  \n  if (x == max(years)) {\n    download.file(url = file_x$link,\n                  destfile = paste0(\"data/\",\n                                    file_x$file_name))\n  }\n  \n  if (!file.exists(paste0(\"data/\", file_x$file_name))) {\n    download.file(url = file_x$link,\n                  destfile = paste0(\"data/\",\n                                    file_x$file_name))\n  }\n})\n\n# Lectura de los ficheros\ndata &lt;- vroom(paste0(\"data/\", links %&gt;% filter(year %in% years) %&gt;% .$file_name))\n\n# Conversión de viariables a numéricas\ncols_to_numeric &lt;-\n  c(\n    \"PROVINCIA\",\n    \"MUNICIPIO\",\n    \"ESTACION\",\n    \"MAGNITUD\",\n    \"PUNTO_MUESTREO\",\n    \"ANO\",\n    \"MES\",\n    str_subset(names(data), pattern = '^D')\n  )\ndata &lt;- data %&gt;%\n  mutate(across(all_of(cols_to_numeric), as.numeric))\n\n# Guardar datos brutos\nwrite_rds(data, \"data/data_raw.RDS\")\n\n# Transformar de datos transversales a longitudinales\nair_mad &lt;- data %&gt;%\n  gather(v, valor, D01:V31) %&gt;%\n  mutate(DIA = str_sub(v, 2, 3),\n         v = str_sub(v, 1, 1))\n\nair_mad &lt;- air_mad %&gt;%\n  mutate(id = ESTACION,\n         fecha = as.Date(paste(ANO, MES, DIA, sep = \"-\"))) %&gt;%\n  select(id, MAGNITUD, fecha, v, valor)\n\nair_mad &lt;- air_mad %&gt;%\n  unique() %&gt;%\n  pivot_wider(names_from = v, values_from = valor)\nair_mad &lt;- air_mad %&gt;%\n  mutate(valor = as.numeric(D)) %&gt;%\n  select(-D)\n\n# Añadir información de diccionarios\nstation_names &lt;- read_csv(\"dictionaries/station_names.csv\")\nmagnitudes_names &lt;- read_csv(\"dictionaries/magnitudes_names.csv\")\n\nair_mad &lt;- left_join(air_mad, station_names, by = \"id\")\nair_mad &lt;- left_join(air_mad, magnitudes_names, by = \"MAGNITUD\")\nair_mad &lt;- air_mad %&gt;% select(-MAGNITUD)\n\n# Guardar los datos\nwrite_rds(air_mad, \"data/air_mad.RDS\")\n\nSe procede a la lectura de los datos obtenidos y procesados que se han guardado en el objeto air_mad.RDS. Posteriormente se consulta la clase del objeto.\n\nlibrary(tidyverse)\nair_mad &lt;- readr::read_rds(\"data/air_mad.RDS\")\nclass(air_mad)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nDe acuerdo con la salida, se trata de un objeto de clase tibble, que forma parte del universo tidyverse. A continuación, se explora el conjunto de datos.\n\nhead(air_mad)\n\n# A tibble: 6 × 12\n     id fecha      V     valor id_name      longitud latitud zona  tipo  nom_mag\n  &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  \n1     4 2023-01-01 V       1   Plaza de Es…    -3.71    40.4 Inte… tráf… Dióxid…\n2     4 2023-02-01 V       3   Plaza de Es…    -3.71    40.4 Inte… tráf… Dióxid…\n3     4 2023-03-01 V       3   Plaza de Es…    -3.71    40.4 Inte… tráf… Dióxid…\n4     4 2023-04-01 V       1   Plaza de Es…    -3.71    40.4 Inte… tráf… Dióxid…\n5     4 2023-05-01 V       1   Plaza de Es…    -3.71    40.4 Inte… tráf… Dióxid…\n6     4 2023-01-01 V       0.4 Plaza de Es…    -3.71    40.4 Inte… tráf… Monóxi…\n# ℹ 2 more variables: nom_abv &lt;chr&gt;, ud_med &lt;chr&gt;\n\n\n\ndim(air_mad)\n\n[1] 559009     12\n\n\n\nsummary(air_mad)\n\n       id            fecha                 V                 valor         \n Min.   : 4.00   Min.   :2013-01-01   Length:559009      Min.   :   -2.00  \n 1st Qu.:18.00   1st Qu.:2015-07-16   Class :character   1st Qu.:    2.00  \n Median :38.00   Median :2018-01-26   Mode  :character   Median :   13.00  \n Mean   :34.95   Mean   :2018-02-03                      Mean   :   26.19  \n 3rd Qu.:54.00   3rd Qu.:2020-08-02                      3rd Qu.:   36.00  \n Max.   :60.00   Max.   :2023-05-31                      Max.   :97396.00  \n                 NA's   :157                                               \n   id_name             longitud         latitud          zona          \n Length:559009      Min.   :-3.775   Min.   :40.35   Length:559009     \n Class :character   1st Qu.:-3.713   1st Qu.:40.41   Class :character  \n Mode  :character   Median :-3.689   Median :40.42   Mode  :character  \n                    Mean   :-3.683   Mean   :40.43                     \n                    3rd Qu.:-3.652   3rd Qu.:40.46                     \n                    Max.   :-3.580   Max.   :40.52                     \n                                                                       \n     tipo             nom_mag            nom_abv             ud_med         \n Length:559009      Length:559009      Length:559009      Length:559009     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n\n\n\nstr(air_mad)\n\ntibble [559,009 × 12] (S3: tbl_df/tbl/data.frame)\n $ id      : num [1:559009] 4 4 4 4 4 4 4 4 4 4 ...\n $ fecha   : Date[1:559009], format: \"2023-01-01\" \"2023-02-01\" ...\n $ V       : chr [1:559009] \"V\" \"V\" \"V\" \"V\" ...\n $ valor   : num [1:559009] 1 3 3 1 1 0.4 0.5 0.4 0.3 0.1 ...\n $ id_name : chr [1:559009] \"Plaza de España\" \"Plaza de España\" \"Plaza de España\" \"Plaza de España\" ...\n $ longitud: num [1:559009] -3.71 -3.71 -3.71 -3.71 -3.71 ...\n $ latitud : num [1:559009] 40.4 40.4 40.4 40.4 40.4 ...\n $ zona    : chr [1:559009] \"Interior M30\" \"Interior M30\" \"Interior M30\" \"Interior M30\" ...\n $ tipo    : chr [1:559009] \"tráfico\" \"tráfico\" \"tráfico\" \"tráfico\" ...\n $ nom_mag : chr [1:559009] \"Dióxido de Azufre\" \"Dióxido de Azufre\" \"Dióxido de Azufre\" \"Dióxido de Azufre\" ...\n $ nom_abv : chr [1:559009] \"SO2\" \"SO2\" \"SO2\" \"SO2\" ...\n $ ud_med  : chr [1:559009] \"µg/m3\" \"µg/m3\" \"µg/m3\" \"µg/m3\" ...\n\n\n\n\n\n\n\n\nWarning\n\n\n\nExisten paquetes como DataExplorer y dlookr que generan informes automáticos con los principales descriptivos.\n\n\n¿Cómo han evolucionado la concentración de contaminantes del aire en la ciudad de Madrid en el periodo considerado? Realizar un primer gráfico de lineas para responder a esta pregunta sería muy esclarecedor.\n\nlibrary(lubridate)\nair_mad %&gt;%\n  filter(V == \"V\") %&gt;% \n  group_by(semana = floor_date(fecha, unit = \"week\"), nom_mag) %&gt;%\n  summarise(media_estaciones = mean(valor, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = semana, y = media_estaciones)) +\n  geom_line(aes(color = nom_mag)) +\n  geom_smooth(size = 0.5, color = \"red\") +\n  labs(\n    x = NULL, y = \"(µg/m3)\", title = \"Evolución de contaminantes en Madrid\",\n    subtitle = \"Concentración media semanal en las estaciones de medición\",\n    caption = \"Fuente: Portal de datos abiertos del Ayuntamiento de Madrid\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_wrap(~nom_mag, scales = \"free_y\", ncol = 2)\n\n\n\n\nFigura 3.1: Evolución de contaminantes en Madrid (2013-2023)\n\n\n\n\nLas emisiones de la mayoría de los contaminantes del aire han disminuido significativamente en las últimas décadas (véase Figure 3.1), sin embargo, sus concentraciones todavía superan los límites legales en la mayoría de los países, lo que indica que el control de la contaminación del aire sigue siendo un desafío para las sociedades modernas. Este análisis se centra en el dióxido de nitrógeno (NO2) por sus implicaciones directas en la ciudad de Madrid, además de por las siguientes razones de peso:\n\nImpacto en la salud humana: el dióxido de nitrógeno son un contaminante del aire asociado a diversos problemas de salud, especialmente afecciones respiratorias. La exposición prolongada al NO2 puede causar irritación en las vías respiratorias, exacerbación de enfermedades pulmonares como el asma y aumentar el riesgo de infecciones respiratorias.\nFuente de emisión: el tráfico rodado es una de las principales fuentes de emisión de dióxido de nitrógeno en áreas urbanas. En ciudades con altos volúmenes de tráfico, como Madrid, se produce una considerable liberación de NO2 debido a la combustión de combustibles fósiles en los vehículos.\nRegulaciones y límites: el dióxido de nitrógeno está sujeto a regulaciones y límites establecidos tanto a nivel nacional como internacional. Estos límites buscan controlar y reducir la concentración de NO2 en el aire para proteger la salud de la población y mejorar la calidad del aire en general.\nDisponibilidad de datos: existen sistemas de monitoreo de calidad del aire que recopilan datos precisos sobre la concentración de dióxido de nitrógeno en tiempo real. En el caso de Madrid, el Ayuntamiento dispone de datos abiertos proporcionados por su red de estaciones de monitoreo, lo que facilita el análisis y seguimiento de los niveles de NO2 en la ciudad.\n\nTeniendo en cuenta estas razones, el análisis de datos de contaminación se centrará en el dióxido de nitrógeno (NO2) para obtener información relevante sobre su impacto en la salud pública y evaluar el cumplimiento de los límites establecidos para este contaminante en la ciudad de Madrid (y concretamente, en la zonas de calidad del aire definidas por el ayuntamiento).\n\nplot_air_mad &lt;- air_mad %&gt;%\n  filter(V == \"V\" & nom_abv == \"NO2\" & fecha &gt;= \"2018-01-01\") %&gt;% \n  group_by(fecha, nom_mag, zona) %&gt;%\n  summarise(media_estaciones = mean(valor, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = fecha, y = media_estaciones)) +\n  geom_line(aes(color = zona)) +\n  geom_smooth(size = 0.5, color = \"red\") +\n  labs(\n    x = NULL, y = \"(µg/m3)\", title = \"Evolución de NO2 en Madrid\",\n    subtitle = \"Concentración por zonas en las estaciones de medición\",\n    caption = \"Fuente: Portal de datos abiertos del Ayuntamiento de Madrid\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_wrap(~zona, ncol = 1)\n\nplot_air_mad"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#prep-datos",
    "href": "003-crispdm-air-madrid.html#prep-datos",
    "title": "3  Análisis de datos de contaminación del aire en la ciudad de Madid",
    "section": "3.3 Preparación de los datos",
    "text": "3.3 Preparación de los datos\nEn esta fase se abordan la limpieza y transformación de los datos de contaminación del aire para su posterior análisis.\nEn esta sección, se explorarán las técnicas y métodos para limpiar los datos de contaminación del aire y abordar los problemas de calidad, como los valores faltantes, los valores atípicos y los errores de medición. Se describirán las siguientes actividades:\n\n\n\n\n\n\nFase III: limpieza y transformación de los datos de calidad del aire\n\n\n\n\nTratamiento de valores faltantes, por averías en las estaciones de monitoreo, por ejemplo. Imputación o eliminación los valores faltantes de manera adecuada.\nIdentificar y analizar los valores atípicos en los datos de contaminación del aire, en días de calima, por ejemplo. Evaluar si los valores atípicos son errores de medición o representan información relevante.\nResolución de errores de medición, si los hubiera.\nIngeniería de variables: normalización, codificación de variables categóricas, agregación (pasar de frecuencia horaria a frecuencia diaria en la medición delos contaminantes).\n\n\n\n\n\n\n\n\n\nLibrarías y funciones en R\n\n\n\n\nManipulación de datos (valores atípicos y datos faltantes): dplyr\nManipulación de datos a lo ancho y largo: tidyr\n\n\n\n\nno2 &lt;- air_mad %&gt;% \n  filter(nom_abv == \"NO2\")\n\n\nno2 %&gt;% \n  count(V)\n\n# A tibble: 2 × 2\n  V         n\n  &lt;chr&gt; &lt;int&gt;\n1 N       819\n2 V     90267\n\n\nHay un total de 819 datos no válidos y 90267 válidos. Se seleccionan solamente los datos válidos:\n\nno2 &lt;- no2 %&gt;% \n  filter(V == \"V\")\n\n\n3.3.1 Valores faltantes (NA)\n¿Existen datos con valores faltantes, codificados como NA (not available)?:\n\nno2 %&gt;%\n  count(is.na(valor))\n\n# A tibble: 1 × 2\n  `is.na(valor)`     n\n  &lt;lgl&gt;          &lt;int&gt;\n1 FALSE          90267\n\n\nNo se encuentran valores válidos con NA en el campo valor.\nComo parte del proceso de preparación de los datos, a continuación, se agrupan los datos según la variable zona y se genera la variable valor_zona que representa la media por zona y día. Se crea el objeto no2_zona para tal efecto.\n\nno2_zona &lt;- no2 %&gt;% \n  group_by(zona, fecha) %&gt;% \n  summarise(valor_zona = mean(valor, na.rm = T))\n\n\n\n3.3.2 Valores atípicos (outliers)\nLa detección de valores atípicos es fundamental en esta tercera fase. A modo de ejemplo, se ilustra como detectar valores atípicos en la zona Interior de la M30 con el criterio de 1,5 veces el rango intercuartílico.\n\nanomalias &lt;- lapply(unique(no2_zona$zona), function(x) {\n  aux_data &lt;- no2_zona %&gt;%\n    filter(zona == x) %&gt;%\n    select(zona, fecha, valor_zona)\n  \n  aux_data &lt;- aux_data %&gt;%\n    group_by(month(fecha), year(fecha)) %&gt;%\n    mutate(anomaly = !between(\n      valor_zona,\n      quantile(aux_data$valor_zona, probs = c(.25)) - 1.5 * IQR(valor_zona),\n      quantile(aux_data$valor_zona, probs = c(.75)) + 1.5 * IQR(valor_zona)\n    )) %&gt;% \n    ungroup() %&gt;% \n    select(zona, fecha, anomaly)\n})\nanomalias &lt;- do.call(rbind, anomalias)\n  \nno2_zona &lt;- left_join(no2_zona, anomalias, by = c(\"zona\", \"fecha\"))\n\n\nno2_zona %&gt;% \n  filter(zona == \"Interior M30\") %&gt;% \n  ggplot(aes(fecha, valor_zona, color = anomaly)) +\n  geom_point() +\n  scale_x_date() + \n  scale_color_manual(values = c(\"black\", \"red\")) +\n  labs(title = \"Anomalías NO2 por zona de interes\",\n       subtitle = \"Interior M30\",\n       x = NULL, \n       y = \"(µg/m3)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n3.3.3 Ingeniería de variables\nUna vez se han realizado los ajustes en los datos se procede a creación de variables de interés. De acuerdo con el DECRETO 140/2017, de 21 de noviembre, del Consejo de Gobierno, por el que se aprueba el protocolo marco de actuación durante episodios de alta contaminación por dióxido de nitrógeno (NO2) en la Comunidad de Madrid, el objetivo de la media anual se fija en los 40 µg/m3. En consecuencia, se genera la variable lógica objetivo_anual, que tomará el valor TRUE cuando los valores son inferiores o iguales a 40 µg/m3 del objetivo anual y FALSE en caso contrario.\n\nno2_zona &lt;- no2_zona %&gt;%\n  mutate(objetivo_anual = valor_zona &lt;= 40)\n\nno2_zona %&gt;% \n  ggplot(aes(zona, fill = objetivo_anual)) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Cumplimiento del tope de 40 µg/m3 de NO2\",\n       subtitle = \"porcentaje de cumplimiento por zona\",\n       x = NULL,\n       y = \"%\", \n       fill = \"Cumplimiento del objetivo\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nSe observa que la zona que se encuentra más cerca del cumplimiento de los objetivos es la zona Noroeste, mientras la que se encuentra más alejada es la zona Suroeste. En General las zonas del Norte obtienen mejores resultados.\nDado que se trata de un objetivo anual se aplica a cada zona una media móvil con ventana de un año para ver la evolución de los datos:\n\nlibrary(zoo) \nno2_zona &lt;- no2_zona %&gt;% \n  group_by(zona) %&gt;%\n  mutate(valor_zona_anual = rollmean(valor_zona, k = 365, fill = NA, align = 'right'))\n\nno2_zona %&gt;% \n  filter(fecha &gt;= \"2014-01-01\") %&gt;% \n  ggplot(aes(fecha, valor_zona_anual, color = zona)) +\n  geom_line() +\n  scale_x_date() +\n  geom_hline(yintercept = 40, color = \"red\") +\n  labs(title = \"Cumplimiento del tope de 40 µg/m3 de NO2\",\n       subtitle = \"media móvil anual por zona\",\n       x = NULL,\n       y = \"µg/m3 de NO2\", \n       color = \"Zona\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nSe observa un claro cambio en los niveles de la contaminación desde 2020, el año del comienzo de la pandemia de COVID-19.\nEl siguiente paso será centrar el estudio en la zona de calidad del aire con mayores niveles de contaminación, el interior de la M30.\nSe filtra la zona de calidad del aire con mayores niveles de contaminación, la zona “Interior M30” y se crea el subconjunto no2_zona_m30.\n\nno2_zona_m30 &lt;- no2_zona %&gt;%\n  filter(zona == \"Interior M30\" & anomaly == FALSE) %&gt;% \n  ungroup() %&gt;% \n  select(fecha, valor_zona)\n\nSe visualizan los datos de NO2 en el interior de la M30\n\nlibrary(timetk)\n\nno2_zona_m30 %&gt;%\n  plot_time_series(fecha, valor_zona)\n\n\n\n\n\n\n\n3.3.4 Ingeniería de variables\nSe generan una serie de variables útiles para la predicción y se define h como el horizonte de predicción y que indica, también, el número de lags.\n\nfecha_num: valor numérico normalizado de la fecha.\ndow: valor numérico indicativo del día de la semana (siendo lunes el primer día).\nmonth: valor numérico indicativo del mes.\nquarter: valor numérico indicativo del trimestre.\nyear: año.\n\nLa creación de variables auxiliares como día de la semana, mes o año en los modelos de machine learning para la predicción de series temporales es necesaria debido a varias razones:\n\nTendencia estacional: Muchas series temporales exhiben patrones estacionales que se repiten en intervalos regulares, como estacionalidad diaria, mensual o anual. Al incluir variables como día de la semana, mes o año, los modelos de machine learning pueden capturar mejor estos patrones y ajustar las predicciones en consecuencia.\nDiferenciación de comportamiento: Las series temporales pueden tener diferentes comportamientos dependiendo del día de la semana o el mes. Por ejemplo, las ventas minoristas pueden ser más altas los fines de semana o durante las vacaciones. Al incluir variables auxiliares, los modelos pueden aprender estas diferencias y mejorar la precisión de las predicciones.\nCambios estacionales y tendencias a largo plazo: Al considerar variables como el año, los modelos pueden capturar cambios estacionales y tendencias a largo plazo en los datos. Por ejemplo, una serie de tiempo que muestra un crecimiento anual constante puede beneficiarse de la inclusión del año como variable para predecir futuros incrementos.\nAjuste a patrones no lineales: Al utilizar variables auxiliares, los modelos de machine learning pueden capturar patrones no lineales en las series temporales. Por ejemplo, puede haber interacciones complejas entre el día de la semana y el mes que influyen en el comportamiento de la serie.\n\nEn resumen, la inclusión de variables auxiliares como día de la semana, mes o año en los modelos de machine learning para la predicción de series temporales permite capturar patrones estacionales, diferencias de comportamiento, cambios estacionales y tendencias a largo plazo, así como ajustar a patrones no lineales. Esto mejora la capacidad del modelo para realizar pronósticos precisos y más sólidos en series temporales.\nAdemás de las variables auxiliares mencionadas anteriormente, la inclusión de lags o retardos en los modelos de machine learning para la predicción de series temporales es otra técnica importante. Los lags representan la observación pasada de la serie en diferentes puntos de tiempo y se utilizan como variables predictoras en el modelo.\nLa inclusión de lags permite capturar la dependencia temporal en los datos. Las observaciones pasadas de la serie pueden contener información relevante para predecir el valor futuro. Al incluir lags en el modelo, se proporciona al algoritmo de machine learning información sobre cómo las observaciones anteriores afectan a las futuras.\nLa elección adecuada de los lags es importante. Dependerá de la naturaleza de la serie y de su periodicidad. Por ejemplo, en una serie de tiempo mensual, se pueden incluir lags correspondientes a los meses anteriores. En una serie de tiempo diaria, se pueden considerar lags que representen los días anteriores.\nEn el caso de los modelos seleccionados se aplica lags de 1 al 14 por medio de la función lag_transformer() creada en el chunk siguiente.\nEn definitiva inclusión de lags permite al modelo capturar patrones de autocorrelación y estacionalidad en la serie temporal. Además, puede ayudar a capturar cambios en la dinámica de la serie y a mejorar la precisión de las predicciones.\n\n\n\n\n\n\nAutocorrelación y estacionalidad\n\n\n\nLa autocorrelación temporal se refiere a la correlación entre una observación y sus observaciones pasadas a lo largo del tiempo. Es una medida que indica cómo una observación en un momento dado está relacionada con las observaciones anteriores en la misma serie (Shumway and Stoffer 2017). Se trata un concepto fundamental en el análisis de series temporales, ya que proporciona información sobre la dependencia temporal y los patrones de comportamiento en los datos. Permite identificar si hay una relación lineal o no lineal entre las observaciones pasadas y presentes.\nLa estacionalidad se refiere a patrones regulares y repetitivos que se observan en los datos a lo largo del tiempo. Estos patrones pueden ser de corto plazo, como fluctuaciones diarias o semanales, o de largo plazo, como fluctuaciones estacionales o anuales (Hyndman 2021). Se trata de una característica común en muchos tipos de datos, como las ventas minoristas, la demanda de productos, el tráfico web, el clima y muchos otros.\n\n\nSin embargo, es importante tener en cuenta que la inclusión de demasiados lags puede aumentar la complejidad del modelo y llevar a problemas de sobreajuste1. Por lo tanto, se debe encontrar un equilibrio adecuado al seleccionar los lags relevantes y evitar la inclusión de lags redundantes.\nPara mitigasr los problemas de sobreajuste existen varias técnicas como:\n\nRegularización: Se aplica una penalización a los coeficientes o parámetros del modelo para evitar que tomen valores extremos.\nValidación cruzada: Se divide el conjunto de datos en subconjuntos de entrenamiento y validación para evaluar el rendimiento del modelo en datos no vistos durante el entrenamiento.\nReducción de la complejidad del modelo: Se puede reducir el número de características o la complejidad del modelo para evitar un ajuste excesivo.\nRecopilación de más datos: Obtener más datos puede ayudar a reducir el sobreajuste al proporcionar una visión más completa y representativa del problema.\n\nEn el caso de este proyecto se aplica el ajuste de parámetros y validación cruzada.\n\n# el horizonte\nh &lt;- 21\n\n# función auxilar para hacer los lags\nlag_transformer &lt;- function(data){\n  data %&gt;%\n    tk_augment_lags(valor_zona, .lags = 1:14) %&gt;%\n    ungroup()\n}\n\n# Extensión de los datos en h\nno2_zona_m30_ext &lt;- no2_zona_m30 %&gt;%\n  future_frame(\n    .date_var = \"fecha\", \n    .length_out = h,\n    .bind_data  = TRUE\n  ) %&gt;%\n  ungroup()\n\n# Generación de variables\nno2_zona_m30_ext &lt;- no2_zona_m30_ext %&gt;% \n  mutate(\n    fecha_num = normalize_vec(as.numeric(fecha)),\n    dow = wday(fecha, week_start = 1), \n    month = month(fecha),\n    quarter = quarter(fecha),\n    year = year(fecha)\n  ) %&gt;%\n  ungroup()\n\n# Añadir lags\nno2_zona_m30_ext &lt;- no2_zona_m30_ext %&gt;%\n  lag_transformer()\n\n\n\n3.3.5 División del dataset (entrenamamineto-validación y predicción)\nA continuación se divide el conjunto de datos en dos subconjuntos: uno para el entrenamiento y validación y otro para la predicción.\n\n# Datos de entrenamiento\ntrain_data &lt;- no2_zona_m30_ext %&gt;%\n  drop_na()\n\n# Datos para la prediccción\nfuture_data &lt;- no2_zona_m30_ext %&gt;%\n  filter(is.na(valor_zona))\n\nLos datos para el entrenamiento.\n\nstr(train_data)\n\ntibble [3,674 × 21] (S3: tbl_df/tbl/data.frame)\n $ fecha           : Date[1:3674], format: \"2013-01-15\" \"2013-01-16\" ...\n $ valor_zona      : num [1:3674] 33.6 31.6 36.2 25.7 28.3 ...\n $ fecha_num       : num [1:3674] 0.00366 0.00392 0.00419 0.00445 0.00471 ...\n $ dow             : num [1:3674] 2 3 4 5 6 7 1 2 3 4 ...\n $ month           : num [1:3674] 1 1 1 1 1 1 1 1 1 1 ...\n $ quarter         : int [1:3674] 1 1 1 1 1 1 1 1 1 1 ...\n $ year            : num [1:3674] 2013 2013 2013 2013 2013 ...\n $ valor_zona_lag1 : num [1:3674] 42.2 33.6 31.6 36.2 25.7 ...\n $ valor_zona_lag2 : num [1:3674] 27.2 42.2 33.6 31.6 36.2 ...\n $ valor_zona_lag3 : num [1:3674] 29.2 27.2 42.2 33.6 31.6 ...\n $ valor_zona_lag4 : num [1:3674] 57.2 29.2 27.2 42.2 33.6 31.6 36.2 25.7 28.3 19.7 ...\n $ valor_zona_lag5 : num [1:3674] 61.5 57.2 29.2 27.2 42.2 33.6 31.6 36.2 25.7 28.3 ...\n $ valor_zona_lag6 : num [1:3674] 52.5 61.5 57.2 29.2 27.2 42.2 33.6 31.6 36.2 25.7 ...\n $ valor_zona_lag7 : num [1:3674] 54.2 52.5 61.5 57.2 29.2 27.2 42.2 33.6 31.6 36.2 ...\n $ valor_zona_lag8 : num [1:3674] 53.1 54.2 52.5 61.5 57.2 29.2 27.2 42.2 33.6 31.6 ...\n $ valor_zona_lag9 : num [1:3674] 75.4 53.1 54.2 52.5 61.5 57.2 29.2 27.2 42.2 33.6 ...\n $ valor_zona_lag10: num [1:3674] 85 75.4 53.1 54.2 52.5 61.5 57.2 29.2 27.2 42.2 ...\n $ valor_zona_lag11: num [1:3674] 81.2 85 75.4 53.1 54.2 52.5 61.5 57.2 29.2 27.2 ...\n $ valor_zona_lag12: num [1:3674] 66.1 81.2 85 75.4 53.1 54.2 52.5 61.5 57.2 29.2 ...\n $ valor_zona_lag13: num [1:3674] 59.1 66.1 81.2 85 75.4 53.1 54.2 52.5 61.5 57.2 ...\n $ valor_zona_lag14: num [1:3674] 40.7 59.1 66.1 81.2 85 75.4 53.1 54.2 52.5 61.5 ...\n\n\nLos datos para la predicción:\n\nstr(future_data)\n\ntibble [21 × 21] (S3: tbl_df/tbl/data.frame)\n $ fecha           : Date[1:21], format: \"2023-06-01\" \"2023-06-02\" ...\n $ valor_zona      : num [1:21] NA NA NA NA NA NA NA NA NA NA ...\n $ fecha_num       : num [1:21] 0.995 0.995 0.995 0.996 0.996 ...\n $ dow             : num [1:21] 4 5 6 7 1 2 3 4 5 6 ...\n $ month           : num [1:21] 6 6 6 6 6 6 6 6 6 6 ...\n $ quarter         : int [1:21] 2 2 2 2 2 2 2 2 2 2 ...\n $ year            : num [1:21] 2023 2023 2023 2023 2023 ...\n $ valor_zona_lag1 : num [1:21] 25.3 NA NA NA NA NA NA NA NA NA ...\n $ valor_zona_lag2 : num [1:21] 31.5 25.3 NA NA NA NA NA NA NA NA ...\n $ valor_zona_lag3 : num [1:21] 28.2 31.5 25.3 NA NA NA NA NA NA NA ...\n $ valor_zona_lag4 : num [1:21] 15.9 28.2 31.5 25.3 NA NA NA NA NA NA ...\n $ valor_zona_lag5 : num [1:21] 16.4 15.9 28.2 31.5 25.3 NA NA NA NA NA ...\n $ valor_zona_lag6 : num [1:21] 19.6 16.4 15.9 28.2 31.5 25.3 NA NA NA NA ...\n $ valor_zona_lag7 : num [1:21] 25.4 19.6 16.4 15.9 28.2 31.5 25.3 NA NA NA ...\n $ valor_zona_lag8 : num [1:21] 18.3 25.4 19.6 16.4 15.9 28.2 31.5 25.3 NA NA ...\n $ valor_zona_lag9 : num [1:21] 15.1 18.3 25.4 19.6 16.4 15.9 28.2 31.5 25.3 NA ...\n $ valor_zona_lag10: num [1:21] 18.2 15.1 18.3 25.4 19.6 16.4 15.9 28.2 31.5 25.3 ...\n $ valor_zona_lag11: num [1:21] 10.5 18.2 15.1 18.3 25.4 19.6 16.4 15.9 28.2 31.5 ...\n $ valor_zona_lag12: num [1:21] 9.9 10.5 18.2 15.1 18.3 25.4 19.6 16.4 15.9 28.2 ...\n $ valor_zona_lag13: num [1:21] 12.5 9.9 10.5 18.2 15.1 18.3 25.4 19.6 16.4 15.9 ...\n $ valor_zona_lag14: num [1:21] 10.7 12.5 9.9 10.5 18.2 15.1 18.3 25.4 19.6 16.4 ...\n\n\n\n\n3.3.6 Validación cruzada para evaluación de los modelos\nLa librería timek con la función time_series_cv() crea conjuntos de validación cruzada de rsample para series temporales. Esta función genera un plan de muestreo comenzando con las observaciones más recientes de la serie temporal y avanzando hacia atrás. El procedimiento de muestreo es similar a rsample::rolling_origin(), pero se enfoca en los datos más recientes de la serie temporal para la validación cruzada.\nEs por ello, que el conjunto de datos de entrenamiento se refiere a la serie completa conocida ya que internamente va a dividir los datos en varias partes con sus subconjuntos de entrenamiento y test.\n\nresamples_tscv &lt;- time_series_cv(\n  data        = train_data,\n  assess      = h,\n  initial     = 12 * 4 * h,\n  skip        = 12 * 3 * h,\n  slice_limit = 4\n)\n\nresamples_tscv\n\n# Time Series Cross Validation Plan \n# A tibble: 4 × 2\n  splits            id    \n  &lt;list&gt;            &lt;chr&gt; \n1 &lt;split [1008/21]&gt; Slice1\n2 &lt;split [1008/21]&gt; Slice2\n3 &lt;split [1008/21]&gt; Slice3\n4 &lt;split [1008/21]&gt; Slice4\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa validación cruzada es una técnica utilizada en el análisis de series de tiempo para evaluar la estabilidad de los modelos a lo largo del tiempo. Consiste en dividir los datos de la serie de tiempo en diferentes subconjuntos, aplicar un modelo predictivo a cada subconjunto y luego comparar las predicciones generadas por cada modelo. Es importante por varias razones:\n\nEvalúa la estabilidad del modelo.\nValida el rendimiento del modelo\nPermite la comparación de modelos.\n\n\n\nEn el gráfico inferior se observa cómo se generó la división de los datos para la validación cruzada. En color azul se muestra la parte de entrenamiento y en rojo la del test. Posteriormente los modelos seleccionados se aplicarán a cada trozo de entrenamiento por separado y gracias a la parte del test se generarán las estadísticas que evaluarán los modelos en cada trozo. De esta forma, se obtendrán 4 estadísticas de rendimiento de cada modelo, lo que ayudará a no caer en el sobreajuste.\n\nresamples_tscv %&gt;%\n  tk_time_series_cv_plan() %&gt;%\n  plot_time_series_cv_plan(fecha, valor_zona, .facet_ncol = 2, .interactive = FALSE)"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#modelado",
    "href": "003-crispdm-air-madrid.html#modelado",
    "title": "3  Análisis de datos de contaminación del aire en la ciudad de Madid",
    "section": "3.4 Modelado",
    "text": "3.4 Modelado\n\n\n\n\n\n\nFase IV: selección de técnicas de modelado\n\n\n\n\nObjetivos del análisis: identificar si el objetivo es la predicción, clasificación, agrupación u otra tarea específica.\nNaturaleza de los datos: evaluar la estructura de los datos, la presencia de variables dependientes e independientes, y si se trata de datos espaciales, temporales, etc.\nDisponibilidad de datos: considerar la cantidad y calidad de los datos disponibles, así como los recursos computacionales y el tiempo requerido para entrenar los modelos.\nConocimiento del dominio: utilizar el conocimiento experto en contaminación del aire para seleccionar técnicas relevantes y realizar ajustes apropiados.\n\n\n\n\n\n\n\n\n\nLibrerías y funciones en R\n\n\n\nTidymodeling + tidymodels + modeltime + modeltime.resample\nBase Models + earth + glmnet + xgboost + lightgbm\nCore Packages + tidyverse + lubridate + timetk\n+ bonsai\nset.seed(160191)\n\n\n\n\n\n\n\nProceso de modelado en ciencia de datos\n\n\n\n\nEl principal paquete que se utilzará en el modelado de los datos es el paquete modeltime de R, apropiado para series temporales como es la distribución del NO2 durante 10 años con mediciones diarias. modeltime ofrece una variedad de herramientas y técnicas avanzadas para el pronóstico de series temporales. Está diseñado para simplificar el proceso de construcción, evaluación y despliegue de modelos de series temporales.\nCon modeltime, los usuarios pueden realizar tareas como la selección automática de modelos, la construcción de modelos en paralelo, la agregación de modelos, la validación cruzada y la generación de pronósticos. También ofrece una interfaz intuitiva para ajustar y personalizar modelos de series temporales, lo que facilita su adaptación a diferentes escenarios. Además, permite la integración con otras bibliotecas populares de R, como tidyverse, lo que brinda un enfoque coherente para el preprocesamiento de datos y el análisis exploratorio.\nPara más información sobre la biblioteca véase la documentación oficial en sobre modeltime.\n\n# Tidymodeling\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(modeltime.resample)\n\n# Base Models\nlibrary(earth)\nlibrary(glmnet)\nlibrary(xgboost)\nlibrary(lightgbm)\n\n# Core Packages\nlibrary(tidyverse)\nlibrary(lubridate)\n#library(timetk)\nlibrary(bonsai)\n\n\nset.seed(160191)\n\n\n3.4.1 La “receta” del modelo\nEl paquete recipes nace de la analogía entre preparar una receta de cocina y preprocesar los datos (como si de una receta de libro de cocina se tratase) y el cocinado (modelado).\nSe crea la receta del modelo, siendo la variable dependiente valor_zona y los predictores el resto de valiables del dataset.\n\n#library(recipes)\nrecipe_spec &lt;- recipe(valor_zona ~ ., train_data)\n\n# recipe_spec &lt;- recipe(valor_zona ~ fecha_num + dow + month + quarter +  year + valor_zona_lag1 + valor_zona_lag2 + ... valor_zona_lag21\", train_data)\n\n\n\n3.4.2 Modelos propuestos\n📉 Modelo 1: Generalized Linear Model with Elastic Net regularization (GLMNET): es una técnica de modelado que combina la regresión lineal generalizada con la regularización elastic net.\nGLMNET es útil cuando se trabaja con conjuntos de datos con un gran número de variables predictoras, ya que ayuda a seleccionar y ajustar las variables más relevantes. GLMNET utiliza una combinación de la regularización L1 (LASSO) y L2 (Ridge) para penalizar los coeficientes de las variables y controlar la complejidad del modelo. Esto permite seleccionar automáticamente las variables más relevantes y reducir la tendencia al sobreajuste. Por ejemplo, supongamos que se desea predecir el precio de la vivienda en función de múltiples variables, como el tamaño, el número de habitaciones, la ubicación, etc. Aplicando GLMNET, se identifican las variables más importantes para predecir el precio y ajustar el modelo de regresión lineal generalizada al mismo tiempo.\nEn el caso NO2 se procede con la siguiente especificación:\n\n# Especificación\nmodel_spec_glmnet &lt;- linear_reg(penalty = 0.04) %&gt;%\n  set_engine(\"glmnet\")\n\n# Ajuste\nworkflow_fit_glmnet &lt;- workflow() %&gt;%\n  add_model(model_spec_glmnet) %&gt;%\n  add_recipe(recipe_spec %&gt;% step_rm(fecha)) %&gt;%\n  fit(train_data) %&gt;%\n    recursive(\n      transform  = lag_transformer,\n      train_tail = tail(train_data, h))\n\n\n\n\n\n\n\nNote\n\n\n\nNótese que cuando se determina el workflow del modelo se desestima la variable fecha aplicando una modificación en la receta: add_recipe(recipe_spec %&gt;% step_rm(fecha))\nEsto es debido a que el modelo no soporta variables en formato de fecha (Date).\n\n\n📊 Modelo 2: Multivariate Adaptive Regression Splines (MARS). es una técnica de modelado que utiliza funciones de base para aproximar relaciones no lineales entre variables predictoras y una variable objetivo. Se adapta a los datos mediante la construcción de una red de segmentos de regresión que capturan cambios en la relación entre las variables.\nPor ejemplo, supongamos que queremos predecir el precio de una vivienda en función de variables como el tamaño, el número de habitaciones y la ubicación. Al aplicar el modelo MARS, éste identificará los segmentos de regresión óptimos para cada variable y construirá un modelo no lineal que se ajuste mejor a los datos. Esto permite capturar relaciones complejas y no lineales entre las variables predictoras y la variable objetivo.\nEste tipo de modelos también puede aplicarse para la modelización de la calidad de aire. García Nieto and Álvarez Antón (2014) modelizó la calidad del aire de Gijón con el modelo MARS. El estudio explora el uso de este algoritmo de regresión no paramétrico que puede aproximar la relación entre las variables de entrada y salida y expresarla matemáticamente. Se recopiló un conjunto de datos experimental de contaminantes del aire peligrosos durante tres años (2006-2008) y se utilizó para crear un modelo altamente no lineal de la calidad del aire en el área urbana de Gijón basado en la técnica de MARS.\nEn el caso NO2 se procede con la siguiente especificación:\n\n# Especificación\nmodel_spec_mars &lt;- mars(mode = \"regression\") %&gt;%\n  set_engine(\"earth\")\n\n# Ajuste\nworkflow_fit_mars &lt;- workflow() %&gt;%\n  add_model(model_spec_mars) %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  fit(train_data) %&gt;%\n    recursive(\n      transform  = lag_transformer,\n      train_tail = tail(train_data, h))\n\n📉 Modelo 3: LightGBM: es un modelo de aprendizaje automático basado en árboles de decisión que se destaca por su eficiencia y velocidad en el procesamiento de grandes volúmenes de datos. Utiliza el algoritmo de refuerzo (boosting) para construir una serie de árboles de decisión que se combinan para mejorar la precisión de las predicciones.\nUna de las principales características de LightGBM es su capacidad para manejar datos con alta dimensionalidad y realizar una selección automática de características importantes. También utiliza un enfoque de partición de datos basado en hojas, lo que permite una mayor eficiencia en el proceso de entrenamiento.\nUn ejemplo de aplicación de LightGBM puede ser en la predicción de la satisfacción del cliente en una empresa de comercio electrónico. Se pueden utilizar características como el historial de compras, el tiempo de respuesta del servicio al cliente y la interacción en redes sociales para predecir la satisfacción del cliente. Al entrenar un modelo LightGBM con estos datos, se puede obtener un modelo eficiente y preciso que permita identificar patrones y factores clave que influyen en la satisfacción del cliente.\nEn el caso NO2 se procede con la siguiente especificación:\n\n# Especificación\nmodel_spec_lightgbm &lt;- boost_tree(mode = \"regression\",\n                                  trees = 400,\n                                  learn_rate = 0.008,\n                                  tree_depth = 12,\n                                  min_n = 50) %&gt;%\n  set_engine(\"lightgbm\")\n\n# Ajuste\nworkflow_fit_lightgbm &lt;- workflow() %&gt;%\n  add_model(model_spec_lightgbm) %&gt;%\n  add_recipe(recipe_spec %&gt;% step_rm(fecha)) %&gt;%\n  fit(train_data) %&gt;%\n    recursive(\n      transform  = lag_transformer,\n      train_tail = tail(train_data, h))\n\n\n\n\n\n\n\nNote\n\n\n\nNótese que cuando se determina el workflow del modelo se desestima la variable fecha aplicando una modificación en la receta: add_recipe(recipe_spec %&gt;% step_rm(fecha))\nEsto es debido a que el modelo no soporta variables en formato de fecha (Date).\n\n\n📊 Modelo 4: Prophet boost con crecimiento logarítmico: es una herramienta de predicción de series temporales desarrollada por Facebook. Al especificar un crecimiento logarítmico en Prophet, se asume que la tasa de crecimiento de la serie temporal disminuye con el tiempo. Esto es útil cuando los datos muestran un crecimiento inicial rápido seguido de una desaceleración.\nPor ejemplo, supongamos que deseamos predecir las ventas de un producto a lo largo del tiempo. Si aplicamos el modelo Prophet con crecimiento logarítmico, capturará la tendencia de crecimiento inicial acelerado y luego la desaceleración esperada. Esto puede ser útil para ajustar mejor la serie temporal y realizar pronósticos más precisos.\nEl modelo Prophet también se utiliza en la predicción del la contaminación del aire. Un ejemplo es la predicción de la contaminación en Seúl (Shen, Valagolam, and McCalla 2020). El modelo logró predecir con precisión la concentración de varios contaminantes hasta un año de antelación, superando a otros modelos similares.\nEn el caso NO2 se procede con la siguiente especificación:\n\n# Especificación\nmodel_spec_prophet_boost_log &lt;- prophet_boost(\n  mode = 'regression',\n  growth = 'linear',\n  seasonality_yearly = T,\n  seasonality_weekly = T,\n  seasonality_daily = F,\n  logistic_floor = min(train_data$valor_zona),\n  logistic_cap = max(train_data$valor_zona),\n  changepoint_num = .8,\n  learn_rate = .001,\n  tree_depth = 4,\n  trees = 2000\n) %&gt;%\n  set_engine(\"prophet_xgboost\")\n\n# Ajuste\nworkflow_fit_prophet_boost_log &lt;- workflow() %&gt;%\n  add_model(model_spec_prophet_boost_log) %&gt;%\n  add_recipe(recipe_spec %&gt;% step_rm(fecha_num, dow, month, quarter, year)) %&gt;%\n  fit(train_data) %&gt;%\n    recursive(\n      transform  = lag_transformer,\n      train_tail = tail(train_data, h))\n\n\n\n\n\n\n\nNote\n\n\n\nNótese que cuando se determina el workflow del modelo se desestima la variable fecha_num, dow, month, quarter, year aplicando una modificación en la receta: add_recipe(recipe_spec %&gt;% step_rm(fecha_num, dow, month, quarter, year))\nEsto se debe a que el modelo Prophet boost con crecimiento logarítmico ya incluye en su estrucura variables que determinan la estacionalidad de los datos como: seasonality_yearly = T o seasonality_weekly = T.\n\n\nEl siguiente paso es agregar cada uno de los modelos a una tabla de Modeltime utilizando modeltime_table().\n\nmodels_tbl &lt;- modeltime_table(\n  workflow_fit_glmnet,\n  workflow_fit_mars,\n  workflow_fit_lightgbm,\n  workflow_fit_prophet_boost_log\n)\n\nmodels_tbl\n\n# Modeltime Table\n# A tibble: 4 × 3\n  .model_id .model     .model_desc                        \n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                              \n1         1 &lt;workflow&gt; RECURSIVE GLMNET                   \n2         2 &lt;workflow&gt; RECURSIVE EARTH                    \n3         3 &lt;workflow&gt; RECURSIVE LIGHTGBM                 \n4         4 &lt;workflow&gt; RECURSIVE PROPHET W/ XGBOOST ERRORS\n\n\nPor último, se realizan las predicciones entrenando los modelos seleccionados para los 4 trozos de datos del plan de validación cruzada con el fin de poder comparar los datos de predicción con los datos reales y obtener una buena evaluación de los modelos propuestos.\n\nresamples_fitted &lt;- models_tbl %&gt;%\n  modeltime_fit_resamples(\n    resamples = resamples_tscv,\n    control   = control_resamples(verbose = FALSE)\n  )\n\nresamples_fitted\n\n# Modeltime Table\n# A tibble: 4 × 4\n  .model_id .model     .model_desc                         .resample_results\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                               &lt;list&gt;           \n1         1 &lt;workflow&gt; RECURSIVE GLMNET                    &lt;rsmp[+]&gt;        \n2         2 &lt;workflow&gt; RECURSIVE EARTH                     &lt;rsmp[+]&gt;        \n3         3 &lt;workflow&gt; RECURSIVE LIGHTGBM                  &lt;rsmp[+]&gt;        \n4         4 &lt;workflow&gt; RECURSIVE PROPHET W/ XGBOOST ERRORS &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#eval-vali",
    "href": "003-crispdm-air-madrid.html#eval-vali",
    "title": "3  Análisis de datos de contaminación del aire en la ciudad de Madid",
    "section": "3.5 Evaluación",
    "text": "3.5 Evaluación\n\n\n\n\n\n\nFase V: Métricas de evaluación de modelos\n\n\n\n\nMAE (Mean Absolute Error): Es la media de las diferencias absolutas entre las predicciones y los valores reales. Un valor más bajo de MAE indica un mejor ajuste del modelo.\nMAPE (Mean Absolute Percentage Error): Es la media de los porcentajes absolutos de error entre las predicciones y los valores reales. Un valor más bajo de MAPE indica una mayor precisión en las predicciones.\nMASE (Mean Absolute Scaled Error): Es una métrica que compara el error absoluto promedio del modelo con el error absoluto promedio de un modelo ingenuo, generalmente un modelo de caminata aleatoria. Un valor de MASE inferior a 1 indica que el modelo es mejor que el modelo de referencia.\nSMAPE (Symmetric Mean Absolute Percentage Error): Es similar al MAPE, pero utiliza el promedio simétrico de los porcentajes absolutos de error entre las predicciones y los valores reales. Un valor bajo de SMAPE indica una mayor precisión en las predicciones.\nRMSE (Root Mean Squared Error): Es la raíz cuadrada de la media de los errores al cuadrado entre las predicciones y los valores reales. Un valor más bajo de RMSE indica una menor dispersión y un mejor ajuste del modelo.\nRSQ (R-squared): También conocido como coeficiente de determinación, es una medida que indica la proporción de la variabilidad de la variable de respuesta que puede explicar el modelo. Un valor más alto de RSQ indica un mejor ajuste del modelo a los datos observados.\n\n\n\nEn el campo de la ciencia de datos, la evaluación de modelos es fundamental para garantizar la calidad y el rendimiento de los modelos utilizados. A medida que los datos se vuelven cada vez más complejos y abundantes, los modelos de machine learning se han convertido en una herramienta poderosa para extraer información y tomar decisiones basadas en datos.\nLa evaluación de modelos de machine learning permite determinar la precisión, la eficacia y la capacidad de generalización de un modelo en función de los datos utilizados. Es crucial evaluar los modelos de forma adecuada para evitar problemas como el sobreajuste (overfitting) o el subajuste (underfitting) y para identificar cualquier sesgo o error en los resultados.\nAdemás, la evaluación de modelos proporciona información valiosa sobre la robustez y la confiabilidad de un modelo en diferentes escenarios y conjuntos de datos. Permite comparar diferentes modelos y enfoques, seleccionar el mejor modelo para una tarea específica y realizar mejoras iterativas para optimizar el rendimiento.\nEvaluación de la precisión:\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = FALSE\n    )\n\n\n\n\n\nresamples_fitted %&gt;%\n    modeltime_resample_accuracy(summary_fns = mean) %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      n\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    1\nRECURSIVE GLMNET\nResamples\n4\n7.57\n26.16\n0.88\n23.00\n9.71\n0.38\n    2\nRECURSIVE EARTH\nResamples\n4\n9.78\n36.55\n1.18\n29.75\n11.73\n0.34\n    3\nRECURSIVE LIGHTGBM\nResamples\n4\n7.90\n31.70\n1.04\n26.01\n9.87\n0.41\n    4\nRECURSIVE PROPHET W/ XGBOOST ERRORS\nResamples\n4\n10.77\n39.32\n1.27\n37.83\n13.19\n0.27\n  \n  \n  \n\n\n\n\nLos tres primeros modelos parecen los que mejor se ajustan a los datos. Sin embargo todos los modelos presentan unas métricas con un MAPE bastante elevado. EN este punto habría que reconsiderar la inclusión de más regresores.\nEl modelo con las métricas más bajas de error es el GLMNET.\nUna vez seleccionado el modelo, el siguiente paso deseado sería la predicción a futuro del valor del NO2.\n\nworkflow_fit_lightgbm %&gt;% \n  modeltime_table() %&gt;%\n  modeltime_forecast(\n    new_data    = future_data,\n    actual_data = no2_zona_m30_ext,\n    keep_data   = TRUE\n  ) %&gt;%\n  filter(fecha &gt;= \"2023-01-01\") %&gt;% \n  plot_modeltime_forecast(\n    .conf_interval_show = FALSE\n  )"
  },
  {
    "objectID": "003-crispdm-air-madrid.html#despliegue",
    "href": "003-crispdm-air-madrid.html#despliegue",
    "title": "3  Análisis de datos de contaminación del aire en la ciudad de Madid",
    "section": "3.6 Despliegue",
    "text": "3.6 Despliegue\n\n\n\n\n\n\nDespliegue de resultados\n\n\n\nPresentación de los resultados del análisis de manera clara y comprensible.\n\nCreación de una ShinyApp: se puede construir una aplicación interactiva que permita a los usuarios explorar y visualizar los datos de contaminación del aire, así como interactuar con los modelos y los resultados obtenidos.\nImplementación de un bot de Twitter: es posible desarrollar un bot de Twitter (rtweet, por ejemplo) que publique actualizaciones y resultados relacionados con la contaminación del aire, lo que permite difundir la información de manera efectiva.\nComunicación de los hallazgos y recomendaciones: Utilizando RMarkdown y RStudio, se pueden crear informes reproducibles que combinen el código, los resultados y las visualizaciones en un documento HTML, PDF u otro formato, lo que facilita la comunicación y la presentación de los hallazgos del análisis.\n\n\n\n\n\n\n\n\n\nLibrarías y funciones en R\n\n\n\n\nshyni\nflexdasboard\n\n\n\nEjemplo de creación de una 📺 ShinyApp en R:\n# Carga de la librería shiny\nlibrary(shiny)\n\n# Definición de la interfaz de la ShinyApp\nui &lt;- fluidPage(\n  titlePanel(\"Análisis de Contaminación del Aire\"),\n  sidebarLayout(\n    sidebarPanel(\n      # Aquí se pueden agregar controles para interactuar con los datos y modelos\n    ),\n    mainPanel(\n      # Aquí se pueden mostrar las visualizaciones y los resultados obtenidos\n    )\n  )\n)\n\n# Definición de la lógica de la ShinyApp\nserver &lt;- function(input, output) {\n  # Aquí se puede incluir el código para cargar los datos, entrenar modelos, etc.\n}\n\n# Creación de la ShinyApp\nshinyApp(ui = ui, server = server)\nLa etapa de “Implementación” es el sexto y último paso en el proceso de CRISP-DM. En esta etapa, debemos implementar el modelo seleccionado y seguir sus resultados. Esto puede incluir la generación de informes y visualizaciones para comunicar los resultados del análisis.\nEn el caso de un análisis de datos sobre la contaminación del aire utilizando R, podríamos utilizar diferentes técnicas para implementar y comunicar los resultados de nuestros modelos. Por ejemplo, si hemos ajustado un modelo de regresión, podríamos utilizar funciones como predict para generar predicciones con nuestro modelo y librerías como ggplot2 para visualizar los resultados. Por ejemplo:\npredicciones &lt;- predict(modelo, newdata = datos_test)\ndatos_test$predicciones &lt;- predicciones\n\nggplot(datos_test, aes(x = fecha, y = contaminacion)) +\n  geom_point() +\n  geom_line(aes(y = predicciones), color = \"red\")\nSi hemos ajustado un modelo de series temporales, podríamos utilizar funciones como forecast para generar predicciones con nuestro modelo y librerías como ggplot2 para visualizar los resultados. Por ejemplo:\npredicciones &lt;- forecast(modelo, h = length(datos_test$contaminacion))\ndatos_test$predicciones &lt;- predicciones$mean\n\nggplot(datos_test, aes(x = fecha, y = contaminacion)) +\n  geom_point() +\n  geom_line(aes(y = predicciones), color = \"red\")\nTambién podemos generar informes y visualizaciones para comunicar los resultados de nuestro análisis a otras personas. Por ejemplo, podríamos utilizar librerías como rmarkdown o shiny para crear informes dinámicos o aplicaciones interactivas que muestren los resultados de nuestro análisis.\n\n\n\n\nGarcía Nieto, P. J., and J. C. Álvarez Antón. 2014. “Nonlinear Air Quality Modeling Using Multivariate Adaptive Regression Splines in Gijón Urban Area (Northern Spain) at Local Scale.” Applied Mathematics and Computation 235 (May): 50–65. https://doi.org/10.1016/j.amc.2014.02.096.\n\n\nHyndman, & Athanasopoulos, R. J. 2021. Forecasting: Principles and Practice, 3rd Edition. OTexts. http://OTexts.com/fpp3.\n\n\nNordhausen, Klaus. 2009. “The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition by Trevor Hastie, Robert Tibshirani, Jerome Friedman.” International Statistical Review 77 (3): 482–82. https://doi.org/10.1111/j.1751-5823.2009.00095_18.x.\n\n\nShen, Justin, Davesh Valagolam, and Serena McCalla. 2020. “Prophet Forecasting Model: A Machine Learning Approach to Predict the Concentration of Air Pollutants (PM2.5, PM10, O3, NO2, SO2, CO) in Seoul, South Korea.” PeerJ 8 (September): e9961. https://doi.org/10.7717/peerj.9961.\n\n\nShumway, Robert H., and David S. Stoffer. 2017. Time Series Analysis and Its Applications. Springer International Publishing. https://doi.org/10.1007/978-3-319-52452-8."
  },
  {
    "objectID": "003-crispdm-air-madrid.html#footnotes",
    "href": "003-crispdm-air-madrid.html#footnotes",
    "title": "3  Análisis de datos de contaminación del aire en la ciudad de Madid",
    "section": "",
    "text": "El sobreajuste (overfitting en inglés) en el contexto del machine learning se refiere a un escenario en el cual un modelo se ajusta demasiado a los datos de entrenamiento y pierde su capacidad de generalización en datos nuevos e invisibles. En otras palabras, el modelo aprende y se ajusta demasiado a las particularidades y el ruido presentes en los datos de entrenamiento, en lugar de capturar los patrones y relaciones generales que se aplicarían a otros conjuntos de datos (Nordhausen 2009).↩︎"
  },
  {
    "objectID": "00a-pre-curso.html#instalación-de-r-y-rstudio",
    "href": "00a-pre-curso.html#instalación-de-r-y-rstudio",
    "title": "Appendix A — Preparación del entorno de trabajo",
    "section": "A.1 Instalación de R y RStudio",
    "text": "A.1 Instalación de R y RStudio\nSe proporcionarán instrucciones detalladas sobre cómo instalar R y RStudio en diferentes sistemas operativos, como Windows, macOS y Linux. Se explicará el proceso paso a paso y se destacarán los enlaces de descarga y recursos adicionales para obtener más información.\n\nA.1.1 Descarga e instalación de R\nR está mantenido por un equipo internacional de desarrolladores que hacen que el lenguaje esté disponible a través de la página web de The Comprehensive R Archive Network. En la parte superior de la página web se encuentran tres enlaces para descargar R. Simplemente debes hacer clic en el enlace que corresponda a tu sistema operativo: Windows, macOS o Linux.\n\nA.1.1.1 Windows\nPara instalar R en Windows, haz clic en el enlace Download R for Windows. Luego, haz clic en el enlace base. A continuación, haz clic en el primer enlace en la parte superior de la nueva página. Este enlace debería decir algo como Download R-4.3.1 for Windows, excepto que el 4.3.1 será reemplazado por la versión más actualizada de R. El enlace descargará un programa de instalación que instalará la versión más reciente de R para Windows. Ejecuta este programa y sigue los pasos del asistente de instalación que aparece. El asistente instalará R en las carpetas de tus archivos de programa y colocará un acceso directo en tu menú de inicio. Ten en cuenta que necesitarás tener todos los privilegios de administración apropiados para instalar nuevo software en tu máquina. Además se recomienda la instalación de Rtools.\n\n\n\n\n\n\nNote\n\n\n\nRtools es un conjunto de herramientas utilizado para construir paquetes de R desde el código fuente (aquellos que requieren la compilación de código C/C++ o Fortran) y para construir R en sí mismo. Rtools43 se utiliza para R 4.3.x y para R-devel, la versión de desarrollo de R.\n\n\n\n\n\n\n\n\nInstalación de Rtools43\n\n\n\n\n\nRtools43 solo es necesario para la instalación de paquetes de R desde el código fuente o la construcción de R desde el código fuente. R se puede instalar desde el instalador binario de R y, de forma predeterminada, se instalarán versiones binarias de los paquetes de CRAN, lo cual no requiere Rtools43.\nAdemás, existen servicios de construcción en línea disponibles para verificar y construir paquetes de R para Windows, para los cuales tampoco es necesario instalar Rtools43 localmente. El servicio de verificación de Winbuilder utiliza una configuración idéntica a la de las verificaciones de paquetes entrantes de CRAN y ya tiene preinstalados todos los paquetes de CRAN y Bioconductor.\nRtools43 se puede instalar desde el instalador de Rtools43. Se recomienda utilizar los valores predeterminados, incluida la ubicación de instalación predeterminada en C:\\rtools43.\nCuando se utiliza R instalado mediante el instalador, no es necesario realizar ninguna configuración adicional después de instalar Rtools43 para construir paquetes de R desde el código fuente. Cuando se utiliza la ubicación de instalación predeterminada, R y Rtools43 se pueden instalar en cualquier orden y Rtools43 se puede instalar incluso si R ya está en ejecución.\n\n\n\n\n\nA.1.1.2 macOS\nPara instalar R en una Mac, haz clic en el enlace Download R for macOS. A continuación, haz clic en el enlace del paquete R-4.3.1 (o el enlace del paquete para la versión más actualizada de R). Se descargará un instalador que te guiará a través del proceso de instalación, el cual es muy fácil. El instalador te permite personalizar tu instalación, pero la configuración predeterminada será adecuada para la mayoría de los usuarios.\n\n\nA.1.1.3 Linux - Ubuntu\nLos paquetes para la versión actual de R 4.2 están disponibles para la mayoría de las versiones estables de Ubuntu para escritorio hasta su fecha oficial de fin de vida. Sin embargo, solo se brinda soporte completo para la última versión de Soporte a Largo Plazo (LTS). A partir del 2 de mayo de 2022, las versiones compatibles son:\n\nJammy Jellyfish (22.04, solo amd64)\nImpish Indri (21.10, solo amd64)\nFocal Fossa (20.04, solo LTS y amd64)\nBionic Beaver (18.04, LTS)\nXenial Xerus (16.04, LTS)\n\nEjecuta estas líneas (si eres root, omite el comando sudo) para informar a Ubuntu sobre los archivos binarios de R en CRAN.\n\n\nTerminal\n\n# update indices\nsudo apt update -qq\n# install two helper packages we need\nsudo apt install --no-install-recommends software-properties-common dirmngr\n# add the signing key (by Michael Rutter) for these repos\n# To verify key, run gpg --show-keys /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc \n# Fingerprint: E298A3A825C0D65DFD57CBB651716619E084DAB9\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\n# add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed\nsudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\"\n\nAquí utilizamos el comando “lsb_release -cs” para acceder a la versión de Ubuntu que estás utilizando, que puede ser una de las siguientes: “jammy”, “impish”, “focal”, “bionic”, …\nLuego ejecuta:\n\n\nTerminal\n\nsudo apt install --no-install-recommends r-base\n\npara instalar R y sus dependencias.\nPara obtener instrucciones más detalladas, incluyendo información sobre la administración y el mantenimiento de paquetes de R, consulta el README completo.\nPara versiones antiguas de R, consulta el README correspondiente.\n\n\n\nA.1.2 Descarga e instalación de RStudio\nRStudio es un entorno de desarrollo integrado (IDE) diseñado específicamente para trabajar con el lenguaje de programación R. Es una aplicación que proporciona una interfaz fácil de usar y funciones adicionales para facilitar la escritura, depuración y ejecución de código en R. RStudio ofrece un entorno de trabajo organizado con paneles que incluyen una consola interactiva para ejecutar comandos en tiempo real, una ventana para ver y editar archivos de código, herramientas de visualización de datos y mucho más. Es una herramienta muy popular y ampliamente utilizada por profesionales y estudiantes en el análisis de datos y la programación en R.\n\nA.1.2.1 Windows\n\nVe al sitio web de RStudio en https://www.rstudio.com/products/rstudio/download/.\nEn la sección “RStudio Desktop”, haz clic en “Download” debajo de la versión gratuita de RStudio.\nSelecciona la descarga correspondiente a tu sistema operativo de Windows (32 o 64 bits).\nUna vez descargado, ejecuta el archivo de instalación.\nSigue las instrucciones del asistente de instalación y acepta los términos y condiciones.\nDespués de completar la instalación, podrás abrir RStudio desde el menú de inicio o mediante un acceso directo en el escritorio.\n\n\n\nA.1.2.2 macOS\n\nVisita el sitio web de RStudio en https://www.rstudio.com/products/rstudio/download/.\nEn la sección “RStudio Desktop”, haz clic en “Download” debajo de la versión gratuita de RStudio.\nDescarga el archivo de instalación correspondiente a macOS.\nUna vez descargado, abre el archivo de instalación.\nArrastra el ícono de RStudio a la carpeta “Applications” para completar la instalación.\nPuedes abrir RStudio desde Launchpad o desde la carpeta “Applications”.\n\n\n\nA.1.2.3 Linux\n\nEn el sitio web de RStudio en https://www.rstudio.com/products/rstudio/download/, selecciona la descarga adecuada para tu distribución de Linux.\nDependiendo de tu distribución, puedes seguir las instrucciones específicas para instalar RStudio en Linux. Estas instrucciones suelen incluir comandos de terminal.\nUna vez instalado, puedes abrir RStudio desde el menú de aplicaciones o utilizando el comando “rstudio” en la terminal.\n\n\n\n\n\n\n\nCaution\n\n\n\nRecuerda que RStudio requiere tener previamente instalado R en tu sistema operativo, ya que RStudio es un entorno que se utiliza para trabajar con el lenguaje R. Si aún no tienes instalado R, puedes descargarlo desde el sitio web de CRAN (Comprehensive R Archive Network) y luego instalar RStudio."
  },
  {
    "objectID": "00a-pre-curso.html#configuración-de-librerías-que-tienen-que-venir-instalados",
    "href": "00a-pre-curso.html#configuración-de-librerías-que-tienen-que-venir-instalados",
    "title": "Appendix A — Preparación del entorno de trabajo",
    "section": "A.2 Configuración de librerías que tienen que venir instalados",
    "text": "A.2 Configuración de librerías que tienen que venir instalados\nEste apartado, se centra en las las librerías esenciales que se van a necesitar para seguir estelibro. Estas librerías son ampliamente utilizadas en el análisis de datos, el modelado estadístico y el aprendizaje automático. Se agrupan por tipología para facilitar su comprensión. A continuación, se explica cómo obtener e instalar cada una de ellas, además de proporcionar una breve descripción de su funcionalidad.\n\nLibrerías para el manejo de series temporales:\n\n\nmodeltime: Esta librería proporciona herramientas para la creación, manipulación y visualización de modelos de series temporales en R. Puedes obtener más información y descargar la librería desde el sitio oficial en GitHub\nmodeltime.resample: Es una extensión de la librería modeltime que ofrece funciones para el muestreo y la reescalada de series temporales.\n\n\nLibrerías para el modelado estadístico:\n\n\ntidymodels: Es un conjunto de librerías diseñadas para facilitar el flujo de trabajo en el modelado estadístico. Proporciona una interfaz consistente para ajustar, evaluar y comparar diferentes modelos.\nearth: Esta librería implementa modelos de regresión basados en funciones suaves y proporciona una interpretación más fácil de los modelos\nglmnet: Es una librería que implementa algoritmos eficientes para el ajuste de modelos de regresión lineal y logística con regularización L1 y L2.\nxgboost: Es una librería de aprendizaje automático que implementa el algoritmo de Gradient Boosting. Proporciona una implementación rápida y eficiente del algoritmo.\nlightgbm: Es otra librería de Gradient Boosting que se centra en la eficiencia y la velocidad de entrenamiento. Puedes instalarla con el comando install.packages(“lightgbm”).\n\n\nLibrerías para el manejo de datos y visualización:\n\n\ntidyverse: Es un conjunto de librerías que incluye ggplot2, dplyr, tidyr y otras herramientas para el manejo de datos y la visualización en R.\nlubridate: Esta librería proporciona funciones para el manejo de fechas y horas en R. Puedes instalarla con el comando install.packages(“lubridate”).\nbonsai: Es una librería para el análisis de datos de series temporales y la creación de modelos predictivos. Puedes obtener más información y descargarla desde el sitio oficial en GitHub: https://github.com/bonsai-team/bonsai\n\n\nLibrerías adicionales:\n\n\nknitr: Es una librería utilizada para la generación de informes dinámicos en R.\nxml2: Es una librería para el manejo de datos XML en R.\nvroom: Esta librería permite la lectura rápida de grandes conjuntos de datos en R.\ntimetk: Es una librería para la manipulación de series temporales en R. Proporciona funciones para la limpieza, transformación y visualización de datos temporales.\n\n\n# Instalación de las bibliotecas necesarias\ninstall.packages(\"modeltime\")\ninstall.packages(\"modeltime.resample\")\ninstall.packages(\"tidymodels\")\ninstall.packages(\"earth\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"xgboost\")\ninstall.packages(\"lightgbm\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"bonsai\")\ninstall.packages(\"knitr\")\ninstall.packages(\"xml2\")\ninstall.packages(\"vroom\")\ninstall.packages(\"timetk\")"
  }
]