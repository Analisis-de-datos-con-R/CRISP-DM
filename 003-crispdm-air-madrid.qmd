# Análisis de datos de contaminación del aire en la ciudad de Madid

:::{.callout-warning}
### Objetivo {-}

+ Proporcionar un caso de uso real para el análisis de datos con **R**.

+ Mostrar como implementar un análisis de ciencia de datos con la metodología CRISP-DM 

:::



## Entendimiento del negocio {#compren-negocio}


Es el primer paso en el proceso de **CRISP-DM** y es fundamental para llevar a cabo un análisis de datos. En esta etapa, se debe definir claramente los objetivos y criterios de cumplimiento del estudio desde la perspectiva de valor añadido para la empresa o entidad pública.

:::{.callout-tip}
### Comprensión de los problemas y desafíos {-}

  - Analizar los problemas y desafíos asociados con la contaminación del aire, como la identificación de fuentes de   
    contaminantes, la evaluación de los impactos en la salud humana y el medio ambiente, y la formulación de políticas y     regulaciones.
Definición de los objetivos y requisitos:
  - Establecer los objetivos específicos del análisis de datos, como predecir los niveles de contaminación, identificar
  áreas de alto riesgo o evaluar la efectividad de medidas de control.
  - Determinar los requisitos de datos, como la disponibilidad de datos espaciales de calidad, datos meteorológicos y datos demográficos..
:::


Se puede comenzar por identificar las preguntas clave que se consideren clave para el análisis, por ejemplo:

  - ¿Cuáles son las áreas con mayor nivel de contaminación del aire?
  - ¿Qué factores contribuyen a la contaminación del aire en estas áreas?
  - ¿Cómo ha evolucionado la contaminación del aire en estas áreas a lo largo
  del tiempo?
  
  
Una vez identificadas las preguntas clave, se podemos definir los objetivos y criterios de éxito para el análisis. Por ejemplo, **el objetivo principal del análisis es proporcionar mapas de predicción de calidad del aire para todo el municipio de Madrid**.

Es importante tener en cuenta que los objetivos deben ser específicos, medibles y realistas. También deben estar alineados con los objetivos empresariales o públicos para proporcionar valor añadido.





## Comprensión de los datos {#compren-datos}

:::{.callout-tip}
Esta fase abarca las siguientes tareas:

  - Exploración de los datos:
    - Analizar la estructura de los datos, como las variables disponibles y sus tipos.
    - Realizar resúmenes estadísticos y análisis exploratorios, como la identificación de valores atípicos y la
    distribución de los datos.
  
  - Visualización de los datos espaciales:
    - Crear gráficos y mapas para visualizar la distribución espacial de la contaminación del aire.
    - Utilizar técnicas de visualización interactiva, como la creación de mapas interactivos y paneles de control.
  
  - Evaluación de la calidad de los datos:
    - Evaluar la calidad de los datos, como la integridad espacial, la consistencia y la completitud.
    - Tratar los valores faltantes y los valores atípicos de manera apropiada.
    
:::


:::{.callout-note}
### Librerías y funciones en R {-}

El paquete integrado de `tidyverse` es un buen aliado en esta tarea, ya que recoge las librerías para lectura, transformación y representación de los datos. Las librerías a tener en cuenta son:

  - Lectura: `reader`.
  - Manejo de datos: `dplyr`.
  - Datos temporales: `lubridate`.
  - Datos espaciales: `sf`, `terra`. 
  - Visualización: `ggplot2`, `leaflet`.
:::

En el caso de un análisis del primer paso es la obtención y carga de los datos. Una vez que se obtienen los datos descargando de una API pública o diréctamente desde la web se procede con la lectura de los mismos por medio de librerías como `readr` o `data.table`, por ejemplo:

### Cómo obtener los datos

En el caso de los datos sobre la calidad de aire en el ayuntamiento de Madrid hay que dirigirse a la página [datos abiertos](https://datos.madrid.es/portal/site/egob) y buscamos los datos del aire diarios [(disponibles aquí)](https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=aecb88a7e2b73410VgnVCM2000000c205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default). Para obtener los datos de forma automatizada se utiliza la API de los datos abiertos. Como base de obtención de los enlaces de descarga se copia el enlace de la descarga en DECAT `https://datos.madrid.es/egob/catalogo/201410-0-calidad-aire-diario.dcat`. Además, es muy importante entender la estructura de los datos, en este caso hay que acceder a la documentación asociada a los datos: [Intérprete de ficheros de calidad del aire](https://datos.madrid.es/FWProjects/egob/Catalogo/MedioAmbiente/Aire/Ficheros/Interprete_ficheros_%20calidad_%20del_%20aire_global.pdf).
  
  
```{r }
#| eval: false
library(tidyverse)
library(xml2)
library(vroom)

# Se guarda la URL RDF de DECAT de los datos
url <- "https://datos.madrid.es/egob/catalogo/201410-0-calidad-aire-diario.dcat"

# Se carga la página
page <- url %>%
  read_xml() %>%
  as_list()

# Se convierte en una lista e inspecciona para identificar la localización de los datos
location <- page[["RDF"]][["Catalog"]][["dataset"]][["Dataset"]]
location <- location[names(location) == "distribution"]


# Tras identificar se genera un tibble con el año como identificación y el link a los datos
links <-
  tibble(
    year = sapply(X = location, function(x) unname(unlist(x[["Distribution"]][["title"]][1]))),
    link = sapply(X = location, function(x) unname(unlist(x[["Distribution"]][["accessURL"]][1])))
  )

# Seleccionar el formato csv
links <- links %>% filter(str_detect(link, pattern = ".csv"))

# Añadir el nombre de fichero a descargar
links <- links %>% 
  mutate(file_name = paste0("datos_aire_madrid_", year, ".csv"))

# Descarga de los ficheros csv desde 2013
years <- links %>% filter(year >= "2013") %>% .$year

lapply(years, function(x) {
  file_x <- links %>%
    filter(year == x & str_detect(link, ".csv"))
  
  if (x == max(years)) {
    download.file(url = file_x$link,
                  destfile = paste0("data/",
                                    file_x$file_name))
  }
  
  if (!file.exists(paste0("data/", file_x$file_name))) {
    download.file(url = file_x$link,
                  destfile = paste0("data/",
                                    file_x$file_name))
  }
})

# Lectura de los ficheros
data <- vroom(paste0("data/", links %>% filter(year %in% years) %>% .$file_name))

# Conversión de viariables a numéricas
cols_to_numeric <-
  c(
    "PROVINCIA",
    "MUNICIPIO",
    "ESTACION",
    "MAGNITUD",
    "PUNTO_MUESTREO",
    "ANO",
    "MES",
    str_subset(names(data), pattern = '^D')
  )
data <- data %>%
  mutate(across(all_of(cols_to_numeric), as.numeric))

# Guardar datos brutos
write_rds(data, "data_raw.RDS")

# Transformar de datos transversales a longitudinales
air_mad <- data %>%
  gather(v, valor, D01:V31) %>%
  mutate(DIA = str_sub(v, 2, 3),
         v = str_sub(v, 1, 1))

air_mad <- air_mad %>%
  mutate(id = ESTACION,
         fecha = as.Date(paste(ANO, MES, DIA, sep = "-"))) %>%
  select(id, MAGNITUD, fecha, v, valor)

air_mad <- air_mad %>%
  unique() %>%
  pivot_wider(names_from = v, values_from = valor)
air_mad <- air_mad %>%
  mutate(valor = as.numeric(D)) %>%
  select(-D)

# Añadir información de diccionarios
station_names <- read_csv("dictionaries/station_names.csv")
magnitudes_names <- read_csv("dictionaries/magnitudes_names.csv")

air_mad <- left_join(air_mad, station_names, by = "id")
air_mad <- left_join(air_mad, magnitudes_names, by = "MAGNITUD")
air_mad <- air_mad %>% select(-MAGNITUD)

# Guardar los datos
write_rds(air_mad, "data/air_mad.RDS")

```

Se procede a la lectura de los datos obtenidos y consulta la clase del objeto cargado:

```{r lee-data}
library(readr)
air_mad <- readr::read_rds("data/air_mad.RDS")
class(air_mad)
```

De acuerdo con la salida se trata de un onjeto de clase tibble, que forma parte del universo `tidyverse`. posteriormente se visionan las primeras líneas pare tener una visión preliminar de los datos. 



```{r}
head(air_mad)
```


Para ver la dimensión de la tabla de datos:

```{r dim-data}
dim(air_mad)
```

Luego se trata de 559009 filas y 12 columnas.


Una vez cargados los datos en **R**, las funciones como `summary()` y `str()` sirven para obtener información sobre la estructura y contenido de los datos, siguiendo con el ejemplo anterior:

DESCRIBIR BREVEMENTE ALGUNO DE ESTOS RESULTADOS

```{r str-data}
summary(air_mad)
str(air_mad)
```

NOTA IMPORTANTE:

Además, existen paquetes como DataExplorer y dlookr que generan informes automáticos con los principales descriptivos.


¿Cómo han evolucionado la concentración de contaminantes en la ciudad de Madrid?
Con las funciones del Tidyverse represente la evolución de todos los contaminantes medidos por las estaciones de monitoreo de la ciudad de Madrid en el periodo (2013-2023).


```{r }
#| message: false
air_mad %>%
  filter(V == "V") %>% 
  group_by(semana = floor_date(fecha, unit = "week"), nom_mag) %>%
  summarise(media_estaciones = mean(valor, na.rm = TRUE)) %>%
  ggplot(aes(x = semana, y = media_estaciones)) +
  geom_line(aes(color = nom_mag)) +
  geom_smooth(size = 0.5, color = "red") +
  labs(
    x = NULL, y = "(µg/m3)", title = "Evolución de partículas contaminantes en Madrid",
    subtitle = "Concentración media semanal en las estaciones de medición",
    caption = "Fuente: Portal de datos abiertos del Ayuntamiento de Madrid"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  facet_wrap(~nom_mag, scales = "free_y", ncol = 2)

```



Según Sanchis, Montero Fernández-Avilés (2022), la contaminación del aire exterior es un problema importante que afecta a la salud humana en las zonas urbanas de todo el mundo. Por lo tanto, no es sorprendente que el control de la contaminación del aire sea una preocupación destacada para los ciudadanos en la actualidad. Aunque las emisiones de la mayoría de los contaminantes del aire han disminuido significativamente en las últimas décadas, sus concentraciones todavía superan los límites legales en la mayoría de los países, lo que indica que el control de la contaminación del aire sigue siendo un desafío para las sociedades modernas. Según la Organización Mundial de la Salud (OMS, 2016), más de 4,2 millones de personas mueren prematuramente cada año debido a la contaminación del aire exterior. Los principales responsables de esto son el ozono (O3), los óxidos de nitrógeno (NOx) y, especialmente, las partículas finas o material particulado (PM) con un diámetro de 10 micrómetros o menos (PM10).


Por otro lado, como señalan Montero y Fernández-Avilés (2015, 2018), Madrid es la tercera ciudad más poblada de la Unión Europea después de Londres y Berlín, y cuenta con una extensa área metropolitana periférica con más de cinco millones de habitantes. Su actividad económica enérgica, incluso durante la pandemia de la Covid-19, resulta en niveles de PM10 superiores a los deseados debido al transporte, en particular al tráfico de vehículos, y a la actividad industrial, que son las principales fuentes de emisión de PM10.

Para realizar el análisis de datos de contaminación, se ha decidido seleccionar el contaminante en forma de  óxidos de nitrógeno (NOx) como uno de los contaminantes a estudiar. Esta elección se basa en varias razones relevantes:

1. Impacto en la salud humana: Los óxidos de nitrógeno son un contaminante del aire asociado a diversos problemas de salud, especialmente afecciones respiratorias. La exposición prolongada al NO2 puede causar irritación en las vías respiratorias, exacerbación de enfermedades pulmonares como el asma y aumentar el riesgo de infecciones respiratorias.

2. Fuente de emisión: El tráfico rodado es una de las principales fuentes de emisión de dióxido de nitrógeno en áreas urbanas. En ciudades con altos volúmenes de tráfico, como Madrid, se produce una considerable liberación de NO2 debido a la combustión de combustibles fósiles en los vehículos.

3. Regulaciones y límites: El dióxido de nitrógeno está sujeto a regulaciones y límites establecidos tanto a nivel nacional como internacional. Estos límites buscan controlar y reducir la concentración de NO2 en el aire para proteger la salud de la población y mejorar la calidad del aire en general.

4. Disponibilidad de datos: Existen sistemas de monitoreo de calidad del aire que recopilan datos precisos sobre la concentración de dióxido de nitrógeno en tiempo real. En el caso de Madrid, el Ayuntamiento dispone de datos abiertos proporcionados por su red de estaciones de monitoreo, lo que facilita el análisis y seguimiento de los niveles de NOx en la ciudad.

Teniendo en cuenta estas razones, el análisis de datos de contaminación se centrará en el dióxido de nitrógeno (NOx) para obtener información relevante sobre su impacto en la salud pública y evaluar el cumplimiento de los límites establecidos para este contaminante en la ciudad de Madrid.

```{r}
#| message: false
plot_air_mad <- air_mad %>%
  filter(V == "V" & nom_abv == "NOx" & fecha >= "2018-01-01") %>% 
  group_by(fecha, nom_mag, zona) %>%
  summarise(media_estaciones = mean(valor, na.rm = TRUE)) %>%
  ggplot(aes(x = fecha, y = media_estaciones)) +
  geom_line(aes(color = zona)) +
  geom_smooth(size = 0.5, color = "red") +
  labs(
    x = NULL, y = "(µg/m3)", title = "Evolución de NOx en Madrid",
    subtitle = "Concentración por zonas en las estaciones de medición",
    caption = "Fuente: Portal de datos abiertos del Ayuntamiento de Madrid"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  facet_wrap(~zona, ncol = 1)

plot_air_mad
```


  
## Preparación de los datos {#prep-datos}

En esta fase se  abordan la limpieza y transformación de los datos de contaminación del aire para su posterior análisis.


En esta sección, se explorarán las técnicas y métodos para limpiar los datos de contaminación del aire y abordar los problemas de calidad, como los valores faltantes, los valores atípicos y los errores de medición. Se describirán las siguientes actividades:

:::{.callout-tip}
### Limpieza de datos {-}

  - Tratamiento de valores faltantes:
    - Identificar y manejar los valores faltantes en los datos.
    - Imputar o eliminar los valores faltantes de manera adecuada.
    
  - Detección y manejo de valores atípicos:
    - Identificar y analizar los valores atípicos en los datos de contaminación del aire.
    - Evaluar si los valores atípicos son errores de medición o representan información relevante.
  
  - Resolución de errores de medición:
    - Identificar y corregir los errores de medición en los datos, 
      como mediciones inconsistentes o fuera de rango.
    - Utilizar técnicas estadísticas y conocimiento del dominio para abordar los errores de medición.
    
    
### Transformación de datos {-}

  - Normalización de variables:
    - Estandarizar o normalizar las variables de los datos de contaminación del aire para que tengan una escala comparable.
    - Utilizar técnicas como la estandarización z-score o la normalización min-max.

  - Codificación de variables categóricas:
    - Convertir las variables categóricas en variables numéricas o factores adecuados para su análisis.
    - Utilizar técnicas como la codificación one-hot o la codificación de etiquetas.
  
  - Agregación y transformación de variables:
    - Agregar o calcular nuevas variables a partir de las variables existentes, como calcular promedios, sumas o tasas de contaminación.
    - Realizar transformaciones matemáticas o logarítmicas en las variables para mejorar su distribución o interpretación.
:::

:::{.callout-note}
### Librarías y funciones en R {-}

  - Manipulación de datos (valores atípicos y datos faltantes): `dplyr`
  - Manipulación de datos a lo ancho y largo: `tidir`
:::


AQUÍ TODO EL **feature engineering** y el **exploratorio**

hacer anova aquí o en modelización 


```r
# Tratamiento de valores faltantes
air_pollution_clean <- na.omit(air_pollution)

# Detección y manejo de valores atípicos
outliers <- boxplot(air_pollution_clean$pollution_level, plot = FALSE)$out
air_pollution_clean <- air_pollution_clean[!air_pollution_clean$pollution_level %in% outliers, ]

# Normalización de variables
air_pollution_clean$pollution_level_normalized <- scale(air_pollution_clean$pollution_level)

# Codificación de variables categóricas
air_pollution_clean$city_category <- as.factor(air_pollution_clean$city)

# Agregación y transformación de variables
air_pollution_agg <- aggregate(pollution_level ~ year, data = air_pollution_clean, FUN = mean)
```


En el caso de un análisis de datos espaciales sobre la contaminación del aire utilizando R y RStudio, podríamos utilizar librerías como dplyr y tidyr para llevar a cabo estas tareas. Por ejemplo, si queremos seleccionar solo las variables relevantes para nuestro análisis, podríamos utilizar el siguiente código:

```r
library(dplyr)

datos_seleccionados <- datos %>%
  select(area, fecha, contaminacion)
```

Si queremos eliminar valores faltantes o atípicos, podríamos utilizar funciones como filter y is.na. Por ejemplo:

```r
datos_limpio <- datos_seleccionados %>%
  filter(!is.na(contaminacion)) %>%
  filter(contaminacion >= 0)
```
  
Si queremos transformar variables, podríamos utilizar funciones como mutate y ifelse. Por ejemplo, si queremos crear una nueva variable que indique si el nivel de contaminación del aire es alto o bajo, podríamos utilizar el siguiente código:

```r
datos_transformados <- datos_limpio %>%
  mutate(nivel_contaminacion = ifelse(contaminacion > 50, "Alto", "Bajo"))
```
  





## Modelado {#modelado}


:::{.callout-tip}
### Selección de técnicas de modelado {-}

  - Objetivos del análisis: Identificar si el objetivo es la predicción, clasificación, agrupación u otra tarea específica.
  - Naturaleza de los datos: Evaluar la estructura de los datos, la presencia de variables dependientes e independientes, y si se trata de datos espaciales o no.
  - Disponibilidad de datos: Considerar la cantidad y calidad de los datos disponibles, así como los recursos computacionales y el tiempo requerido para entrenar los modelos.
  - Conocimiento del dominio: Utilizar el conocimiento experto en contaminación del aire para seleccionar técnicas relevantes y realizar ajustes apropiados.
:::




:::{.callout-note}
### librarías y funciones en R {-}

  - `lm` y `glm`: Regresión lineal: Utilizada para modelar la relación entre variables dependientes e independientes y predecir los niveles de contaminación del aire.
  - `gstat` y `geoR`: Kriging: Un método de interpolación espacial utilizado para estimar valores de contaminación en ubicaciones no muestreadas, basado en la variabilidad espacial de los datos.
::: 


**SE PUEDE HACER UN KRIGING Y COMPARAR CON IDW COMO EN EL EJEMPLO DE CLIMAEMET QUE TE PASÉ, EL PROBLEMA ES QEU NO QUEDA MUY LUCIDO PORQUE COMO HAY POCAS ESTACIONES DE MONITOREO ES DIFÍCIAL CAPTAR LA DEPENDENCIA ESPACIAL**
  
Ejemplo de Kriging en R:

```r
# Carga de la librería gstat para el Kriging
library(gstat)

# Creación del objeto de datos espaciales
air_pollution_sp <- st_as_sf(air_pollution_clean)

# Ajuste del modelo de Kriging
krige_model <- gstat::krige(pollution_level ~ 1, locations = air_pollution_sp, newdata = new_locations)

# Visualización de los resultados del Kriging
plot(krige_model)
```

- Interpolación por distancia inversa (IDW): Otro método de interpolación espacial que asigna valores de contaminación a ubicaciones no muestreadas basándose en la inversa de la distancia ponderada a los puntos de muestra.
Ejemplo de Interpolación por distancia inversa en R:

```r
# Carga de la librería gstat para la interpolación por distancia inversa
library(gstat)

# Creación del objeto de datos espaciales
air_pollution_sp <- st_as_sf(air_pollution_clean)

# Ajuste del modelo de Interpolación por distancia inversa
idw_model <- gstat::idw(pollution_level ~ 1, locations = air_pollution_sp, newdata = new_locations)

# Visualización de los resultados de la interpolación por distancia inversa
plot(idw_model)

```




La librería gstat para llevar a cabo una interpolación por distancia inversa. Por ejemplo:

```r
idw_model <- idw(formula = contaminacion ~ 1, data = datos_train, newdata = datos_test)
```

Una vez que hemos ajustado nuestros modelos, podemos evaluar su rendimiento utilizando técnicas como la validación cruzada. Por ejemplo:

```r
kriging_cv <- krige.cv(contaminacion ~ 1, datos_train, model = kriging_model)
mean((kriging_cv$residual)^2)
```





## Evaluación {#eval-vali}


:::{.callout-tip}
### Métricas de evaluación de modelos {-}

  - Validación cruzada de k-fold: Divide los datos en k subconjuntos (folds), donde cada subconjunto se utiliza como conjunto de prueba y el resto como conjunto de entrenamiento. Se repite el proceso k veces y se promedian los resultados.
  
  - Validación cruzada dejando uno fuera (LOOCV): Utiliza un único punto de datos como conjunto de prueba y el resto como conjunto de entrenamiento. Este proceso se repite para todos los puntos de datos y se obtiene un promedio de los resultados.
  
  
  - Error cuadrático medio (ECM): Mide la diferencia cuadrada promedio entre los valores predichos y los valores reales de contaminación del aire.
  - Error absoluto medio (EAM): Calcula la diferencia promedio entre los valores predichos y los valores reales de contaminación del aire.
  - Coeficiente de determinación (R^2): Proporciona una medida de cuánta variabilidad en los datos de contaminación del aire es explicada por el modelo.
  - Índices de bondad de ajuste específicos del modelo utilizado, como el error de predicción promedio (RMSE) para modelos de kriging y la interpolación por distancia inversa ponderada.

:::


:::{.callout-note}
### Librarías y funciones en R {-}

- gstat::xxx

- geoR::

  - Validación de modelos de kriging: Se evalúa el rendimiento del modelo de kriging utilizando métricas como el RMSE (Root Mean Square Error) para comparar los valores predichos con los valores reales de contaminación del aire.

  - Validación de la interpolación por distancia inversa: Se utiliza la misma métrica RMSE para evaluar el rendimiento del modelo de interpolación por distancia inversa y comparar los valores predichos con los valores reales de contaminación del aire.
:::




Ejemplo de validación de modelos de interpolación en R:



```r
# Carga de la librería caret para la validación cruzada
library(caret)

# Creación del objeto de control para la validación cruzada
ctrl <- trainControl(method = "cv", number = 5)

# Entrenamiento y evaluación del modelo de kriging utilizando validación cruzada
krige_model_cv <- train(pollution_level ~ ., data = air_pollution_train, method = "krige",
                        trControl = ctrl)

# Evaluación de las métricas de rendimiento del modelo
krige_model_cv$results
```


```r
# Carga de la librería gstat para el kriging y la interpolación por distancia inversa
library(gstat)

# Ajuste del modelo de kriging
krige_model <- gstat::krige(pollution_level ~ 1, locations = air_pollution_train)

# Evaluación del rendimiento del modelo de kriging
krige_predictions <- predict(krige_model, locations = air_pollution_test)
krige_rmse <- sqrt(mean((krige_predictions - air_pollution_test$pollution_level)^2))

# Ajuste del modelo de interpolación por distancia inversa
idw_model <- gstat::idw(pollution_level ~ 1, locations = air_pollution_train)

# Evaluación del rendimiento del modelo de interpolación por distancia inversa
idw_predictions <- predict(idw_model, locations = air_pollution_test)
idw_rmse <- sqrt(mean((idw_predictions - air_pollution_test$pollution_level)^2))

```




Evaluación de la precisión y el rendimiento de los modelos desarrollados.
Utilización de métricas y técnicas de validación cruzada.




En el caso de un análisis de datos sobre la contaminación del aire utilizando R, podríamos utilizar diferentes técnicas para evaluar el rendimiento de nuestros modelos. Por ejemplo, si hemos ajustado un modelo de regresión, podríamos utilizar medidas como el error cuadrático medio (MSE) o el coeficiente de determinación (R^2) para evaluar su rendimiento. Por ejemplo:

```r
modelo <- lm(contaminacion ~ fecha + area, data = datos_train)
predicciones <- predict(modelo, newdata = datos_test)
mse <- mean((predicciones - datos_test$contaminacion)^2)
r2 <- summary(modelo)$r.squared
```

Si hemos ajustado un modelo de series temporales, podríamos utilizar medidas como el error cuadrático medio (MSE) o el error absoluto medio (MAE) para evaluar su rendimiento. Por ejemplo:

```r
library(forecast)

modelo <- auto.arima(datos_train$contaminacion)
predicciones <- forecast(modelo, h = length(datos_test$contaminacion))$mean
mse <- mean((predicciones - datos_test$contaminacion)^2)
mae <- mean(abs(predicciones - datos_test$contaminacion))
```

También podemos utilizar técnicas como la validación cruzada para evaluar el rendimiento de nuestros modelos. Por ejemplo:

```r
cv_resultados <- cv.glm(datos_train, modelo, K = 10)
mean(cv_resultados$delta)
```


SE PUEDE ENFOCAR TAMBIEN DE FORMA ALTERNATIVA COMO UNA PREDICCION TEMPORAL, VER.

SE PUEDE PONER UN EJEMPLO CON TIDYMODELS...



## Despliegue {#despliegue}

:::{.callout-tip}
### Despliegue de resultados {-}

  - Creación de una ShinyApp: Utilizando el paquete Shiny, se puede construir una aplicación interactiva que permita a los usuarios explorar y visualizar los datos de contaminación del aire, así como interactuar con los modelos y los resultados obtenidos.
  - Implementación de un bot de Twitter: Utilizando paquetes como rtweet, es posible desarrollar un bot de Twitter que publique actualizaciones y resultados relacionados con la contaminación del aire, lo que permite difundir la información de manera efectiva.
  - Informes reproducibles: Utilizando RMarkdown y RStudio, se pueden crear informes reproducibles que combinen el código, los resultados y las visualizaciones en un documento HTML, PDF u otro formato, lo que facilita la comunicación y la presentación de los hallazgos del análisis.
:::


:::{.callout-note}
### Librarías y funciones en R {-}

- shyni

- flexdasboard
:::

Ejemplo de creación de una ShinyApp en R:

```r
# Carga de la librería shiny
library(shiny)

# Definición de la interfaz de la ShinyApp
ui <- fluidPage(
  titlePanel("Análisis de Contaminación del Aire"),
  sidebarLayout(
    sidebarPanel(
      # Aquí se pueden agregar controles para interactuar con los datos y modelos
    ),
    mainPanel(
      # Aquí se pueden mostrar las visualizaciones y los resultados obtenidos
    )
  )
)

# Definición de la lógica de la ShinyApp
server <- function(input, output) {
  # Aquí se puede incluir el código para cargar los datos, entrenar modelos, etc.
}

# Creación de la ShinyApp
shinyApp(ui = ui, server = server)

```


Presentación de los resultados del análisis de manera clara y comprensible.
Comunicación de los hallazgos y recomendaciones.



La etapa de “Implementación” es el sexto y último paso en el proceso de CRISP-DM. En esta etapa, debemos implementar el modelo seleccionado y seguir sus resultados. Esto puede incluir la generación de informes y visualizaciones para comunicar los resultados del análisis.

En el caso de un análisis de datos sobre la contaminación del aire utilizando R, podríamos utilizar diferentes técnicas para implementar y comunicar los resultados de nuestros modelos. Por ejemplo, si hemos ajustado un modelo de regresión, podríamos utilizar funciones como predict para generar predicciones con nuestro modelo y librerías como ggplot2 para visualizar los resultados. Por ejemplo:


```r
predicciones <- predict(modelo, newdata = datos_test)
datos_test$predicciones <- predicciones

ggplot(datos_test, aes(x = fecha, y = contaminacion)) +
  geom_point() +
  geom_line(aes(y = predicciones), color = "red")
```


Si hemos ajustado un modelo de series temporales, podríamos utilizar funciones como forecast para generar predicciones con nuestro modelo y librerías como ggplot2 para visualizar los resultados. Por ejemplo:

```r
predicciones <- forecast(modelo, h = length(datos_test$contaminacion))
datos_test$predicciones <- predicciones$mean

ggplot(datos_test, aes(x = fecha, y = contaminacion)) +
  geom_point() +
  geom_line(aes(y = predicciones), color = "red")
```
  

También podemos generar informes y visualizaciones para comunicar los resultados de nuestro análisis a otras personas. Por ejemplo, podríamos utilizar librerías como rmarkdown o shiny para crear informes dinámicos o aplicaciones interactivas que muestren los resultados de nuestro análisis.

